{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3fb22b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Mon May 18 17:59:23 2020\n",
    "@author: rfablet\n",
    "\"\"\"\n",
    "\n",
    "#######################################\n",
    "\n",
    "import argparse\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "#import os\n",
    "#import tensorflow.keras as keras\n",
    "\n",
    "import time\n",
    "import copy\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.nn.functional as F\n",
    "import scipy\n",
    "from scipy.integrate import solve_ivp\n",
    "from sklearn.feature_extraction import image\n",
    "from netCDF4 import Dataset\n",
    "import datetime\n",
    "\n",
    "# specific torch module \n",
    "#import dinAE_solver_torch as dinAE\n",
    "import torch_4DVarNN_dinAE_Copy1 as NN_4DVar\n",
    "#import torch4DVarNN_solver as NN_4DVar\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "############################\n",
    "'''\n",
    "# !/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "from pathlib import Path\n",
    "import xarray as xr\n",
    "import argparse\n",
    "import numpy as np\n",
    "import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "from netCDF4 import Dataset\n",
    "\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('../4dvarnet-core')\n",
    "import solver as NN_4DVar\n",
    "\n",
    "from sklearn.feature_extraction import image\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from scipy import ndimage\n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93af09da",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "flagProcess    = [0,1,2,3,4]#Sequence fo processes to be run\n",
    "    \n",
    "flagRandomSeed = 0\n",
    "flagSaveModel  = 1\n",
    "     \n",
    "batch_size  = 96#4#4#8#12#8#256#8\n",
    "\n",
    "dirSAVE     = './ResDanube4DVar/'\n",
    "genFilename = 'Debit_v11'\n",
    "  \n",
    "flagAEType = 2 # 0: L96 model, 1-2: GENN\n",
    "DimAE      = 50#50#10#50\n",
    "    \n",
    "UsePriodicBoundary = True # use a periodic boundary for all conv operators in the gradient model (see torch_4DVarNN_dinAE)\n",
    "InterpFlag         = False\n",
    "\n",
    "NbDays          = 18244\n",
    "\n",
    "time_step  = 1\n",
    "DT = 21\n",
    "sigNoise   = np.sqrt(2)\n",
    "rateMissingData = 0.5#0.9\n",
    "\n",
    "flagTypeMissData = 3\n",
    "\n",
    "#####################################\n",
    "\n",
    "\n",
    "\n",
    "#import torch.distributed as dist\n",
    "\n",
    "## NN architectures and optimization parameters\n",
    "#batch_size      = 2#16#4#4#8#12#8#256#\n",
    "#DimAE           = 50#10#10#50\n",
    "#dimGradSolver   = 100 # dimension of the hidden state of the LSTM cell\n",
    "#rateDropout     = 0.25 # dropout rate \n",
    "#flag_aug_state = 2#True#\n",
    "#flag_augment_training_data = True#False#\n",
    "\n",
    "# data generation\n",
    "#sigNoise = 0. ## additive noise standard deviation\n",
    "#flagSWOTData = True #False # rue ## use SWOT data or not\n",
    "#flagNoSSTObs = False #True #\n",
    "#flag_vv  = 'vv_10m'\n",
    "\n",
    "#width_med_filt_spatial = 5\n",
    "#width_med_filt_temp = 1\n",
    "\n",
    "#dT              = 5 ## Time window of each space-time patch\n",
    "#W               = 200 ## width/height of each space-time patch\n",
    "#dx              = 1 ## subsampling step if > 1\n",
    "#Nbpatches       = 1#10#10#25 ## number of patches extracted from each time-step \n",
    "#rnd1            = 0 ## random seed for patch extraction (space sam)\n",
    "#rnd2            = 100 ## random seed for patch extraction\n",
    "#dwscale         = 1\n",
    "\n",
    "# loss\n",
    "#p_norm_loss = 2. \n",
    "#q_norm_loss = 2. \n",
    "#r_norm_loss = 2. \n",
    "#thr_norm_loss = 0.\n",
    "\n",
    "#W = int(W/dx)\n",
    "\n",
    "#UsePriodicBoundary = False # use a periodic boundary for all conv operators in the gradient model (see torch_4DVarNN_dinAE)\n",
    "#InterpFlag         = False # True => force reconstructed field to observed data after each gradient-based update\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb65781",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('........ Data extraction')\n",
    "if flagRandomSeed == 0:\n",
    "    print('........ Random seed set to 100')\n",
    "    np.random.seed(100)\n",
    "        \n",
    "###############################################################\n",
    "## data extraction\n",
    "ncfile = Dataset('Dataset_danube.nc',\"r\")\n",
    "L=[]\n",
    "for i in range(31):\n",
    "    L.append(ncfile['S'+str(i+1)][:].reshape(18244,1))\n",
    "        \n",
    "dataset = np.concatenate((L[0],L[1],L[2],L[3],L[4],L[5],L[6],L[7],L[8],L[9],L[10],L[11],L[12],L[13],L[14],L[15],L[16],L[17],L[18],L[19],L[20],L[21],L[22],L[23],L[24],L[25],L[26],L[27],L[28],L[29],L[30]),axis=1)\n",
    "\n",
    "# Definiton of training, validation and test dataset    \n",
    "i=0\n",
    "Indtrain=[]\n",
    "Indval=[]\n",
    "Indtest=[]\n",
    "while (i+1)*395<(NbDays-1):\n",
    "    x=395*i\n",
    "    Indtrain.append([x,(x+305)])\n",
    "    Indval.append([(x+319),(x+350)])\n",
    "    Indtest.append([x+364,x+395])\n",
    "    i+=1\n",
    "    \n",
    "\n",
    "#Se restreindre à l'été car pas de pluie??\n",
    "day0=datetime.date(1960,1,1)\n",
    "dayend=datetime.date(2009,12,12)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Definiton of training, validation and test dataset\n",
    "# from dayly indices over a one-year time series\n",
    "\n",
    "suffix_exp = \"exp2\"\n",
    "day_0 = datetime.date(2012,10,1)\n",
    "\n",
    "if suffix_exp == \"exp3\" :\n",
    "    iiVal = 60 - int(dT / 2)\n",
    "    jjVal = iiVal + 20 + int(dT / 2) #int(dT / 2)\n",
    "    \n",
    "    iiTest = jjVal + 10 #90 - int(dT / 2)\n",
    "    jjTest = iiTest + 20 + 2*int(dT / 2) # 110 + int(dT / 2)\n",
    "     \n",
    "    iiTr1 = 0\n",
    "    jjTr1 = iiVal - 10 #50 - int(dT / 2)\n",
    "    \n",
    "    iiTr2 = jjTest + 10 #130 + int(dT / 2)\n",
    "    jjTr2 = 365\n",
    "elif suffix_exp == \"exp2\" :\n",
    "    day_val  = datetime.date(2013,1,1)\n",
    "    iiVal = int((day_val - day_0).days)\n",
    "    jjVal = iiVal + 20 + 2*int(dT / 2) #int(dT / 2)\n",
    "    \n",
    "    day_test_0  = datetime.date(2012,10,22)\n",
    "    day_test_1  = datetime.date(2012,12,2)\n",
    "    iiTest = int((day_test_0 - day_0).days) - int(dT / 2) #90 - int(dT / 2)\n",
    "    jjTest = int((day_test_1 - day_0).days) + int(dT / 2) # 110 + int(dT / 2)\n",
    "     \n",
    "    iiTr1 = jjVal + 10\n",
    "    jjTr1 = 365\n",
    "    \n",
    "    iiTr2 = 365 #130 + int(dT / 2)\n",
    "    jjTr2 = 365\n",
    "elif suffix_exp == \"summer\" : # Summer\n",
    "    day_val  = datetime.date(2013,7,1)\n",
    "    iiVal = int((day_val - day_0).days)\n",
    "    jjVal = iiVal + 20 + 2*int(dT / 2) #int(dT / 2)\n",
    "    \n",
    "    iiTest = jjVal + 10 #90 - int(dT / 2)\n",
    "    jjTest = iiTest + 20 + 2*int(dT / 2) # 110 + int(dT / 2)\n",
    "     \n",
    "    iiTr1 = 0\n",
    "    jjTr1 = iiVal - 10 #50 - int(dT / 2)\n",
    "    \n",
    "    iiTr2 = jjTest + 10 #130 + int(dT / 2)\n",
    "    jjTr2 = 365\n",
    "elif suffix_exp == \"spring\" : # Spring\n",
    "    day_val  = datetime.date(2013,4,1)\n",
    "    iiVal = int((day_val - day_0).days)\n",
    "    jjVal = iiVal + 20 + 2*int(dT / 2) #int(dT / 2)\n",
    "    \n",
    "    iiTest = jjVal + 10 #90 - int(dT / 2)\n",
    "    jjTest = iiTest + 20 + 2*int(dT / 2) # 110 + int(dT / 2)\n",
    "     \n",
    "    iiTr1 = 0\n",
    "    jjTr1 = iiVal - 10 #50 - int(dT / 2)\n",
    "    \n",
    "    iiTr2 = jjTest + 10 #130 + int(dT / 2)\n",
    "    jjTr2 = 365\n",
    "elif suffix_exp == \"winter\" : # Winter\n",
    "    day_val  = datetime.date(2013,1,1)\n",
    "    iiVal = int((day_val - day_0).days)\n",
    "    jjVal = iiVal + 20 + 2*int(dT / 2) #int(dT / 2)\n",
    "    \n",
    "    iiTest = jjVal + 10 #90 - int(dT / 2)\n",
    "    jjTest = iiTest + 20 + 2*int(dT / 2) # 110 + int(dT / 2)\n",
    "     \n",
    "    iiTr1 = 0\n",
    "    jjTr1 = iiVal - 10 #50 - int(dT / 2)\n",
    "    \n",
    "    iiTr2 = jjTest + 10 #130 + int(dT / 2)\n",
    "    jjTr2 = 365\n",
    "elif suffix_exp == \"fall\" : # Fall\n",
    "    day_val  = datetime.date(2012,10,1)\n",
    "    iiVal = int((day_val - day_0).days)\n",
    "    jjVal = iiVal + 20 + 2*int(dT / 2) #int(dT / 2)\n",
    "    \n",
    "    iiTest = jjVal + 10 #90 - int(dT / 2)\n",
    "    jjTest = iiTest + 20 + 2*int(dT / 2) # 110 + int(dT / 2)\n",
    "     \n",
    "    iiTr1 = 0\n",
    "    jjTr1 = iiVal - 10 #50 - int(dT / 2)\n",
    "    \n",
    "    iiTr2 = jjTest + 10 #130 + int(dT / 2)\n",
    "    jjTr2 = 365\n",
    "elif suffix_exp == \"winter2\" : # Winter\n",
    "    day_val  = datetime.date(2013,1,1)\n",
    "    iiVal = int((day_val - day_0).days)\n",
    "    jjVal = iiVal + 20 + 2*int(dT / 2) #int(dT / 2)\n",
    "    \n",
    "    iiTest = jjVal + 30 #90 - int(dT / 2)\n",
    "    jjTest = iiTest + 20 + 2*int(dT / 2) # 110 + int(dT / 2)\n",
    "     \n",
    "    iiTr1 = 0\n",
    "    jjTr1 = iiVal - 10 #50 - int(dT / 2)\n",
    "    \n",
    "    iiTr2 = jjTest + 30 #130 + int(dT / 2)\n",
    "    jjTr2 = 365\n",
    "elif suffix_exp == \"spring2\" : # Spring\n",
    "    day_val  = datetime.date(2013,4,1)\n",
    "    iiVal = int((day_val - day_0).days)\n",
    "    jjVal = iiVal + 20 + 2*int(dT / 2) #int(dT / 2)\n",
    "    \n",
    "    iiTest = jjVal + 30 #90 - int(dT / 2)\n",
    "    jjTest = iiTest + 20 + 2*int(dT / 2) # 110 + int(dT / 2)\n",
    "     \n",
    "    iiTr1 = 0\n",
    "    jjTr1 = iiVal - 10 #50 - int(dT / 2)\n",
    "    \n",
    "    iiTr2 = jjTest + 30 #130 + int(dT / 2)\n",
    "    jjTr2 = 365\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9902ce97",
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "    ####################################################\n",
    "## Generation of training  validationand test dataset\n",
    "## Extraction of time series of dT time steps\n",
    "#NbTraining = 6000#2000\n",
    "#NbTest     = 256#256#500\n",
    "#NbVal = ?\n",
    "    \n",
    "dataTrainingNoNaND = image.extract_patches_2d(dataset[Indtrain[0][0]:Indtrain[0][1],:],(DT,31)) \n",
    "for k in Indtrain[1::]:\n",
    "    d= image.extract_patches_2d(dataset[k[0]:k[1],:],(DT,31))\n",
    "    dataTrainingNoNaND=np.concatenate((dataTrainingNoNaND,d),axis=0)\n",
    "        \n",
    "    \n",
    "dataValNoNaND = image.extract_patches_2d(dataset[Indval[0][0]:Indval[0][1],:],(DT,31))    \n",
    "for k in Indval[1::]:\n",
    "    d= image.extract_patches_2d(dataset[k[0]:k[1],:],(DT,31))\n",
    "    dataValNoNaND=np.concatenate((dataValNoNaND,d),axis=0)\n",
    "print(dataValNoNaND.shape )  \n",
    "    \n",
    "dataTestNoNaND = image.extract_patches_2d(dataset[Indtest[0][0]:Indtest[0][1],:],(DT,31))\n",
    "for k in Indtest[1::]:\n",
    "    d= image.extract_patches_2d(dataset[k[0]:k[1],:],(DT,31))\n",
    "    dataTestNoNaND=np.concatenate((dataTestNoNaND,d),axis=0)\n",
    "print(dataTestNoNaND.shape ) \n",
    "        \n",
    "# create missing data\n",
    "#flagTypeMissData = 0 : Missing data randomly chosen on the patch driven by rateMissingData\n",
    "#flagTypeMissData = 1 : Almost the same\n",
    "#flagTypeMissData = 2 : In each patch, different station are randomly chosen and are masked according to rateMissingData\n",
    "#flagTypeMissData = 3 : The same stations listed in MaskedStations are masked\n",
    "flagTypeMissData = 3\n",
    "if flagTypeMissData == 0:\n",
    "    indRandD         = np.random.permutation(dataTrainingNoNaND.shape[0]*dataTrainingNoNaND.shape[1]*dataTrainingNoNaND.shape[2])\n",
    "    indRandD         = indRandD[0:int(rateMissingData*len(indRandD))]\n",
    "    dataTrainingD    = np.copy(dataTrainingNoNaND).reshape((dataTrainingNoNaND.shape[0]*dataTrainingNoNaND.shape[1]*dataTrainingNoNaND.shape[2],1))\n",
    "    dataTrainingD[indRandD] = float('nan')\n",
    "    dataTrainingD    = np.reshape(dataTrainingD,(dataTrainingNoNaND.shape[0],dataTrainingNoNaND.shape[1],dataTrainingNoNaND.shape[2]))\n",
    "            \n",
    "    indRandD         = np.random.permutation(dataValNoNaND.shape[0]*dataValNoNaND.shape[1]*dataValNoNaND.shape[2])\n",
    "    indRandD         = indRandD[0:int(rateMissingData*len(indRandD))]\n",
    "    dataValD    = np.copy(dataValNoNaND).reshape((dataValNoNaND.shape[0]*dataValNoNaND.shape[1]*dataValNoNaND.shape[2],1))\n",
    "    dataValD[indRandD] = float('nan')\n",
    "    dataValD    = np.reshape(dataValD,(dataValNoNaND.shape[0],dataValNoNaND.shape[1],dataValNoNaND.shape[2]))\n",
    "            \n",
    "            \n",
    "    indRandD         = np.random.permutation(dataTestNoNaND.shape[0]*dataTestNoNaND.shape[1]*dataTestNoNaND.shape[2])\n",
    "    indRandD         = indRandD[0:int(rateMissingData*len(indRandD))]\n",
    "    dataTestD        = np.copy(dataTestNoNaND).reshape((dataTestNoNaND.shape[0]*dataTestNoNaND.shape[1]*dataTestNoNaND.shape[2],1))\n",
    "    dataTestD[indRandD] = float('nan')\n",
    "    dataTestD          = np.reshape(dataTestD,(dataTestNoNaND.shape[0],dataTestNoNaND.shape[1],dataTestNoNaND.shape[2]))\n",
    "\n",
    "    genSuffixObs    = '_ObsRnd_%02d_%02d'%(100*rateMissingData,10*sigNoise**2)\n",
    "        \n",
    "elif flagTypeMissData==1:\n",
    "    time_step_obs   = int(1./(1.-rateMissingData))\n",
    "    dataTrainingD    = np.zeros((dataTrainingNoNaND.shape))\n",
    "    dataTrainingD[:] = float('nan')\n",
    "            \n",
    "    dataValD    = np.zeros((dataValNoNaND.shape))\n",
    "    dataValD[:] = float('nan')\n",
    "            \n",
    "    dataTestD        = np.zeros((dataTestNoNaND.shape))\n",
    "    dataTestD[:]     = float('nan')\n",
    "               \n",
    "    if 1*0:\n",
    "                \n",
    "        dataTrainingD[:,::time_step_obs,:] = dataTrainingNoNaND[:,::time_step_obs,:]\n",
    "        dataValD[:,::time_step_obs,:] = dataValNoNaND[:,::time_step_obs,:]\n",
    "        dataTestD[:,::time_step_obs,:]     = dataTestNoNaND[:,::time_step_obs,:]\n",
    "                    \n",
    "        genSuffixObs    = '_ObsSub_%02d_%02d'%(100*rateMissingData,10*sigNoise**2)\n",
    "    else:\n",
    "        for nn in range(0,dataTrainingD.shape[1],time_step_obs):\n",
    "            indrand = np.random.permutation(dataTrainingD.shape[2])[0:int(0.5*dataTrainingD.shape[1])]\n",
    "            dataTrainingD[:,nn,indrand] = dataTrainingNoNaND[:,nn,indrand]\n",
    "                    \n",
    "        for nn in range(0,dataTrainingD.shape[1],time_step_obs):\n",
    "            indrand = np.random.permutation(dataTrainingD.shape[2])[0:int(0.5*dataTrainingD.shape[1])]\n",
    "            dataValD[:,nn,indrand] = dataValNoNaND[:,nn,indrand]\n",
    "                    \n",
    "        for nn in range(0,dataTrainingD.shape[1],time_step_obs):\n",
    "            indrand = np.random.permutation(dataTrainingD.shape[2])[0:int(0.5*dataTrainingD.shape[1])]\n",
    "            dataTestD[:,nn,indrand] = dataTestNoNaND[:,nn,indrand]\n",
    "\n",
    "        genSuffixObs    = '_ObsSubRnd_%02d_%02d'%(100*rateMissingData,10*sigNoise**2)\n",
    "        \n",
    "elif flagTypeMissData == 2 :\n",
    "    #\n",
    "    Nbtraining=13110\n",
    "    Nbval=506\n",
    "    Nbtest=506\n",
    "            \n",
    "    ratemissingdata_space = 0.15\n",
    "    time_step_obs   = int(1./(1.-rateMissingData))\n",
    "    dataTrainingD    = np.zeros(([Nbtraining,dataTrainingNoNaND.shape[1],dataTrainingNoNaND.shape[2]]))\n",
    "    dataTrainingD[:] = float('nan')\n",
    "    dataTrainingNoNaND2    = np.zeros(([Nbtraining,dataTrainingNoNaND.shape[1],dataTrainingNoNaND.shape[2]]))\n",
    "    dataTrainingNoNaND2[:] = float('nan')\n",
    "            \n",
    "    dataValD    = np.zeros(([Nbval,dataTrainingNoNaND.shape[1],dataTrainingNoNaND.shape[2]]))           \n",
    "    dataValD[:] = float('nan')\n",
    "    dataValNoNaND2=np.zeros(([Nbval,dataTrainingNoNaND.shape[1],dataTrainingNoNaND.shape[2]]))\n",
    "    dataValNoNaND2[:] = float('nan')\n",
    "            \n",
    "    dataTestD        = np.zeros(([Nbtest,dataTrainingNoNaND.shape[1],dataTrainingNoNaND.shape[2]]))\n",
    "    dataTestD[:]     = float('nan') \n",
    "    dataTestNoNaND2 =np.zeros(([Nbtest,dataTrainingNoNaND.shape[1],dataTrainingNoNaND.shape[2]]))\n",
    "    dataTestNoNaND2[:] = float('nan')\n",
    "            \n",
    "    ind=0\n",
    "    print(dataTrainingD.shape)\n",
    "    while ind<Nbtraining:\n",
    "        indrand=np.random.permutation(dataTrainingD.shape[2])[0:int((1-ratemissingdata_space)*dataTrainingD.shape[2])]\n",
    "        dataTrainingD[ind,:,indrand]=dataTrainingNoNaND[ind%dataTrainingNoNaND.shape[0],:,indrand]\n",
    "        dataTrainingNoNaND2[ind,:,:]=dataTrainingNoNaND[ind%dataTrainingNoNaND.shape[0],:,:]\n",
    "                \n",
    "        if ind <Nbval:\n",
    "            indrand2=np.random.permutation(dataTrainingD.shape[2])[0:int((1-ratemissingdata_space)*dataTrainingD.shape[2])]\n",
    "            dataValD[ind,:,indrand2]=dataValNoNaND[ind%dataValNoNaND.shape[0],:,indrand2]\n",
    "            dataValNoNaND2[ind,:,:]=dataValNoNaND[ind%dataValNoNaND.shape[0],:,:]\n",
    "                \n",
    "            indrand3=np.random.permutation(dataTrainingD.shape[2])[0:int((1-ratemissingdata_space)*dataTrainingD.shape[2])]\n",
    "            dataTestD[ind,:,indrand3]=dataTestNoNaND[ind%dataTestNoNaND.shape[0],:,indrand3]\n",
    "            dataTestNoNaND2[ind,:,:] = dataTestNoNaND[ind%dataTestNoNaND.shape[0],:,:]\n",
    "        ind+=1\n",
    "                \n",
    "    dataTrainingNoNaND =dataTrainingNoNaND2\n",
    "    dataValNoNaND = dataValNoNaND2\n",
    "    dataTestNoNaND =dataTestNoNaND2        \n",
    "    genSuffixObs    = '_ObsSubRnd_%02d_%02d'%(100*rateMissingData,10*sigNoise**2) \n",
    "        \n",
    "#mask only on specific station\n",
    " else :\n",
    "    MaskedStations=[2,4,16,25]\n",
    "            \n",
    "    dataTrainingD    = np.zeros((dataTrainingNoNaND.shape))\n",
    "    dataTrainingD[:] = float('nan')\n",
    "            \n",
    "    dataValD    = np.zeros((dataValNoNaND.shape))\n",
    "    dataValD[:] = float('nan')\n",
    "            \n",
    "    dataTestD        = np.zeros((dataTestNoNaND.shape))\n",
    "    dataTestD[:]     = float('nan')\n",
    "    print(dataTrainingNoNaND[0,:,:])  \n",
    "    for i in range(31):\n",
    "        dataTrainingD[:,:,i] = dataTrainingNoNaND[:,:,i]\n",
    "        dataValD[:,:,i] = dataValNoNaND[:,:,i]\n",
    "        dataTestD[:,:,i] = dataTestNoNaND[:,:,i]\n",
    "    for i in MaskedStations:\n",
    "        dataTrainingD[:,:,i-1] = float('nan')\n",
    "        dataValD[:,:,i-1] = float('nan')\n",
    "        dataTestD[:,:,i-1] = float('nan')\n",
    "    genSuffixObs    = '_ObsSubRnd_%02d_%02d'%(100*rateMissingData,10*sigNoise**2)\n",
    "    print(dataTrainingNoNaND[0,:,:])    \n",
    "print('... Data type: '+genSuffixObs)\n",
    "    #for nn in range(0,dataTraining.shape[1],time_step_obs):\n",
    "    #    dataTraining[:,::time_step_obs,:] = dataTrainingNoNaN[:,::time_step_obs,:]\n",
    "    #dataTest    = np.zeros((dataTestNoNaN.shape))\n",
    "    #dataTest[:] = float('nan')\n",
    "    #dataTest[:,::time_step_obs,:] = dataTestNoNaN[:,::time_step_obs,:]\n",
    "        \n",
    "# set to NaN patch boundaries    \n",
    "if 1*0:\n",
    "    dataTrainingD[:,0:10,:] =  float('nan')\n",
    "    dataValD[:,0:10,:] =  float('nan')\n",
    "    dataTestD[:,0:10,:]     =  float('nan')\n",
    "    dataTrainingD[:,dT-10:dT,:] =  float('nan')\n",
    "    dataValD[:,dT-10:dT,:] =  float('nan')\n",
    "    dataTestD[:,dT-10:dT,:]     =  float('nan')\n",
    "            \n",
    "                                \n",
    "# mask for NaN\n",
    "maskTrainingD = (dataTrainingD == dataTrainingD).astype('float')\n",
    "maskValD = (dataValD == dataValD).astype('float')\n",
    "maskTestD     = ( dataTestD    ==  dataTestD   ).astype('float')\n",
    "            \n",
    "dataTrainingD = np.nan_to_num(dataTrainingD)\n",
    "        \n",
    "dataValD = np.nan_to_num(dataValD)\n",
    "dataTestD     = np.nan_to_num(dataTestD)\n",
    "            \n",
    "    # Permutation to have channel as #1 component\n",
    "dataTrainingD      = np.moveaxis(dataTrainingD,-1,1)\n",
    "maskTrainingD      = np.moveaxis(maskTrainingD,-1,1)\n",
    "dataTrainingNoNaND = np.moveaxis(dataTrainingNoNaND,-1,1)\n",
    "        \n",
    "dataValD      = np.moveaxis(dataValD,-1,1)\n",
    "maskValD      = np.moveaxis(maskValD,-1,1)\n",
    "dataValNoNaND = np.moveaxis(dataValNoNaND,-1,1)\n",
    "            \n",
    "dataTestD      = np.moveaxis(dataTestD,-1,1)\n",
    "maskTestD      = np.moveaxis(maskTestD,-1,1)\n",
    "dataTestNoNaND = np.moveaxis(dataTestNoNaND,-1,1)\n",
    "            \n",
    "# set to NaN patch boundaries\n",
    "#dataTraining[:,0:5,:] =  dataTrainingNoNaN[:,0:5,:]\n",
    "#dataTest[:,0:5,:]     =  dataTestNoNaN[:,0:5,:]\n",
    "    \n",
    "############################################\n",
    "## raw data\n",
    "X_trainD         = dataTrainingNoNaND\n",
    "        \n",
    "X_train_missingD = dataTrainingD\n",
    "mask_trainD      = maskTrainingD\n",
    "        \n",
    "X_valD         = dataValNoNaND\n",
    "X_val_missingD = dataValD\n",
    "mask_valD      = maskValD\n",
    "        \n",
    "X_testD         = dataTestNoNaND\n",
    "X_test_missingD = dataTestD\n",
    "mask_testD      = maskTestD\n",
    "            \n",
    "############################################\n",
    "## normalized data wrt to each measurement station\n",
    "        \n",
    "if flagTypeMissData ==2 :\n",
    "    mean2 = np.mean(X_train_missingD[:],0)\n",
    "    mean2mask = np.mean(mask_trainD[:],0)\n",
    "\n",
    "\n",
    "    mean3 = np.mean(mean2,1)\n",
    "    mean3 = mean3.reshape(31,1)\n",
    "    print(mean3)\n",
    "    mean3mask = np.mean(mean2mask,1)\n",
    "    mean3mask = mean3mask.reshape(31,1)\n",
    "    print(mean3mask)\n",
    "    meanTr          = mean3/mean3mask\n",
    "    print(meanTr)\n",
    "    mean2true = np.mean(X_trainD[:],0)\n",
    "    mean3true = np.mean(mean2true,1)\n",
    "    mean3true = mean3true.reshape(31,1)\n",
    "    meanTrtrue = mean3true\n",
    "    print(meanTrtrue)\n",
    "            \n",
    "    meansquaretrue = np.mean( (X_trainD-meanTrtrue)**2,0)\n",
    "    meansquare2true = np.mean(meansquaretrue,1)\n",
    "    meansquare2true=meansquare2true.reshape(31,1)\n",
    "    stdTrtrue           = np.sqrt(meansquare2true )\n",
    "    print(stdTrtrue)\n",
    "            \n",
    "            \n",
    "    x_train_missingD = X_train_missingD - meanTr*mask_trainD\n",
    "            \n",
    "    x_val_missingD = X_val_missingD - meanTr*mask_valD\n",
    "    x_test_missingD  = X_test_missingD - meanTr*mask_testD\n",
    "            \n",
    "    # scale wrt to each station\n",
    "    meansquare = np.mean( X_train_missingD**2,0)\n",
    "    meansquare2 = np.mean(meansquare,1)\n",
    "    meansquare2=meansquare2.reshape(31,1)\n",
    "    stdTr           = np.sqrt(meansquare2 / mean3mask)\n",
    "            \n",
    "    x_train_missingD = x_train_missingD / stdTr\n",
    "    x_val_missingD = x_val_missingD / stdTr\n",
    "    x_test_missingD  = x_test_missingD / stdTr\n",
    "            \n",
    "    x_trainD = (X_trainD - meanTr) / stdTr\n",
    "    x_valD = (X_valD - meanTr) / stdTr\n",
    "    x_testD  = (X_testD - meanTr) / stdTr\n",
    "            \n",
    "    print(np.mean(x_train_missingD))\n",
    "    print(np.mean(x_trainD))\n",
    "    print(np.mean(x_val_missingD))\n",
    "    print(np.mean(x_valD))\n",
    "    print(np.mean(x_test_missingD))\n",
    "    print(np.mean(x_testD))\n",
    "            \n",
    "            \n",
    "               \n",
    "elif flagTypeMissData==3 :\n",
    "    mean2 = np.mean(X_train_missingD[:],0)\n",
    "    mean3 = np.mean(mean2,1)\n",
    "    mean3 = mean3.reshape(31,1)\n",
    "    meanTr = mean3\n",
    "    x_train_missingD = X_train_missingD - meanTr\n",
    "    x_val_missingD = X_val_missingD - meanTr\n",
    "    x_test_missingD  = X_test_missingD - meanTr\n",
    "    meansquare = np.mean( x_train_missingD**2,0)\n",
    "    meansquare2 = np.mean(meansquare,1)\n",
    "    meansquare2=meansquare2.reshape(31,1)\n",
    "            \n",
    "    stdTr           = np.sqrt(meansquare2)\n",
    "            \n",
    "    for i in MaskedStations :\n",
    "        stdTr[i-1] =1\n",
    "    print(stdTr)\n",
    "    print(X_trainD[0,:,:])\n",
    "    x_train_missingD = x_train_missingD / stdTr\n",
    "    x_val_missingD = x_val_missingD / stdTr\n",
    "    x_test_missingD  = x_test_missingD / stdTr\n",
    "    mean2true = np.mean(X_trainD[:],0)\n",
    "    mean3true = np.mean(mean2true,1)\n",
    "    mean3true = mean3true.reshape(31,1)\n",
    "    meanTrtrue = mean3true\n",
    "            \n",
    "    meansquaretrue = np.mean( (X_trainD-meanTrtrue)**2,0)\n",
    "    meansquare2true = np.mean(meansquaretrue,1)\n",
    "    meansquare2true=meansquare2true.reshape(31,1)\n",
    "    stdTrtrue           = np.sqrt(meansquare2true )\n",
    "    print(stdTrtrue)\n",
    "    print(meanTrtrue)\n",
    "            \n",
    "    x_trainD = (X_trainD - meanTrtrue) / stdTrtrue\n",
    "    x_valD = (X_valD - meanTrtrue) / stdTrtrue\n",
    "    x_testD  = (X_testD - meanTrtrue) / stdTrtrue\n",
    "            \n",
    "            \n",
    "            \n",
    "else : \n",
    "    mean2 = np.mean(X_train_missingD[:],0)\n",
    "    mean2mask = np.mean(mask_trainD[:],0)\n",
    "            \n",
    "\n",
    "    mean3 = np.mean(mean2,1)\n",
    "    mean3 = mean3.reshape(31,1)\n",
    "    print(mean3)\n",
    "    mean3mask = np.mean(mean2mask,1)\n",
    "    mean3mask = mean3mask.reshape(31,1)\n",
    "    print(mean3mask)\n",
    "    meanTr          = mean3/mean3mask\n",
    "    print(meanTr)\n",
    "            \n",
    "    x_train_missingD = X_train_missingD - meanTr*mask_trainD\n",
    "    x_val_missingD = X_val_missingD - meanTr\n",
    "    x_test_missingD  = X_test_missingD - meanTr\n",
    "            \n",
    "    # scale wrt to each station\n",
    "    meansquare = np.mean( X_train_missingD**2,0)\n",
    "    meansquare2 = np.mean(meansquare,1)\n",
    "    meansquare2=meansquare2.reshape(31,1)\n",
    "    stdTr           = np.sqrt(meansquare2 / mean3mask)\n",
    "            \n",
    "    x_train_missingD = x_train_missingD / stdTr\n",
    "    x_val_missingD = x_val_missingD / stdTr\n",
    "    x_test_missingD  = x_test_missingD / stdTr\n",
    "            \n",
    "    x_trainD = (X_trainD - meanTr) / stdTr\n",
    "    x_valD = (X_valD - meanTr) / stdTr\n",
    "    x_testD  = (X_testD - meanTr) / stdTr\n",
    "            \n",
    "# Generate noisy observsation\n",
    "        \n",
    "#X_train_obsD = X_train_missingD + sigNoise * maskTrainingD * np.random.randn(X_train_missingD.shape[0],X_train_missingD.shape[1],X_train_missingD.shape[2])\n",
    "#X_val_obsD = X_val_missingD + sigNoise * maskValD * np.random.randn(X_val_missingD.shape[0],X_val_missingD.shape[1],X_val_missingD.shape[2])\n",
    "#X_test_obsD  = X_test_missingD  + sigNoise * maskTestD * np.random.randn(X_test_missingD.shape[0],X_test_missingD.shape[1],X_test_missingD.shape[2])\n",
    "            \n",
    "#x_train_obsD = (X_train_obsD - meanTr) / stdTr\n",
    "#x_val_obsD = (X_val_obsD - meanTr) / stdTr\n",
    "#x_test_obsD  = (X_test_obsD - meanTr) / stdTr\n",
    "        \n",
    "#Without noise :\n",
    "X_train_obsD = X_train_missingD \n",
    "X_val_obsD = X_val_missingD \n",
    "X_test_obsD  = X_test_missingD\n",
    "        \n",
    "x_train_obsD = x_train_missingD\n",
    "x_val_obsD = x_val_missingD\n",
    "x_test_obsD = x_test_missingD \n",
    "        \n",
    "print('..... Training dataset: %dx%dx%d'%(x_train_missingD.shape[0],x_trainD.shape[1],x_trainD.shape[2]))\n",
    "print('..... Validation dataset: %dx%dx%d'%(x_valD.shape[0],x_valD.shape[1],x_valD.shape[2]))\n",
    "print('..... Test dataset    : %dx%dx%d'%(x_testD.shape[0],x_testD.shape[1],x_testD.shape[2]))\n",
    "            \n",
    "\n",
    "print('........ Initialize interpolated states')\n",
    "## Initial interpolation\n",
    "flagInit = 0\n",
    "            \n",
    "if flagInit == 0: \n",
    "    X_train_InitD = mask_trainD * X_train_obsD + (1. - mask_trainD) * (np.zeros(X_train_missingD.shape) + meanTr)\n",
    "    X_val_InitD = mask_valD * X_val_obsD + (1. - mask_valD) * (np.zeros(X_val_missingD.shape) + meanTr)\n",
    "    X_test_InitD  = mask_testD * X_test_obsD + (1. - mask_testD) * (np.zeros(X_test_missingD.shape) + meanTr)\n",
    "    else:\n",
    "    X_train_InitD = np.zeros(X_trainD.shape)\n",
    "    for ii in range(0,X_trainD.shape[0]):\n",
    "        # Initial linear interpolation for each component\n",
    "        XInitD = np.zeros((X_trainD.shape[1],X_trainD.shape[2]))\n",
    "           \n",
    "        for kk in range(0,mask_trainD.shape[1]):\n",
    "            indt  = np.where( mask_trainD[ii,kk,:] == 1.0 )[0]\n",
    "            indt_ = np.where( mask_trainD[ii,kk,:] == 0.0 )[0]\n",
    "           \n",
    "            if len(indt) > 1:\n",
    "                indt_[ np.where( indt_ < np.min(indt)) ] = np.min(indt)\n",
    "                indt_[ np.where( indt_ > np.max(indt)) ] = np.max(indt)\n",
    "                fkk = scipy.interpolate.interp1d(indt, X_train_obsD[ii,kk,indt])\n",
    "                XInitD[kk,indt]  = X_train_obsD[ii,kk,indt]\n",
    "                XInitD[kk,indt_] = fkk(indt_)\n",
    "            else:\n",
    "                XInitD = XInitD + meanTr\n",
    "            \n",
    "        X_train_InitD[ii,:,:] = XInitD\n",
    "            \n",
    "    X_val_InitD = np.zeros(X_valD.shape)\n",
    "    for ii in range(0,X_valD.shape[0]):\n",
    "        # Initial linear interpolation for each component\n",
    "        XInitD = np.zeros((X_valD.shape[1],X_valD.shape[2]))\n",
    "           \n",
    "        for kk in range(0,mask_valD.shape[1]):\n",
    "            indt  = np.where( mask_valD[ii,kk,:] == 1.0 )[0]\n",
    "            indt_ = np.where( mask_valD[ii,kk,:] == 0.0 )[0]\n",
    "           \n",
    "            if len(indt) > 1:\n",
    "                indt_[ np.where( indt_ < np.min(indt)) ] = np.min(indt)\n",
    "                indt_[ np.where( indt_ > np.max(indt)) ] = np.max(indt)\n",
    "                fkk = scipy.interpolate.interp1d(indt, X_val_obsD[ii,kk,indt])\n",
    "                XInitD[kk,indt]  = X_val_obsD[ii,kk,indt]\n",
    "                XInitD[kk,indt_] = fkk(indt_)\n",
    "            else:\n",
    "                XInitD = XInitD + meanTr\n",
    "            \n",
    "        X_val_InitD[ii,:,:] = XInitD\n",
    "            \n",
    "    X_test_InitD = np.zeros(X_testD.shape)\n",
    "    for ii in range(0,X_testD.shape[0]):\n",
    "        # Initial linear interpolation for each component\n",
    "        XInit = np.zeros((X_testD.shape[1],X_testD.shape[2]))\n",
    "            \n",
    "        for kk in range(0,X_testD.shape[1]):\n",
    "            indt  = np.where( mask_testD[ii,kk,:] == 1.0 )[0]\n",
    "            indt_ = np.where( mask_testD[ii,kk,:] == 0.0 )[0]\n",
    "            \n",
    "            if len(indt) > 1:\n",
    "                indt_[ np.where( indt_ < np.min(indt)) ] = np.min(indt)\n",
    "                indt_[ np.where( indt_ > np.max(indt)) ] = np.max(indt)\n",
    "                fkk = scipy.interpolate.interp1d(indt, X_test_obsD[ii,kk,indt])\n",
    "                XInit[kk,indt]  = X_test_obsD[ii,kk,indt]\n",
    "                XInit[kk,indt_] = fkk(indt_)\n",
    "            else:\n",
    "                XInit = XInit + meanTr\n",
    "        \n",
    "        X_test_InitD[ii,:,:] = XInit\n",
    "        #plt.figure()\n",
    "        #plt.figure()\n",
    "        #plt.plot(YObs[0:200,1],'r.')\n",
    "        #plt.plot(XGT[0:200,1],'b-')\n",
    "        #plt.plot(XInit[0:200,1],'k-')\n",
    "                        \n",
    "x_train_InitD = ( X_train_InitD - meanTr ) / stdTr\n",
    "x_val_InitD = ( X_val_InitD - meanTr ) / stdTr\n",
    "x_test_InitD = ( X_test_InitD - meanTr ) / stdTr\n",
    "        \n",
    "# reshape to dT-1 for time dimension\n",
    "DT = DT-1\n",
    "X_train_obsD        = X_train_obsD[:,:,0:DT]\n",
    "X_trainD            = X_trainD[:,:,0:DT]\n",
    "X_train_missingD    = X_train_missingD[:,:,0:DT]\n",
    "mask_trainD         = mask_trainD[:,:,0:DT]\n",
    "            \n",
    "x_train_obsD        = x_train_obsD[:,:,0:DT]\n",
    "x_trainD            = x_trainD[:,:,0:DT]\n",
    "x_train_InitD       = x_train_InitD[:,:,0:DT]\n",
    "X_train_InitD       = X_train_InitD[:,:,0:DT]\n",
    "        \n",
    "X_val_obsD        = X_val_obsD[:,:,0:DT]\n",
    "X_valD            = X_valD[:,:,0:DT]\n",
    "X_val_missingD    = X_val_missingD[:,:,0:DT]\n",
    "mask_valD         = mask_valD[:,:,0:DT]\n",
    "            \n",
    "x_val_obsD        = x_val_obsD[:,:,0:DT]\n",
    "x_valD            = x_valD[:,:,0:DT]\n",
    "x_val_InitD       = x_val_InitD[:,:,0:DT]\n",
    "X_val_InitD       = X_val_InitD[:,:,0:DT]\n",
    "\n",
    "X_test_obsD        = X_test_obsD[:,:,0:DT]\n",
    "X_testD            = X_testD[:,:,0:DT]\n",
    "X_test_missingD    = X_test_missingD[:,:,0:DT]\n",
    "mask_testD         = mask_testD[:,:,0:DT]\n",
    "\n",
    "x_test_obsD        = x_test_obsD[:,:,0:DT]\n",
    "x_testD            = x_testD[:,:,0:DT]\n",
    "x_test_InitD       = x_test_InitD[:,:,0:DT]\n",
    "X_test_InitD       = X_test_InitD[:,:,0:DT]\n",
    "\n",
    "print('..... Training dataset: %dx%dx%d'%(x_trainD.shape[0],x_trainD.shape[1],x_trainD.shape[2]))\n",
    "print('..... Validation dataset: %dx%dx%d'%(x_valD.shape[0],x_valD.shape[1],x_valD.shape[2]))\n",
    "print('..... Test dataset    : %dx%dx%d'%(x_testD.shape[0],x_testD.shape[1],x_testD.shape[2]))\n",
    "    \n",
    "    \n",
    "    \n",
    "\"\"\"\n",
    "############################################## Data generation ###############################################################\n",
    "print('........ Random seed set to 100')\n",
    "np.random.seed(100)\n",
    "torch.manual_seed(100)\n",
    "\n",
    "genSuffixObs = ''\n",
    "\n",
    "dirREF = \"/gpfsstore/rech/yrf/commun/NATL60/GULFSTREAM/\"\n",
    "if os.path.isdir(dirREF) == True :            \n",
    "    dirDATA = dirREF\n",
    "    dirSAVE = ' ./SSVVNATL60_ChckPt'\n",
    "else:\n",
    "    dirDATA = '/users/local/DATA/DataNATL60/GULFSTREAM/'\n",
    "    dirSAVE = './SSVVNATL60_ChckPt'\n",
    "\n",
    "ncfile = Dataset(dirDATA+\"ref/NATL60-CJM165_GULFSTREAM_ssh_y2013.1y.nc\",\"r\")\n",
    "qHR    = ncfile.variables['ssh'][:]\n",
    "ncfile.close()\n",
    "\n",
    "ncfile = Dataset(dirDATA+\"ref/NATL60-CJM165_GULFSTREAM_sst_y2013.1y.nc\",\"r\")\n",
    "qSST   = ncfile.variables['sst'][:]\n",
    "ncfile.close()\n",
    "\n",
    "ncfile = Dataset(dirDATA+\"ref/NATL60-CJM165_GULFSTREAM_u_y2013.1y.nc\",\"r\")\n",
    "q_u = ncfile.variables['u'][:]\n",
    "ncfile = Dataset(dirDATA+\"ref/NATL60-CJM165_GULFSTREAM_v_y2013.1y.nc\",\"r\")\n",
    "q_v = ncfile.variables['v'][:]\n",
    "\n",
    "ncfile = Dataset(dirDATA+'ref/NATL60-CJM165_GULFSTREAM_'+flag_vv+'_y2013.1y.nc',\"r\")\n",
    "q_vv    = ncfile.variables['vv'][:]\n",
    "ncfile.close()\n",
    "\n",
    "if flagSWOTData == True :\n",
    "    print('.... Use SWOT+4-nadir dataset')\n",
    "    #genFilename  = 'resInterpSLAwSWOT_Exp3_NewSolver_'+str('%03d'%(W))+'x'+str('%03d'%(W))+'x'+str('%02d'%(dT))\n",
    "    # OI data using a noise-free OSSE (ssh_mod variable)\n",
    "    ncfile = Dataset(dirDATA+\"oi/ssh_NATL60_swot_4nadir.nc\",\"r\")\n",
    "    qOI    = ncfile.variables['ssh_mod'][:]\n",
    "    ncfile.close()\n",
    "\n",
    "    # OI data using a noise-free OSSE (ssh_mod variable)\n",
    "    ncfile = Dataset(dirDATA+\"data/dataset_nadir_0d_swot.nc\",\"r\")\n",
    "    ncfile = Dataset(dirDATA+\"data_new/dataset_nadir_0d_swot.nc\",\"r\")\n",
    "    \n",
    "    qMask   = ncfile.variables['ssh_mod'][:]\n",
    "    qMask   = 1.0-qMask.mask.astype(float)\n",
    "    \n",
    "    if 1*1 :\n",
    "        #qObs   = ncfile.variables['ssh_obs'][:]\n",
    "        qObs   = ncfile.variables['ssh_mod'][:]\n",
    "    else:\n",
    "        qObs   = qHR * qMask       \n",
    "    ncfile.close()\n",
    "    \n",
    "    mse_obs_true = np.sum( qMask * (qObs-qHR)**2  ) / np.sum( qMask ) \n",
    "    print('... RMSE obs-true %f -- %f'%( mse_obs_true , 1.0 - mse_obs_true / np.var(qHR) ) )\n",
    "\n",
    "else:\n",
    "    #genFilename  = 'resInterp4NadirSLAwOInoSST_'+str('%03d'%(W))+'x'+str('%03d'%(W))+'x'+str('%02d'%(dT))\n",
    "    print('.... Use 4-nadir dataset')\n",
    "    # OI data using a noise-free OSSE (ssh_mod variable)\n",
    "    ncfile = Dataset(dirDATA+\"oi/ssh_NATL60_4nadir.nc\",\"r\")\n",
    "    qOI    = ncfile.variables['ssh_mod'][:]\n",
    "    ncfile.close()\n",
    "\n",
    "    # OI data using a noise-free OSSE (ssh_mod variable)\n",
    "    ncfile = Dataset(dirDATA+\"data/dataset_nadir_0d.nc\",\"r\")\n",
    "    \n",
    "    qMask   = ncfile.variables['ssh_mod'][:]\n",
    "    qMask   = 1.0-qMask.mask.astype(float)\n",
    "    qObs   = ncfile.variables['ssh_mod'][:]\n",
    "\n",
    "    mse_obs_true = np.sum( qMask * (qObs-qHR)**2  ) / np.sum( qMask ) \n",
    "    print('... RMSE obs-true %f -- %f'%( mse_obs_true , 1.0 - mse_obs_true / np.var(qHR) ) )\n",
    "    ncfile.close()\n",
    "\n",
    "print('----- MSE OI: %.3f'%np.mean((qOI-qHR)**2))\n",
    "print()\n",
    "\n",
    "## extraction of patches from the SSH field\n",
    "#NoRndPatches = False  \n",
    "#if ( Nbpatches == 1 ) & ( W == 200 ):\n",
    "#NoRndPatches = True\n",
    "print('... No random seed for the extraction of patches')\n",
    "\n",
    "qHR   = qHR[:,0:200,0:200]\n",
    "qOI   = qOI[:,0:200,0:200]\n",
    "qMask = qMask[:,0:200,0:200]\n",
    "qSST = qSST[:,0:200,0:200]\n",
    "q_u = q_u[:,0:200,0:200]\n",
    "q_v = q_v[:,0:200,0:200]\n",
    "q_vv = q_vv[:,0:200,0:200]\n",
    "qObs = qObs[:,0:200,0:200]\n",
    "qObs[ np.isnan(qObs) ] = 0.\n",
    "\n",
    "if width_med_filt_spatial > 1 :\n",
    "    q_vv = ndimage.median_filter(q_vv,size=(width_med_filt_temp,width_med_filt_spatial,width_med_filt_spatial))\n",
    "    q_v = ndimage.median_filter(q_v,size=(width_med_filt_temp,width_med_filt_spatial,width_med_filt_spatial))\n",
    "    q_u = ndimage.median_filter(q_u,size=(width_med_filt_temp,width_med_filt_spatial,width_med_filt_spatial))\n",
    "\n",
    "def extract_SpaceTimePatches(q,i1,i2,W,dT,rnd1,rnd2,D=1):\n",
    "    dataTraining  = image.extract_patches_2d(np.moveaxis(q[i1:i2,::D,::D], 0, -1),(W,W),max_patches=Nbpatches,random_state=rnd1)\n",
    "    dataTraining  = np.moveaxis(dataTraining, -1, 1)\n",
    "    dataTraining  = dataTraining.reshape((Nbpatches,dataTraining.shape[1],W*W)) \n",
    "    \n",
    "    dataTraining  = image.extract_patches_2d(dataTraining,(Nbpatches,dT),max_patches=None)\n",
    "\n",
    "    dataTraining  = dataTraining.reshape((dataTraining.shape[0],dataTraining.shape[1],dT,W,W)) \n",
    "    dataTraining  = np.moveaxis(dataTraining, 0, -1)\n",
    "    dataTraining  = np.moveaxis(dataTraining, 0, -1)\n",
    "    dataTraining  = dataTraining.reshape((dT,W,W,dataTraining.shape[3]*dataTraining.shape[4])) \n",
    "    dataTraining  = np.moveaxis(dataTraining, -1, 0)\n",
    "    return dataTraining     \n",
    "\n",
    "# training dataset\n",
    "if jjTr1-iiTr1 >= dT :\n",
    "    dataTraining1     = extract_SpaceTimePatches(qHR,iiTr1,jjTr1,W,dT,rnd1,rnd2,dx)\n",
    "    dataTrainingMask1 = extract_SpaceTimePatches(qMask,iiTr1,jjTr1,W,dT,rnd1,rnd2,dx)\n",
    "    dataTrainingOI1   = extract_SpaceTimePatches(qOI,iiTr1,jjTr1,W,dT,rnd1,rnd2,dx)\n",
    "    dataTrainingSST1 = extract_SpaceTimePatches(qSST,iiTr1,jjTr1,W,dT,rnd1,rnd2,dx)\n",
    "    dataTraining_u1 = extract_SpaceTimePatches(q_u,iiTr1,jjTr1,W,dT,rnd1,rnd2,dx)\n",
    "    dataTraining_v1 = extract_SpaceTimePatches(q_v,iiTr1,jjTr1,W,dT,rnd1,rnd2,dx)\n",
    "    dataTrainingObs1   = extract_SpaceTimePatches(qObs,iiTr1,jjTr1,W,dT,rnd1,rnd2,dx)\n",
    "    dataTraining_vv1 = extract_SpaceTimePatches(q_vv,iiTr1,jjTr1,W,dT,rnd1,rnd2,dx)\n",
    "\n",
    "if jjTr2-iiTr2 >= dT :\n",
    "    dataTraining2     = extract_SpaceTimePatches(qHR,iiTr2,jjTr2,W,dT,rnd1,rnd2,dx)\n",
    "    dataTrainingMask2 = extract_SpaceTimePatches(qMask,iiTr2,jjTr2,W,dT,rnd1,rnd2,dx)\n",
    "    dataTrainingOI2  = extract_SpaceTimePatches(qOI,iiTr2,jjTr2,W,dT,rnd1,rnd2,dx)\n",
    "    dataTrainingSST2  = extract_SpaceTimePatches(qSST,iiTr2,jjTr2,W,dT,rnd1,rnd2,dx)\n",
    "    dataTraining_u2 = extract_SpaceTimePatches(q_u,iiTr2,jjTr2,W,dT,rnd1,rnd2,dx)\n",
    "    dataTraining_v2 = extract_SpaceTimePatches(q_v,iiTr2,jjTr2,W,dT,rnd1,rnd2,dx)\n",
    "    dataTraining_vv2 = extract_SpaceTimePatches(q_vv,iiTr2,jjTr2,W,dT,rnd1,rnd2,dx)\n",
    "    dataTrainingObs2  = extract_SpaceTimePatches(qObs,iiTr2,jjTr2,W,dT,rnd1,rnd2,dx)\n",
    "\n",
    "if jjTr1-iiTr1 < dT :\n",
    "    dataTraining      = dataTraining2\n",
    "    dataTrainingMask  = dataTrainingMask2\n",
    "    dataTrainingOI    = dataTrainingOI2    \n",
    "    dataTrainingSST   = dataTrainingSST2\n",
    "    dataTraining_u    = dataTraining_u2\n",
    "    dataTraining_v    = dataTraining_v2\n",
    "    dataTraining_vv   = dataTraining_vv2\n",
    "    dataTrainingObs    = dataTrainingObs2  \n",
    "elif jjTr2-iiTr2 < dT :\n",
    "    dataTraining      = dataTraining1\n",
    "    dataTrainingMask  = dataTrainingMask1\n",
    "    dataTrainingOI    = dataTrainingOI1    \n",
    "    dataTrainingSST   = dataTrainingSST1\n",
    "    dataTraining_u    = dataTraining_u1\n",
    "    dataTraining_v    = dataTraining_v1\n",
    "    dataTraining_vv    = dataTraining_vv1\n",
    "    dataTrainingObs    = dataTrainingObs1  \n",
    "else:\n",
    "    dataTraining      = np.concatenate((dataTraining1,dataTraining2),axis=0)\n",
    "    dataTrainingMask  = np.concatenate((dataTrainingMask1,dataTrainingMask2),axis=0)\n",
    "    dataTrainingOI    = np.concatenate((dataTrainingOI1,dataTrainingOI2),axis=0)\n",
    "    dataTrainingSST    = np.concatenate((dataTrainingSST1,dataTrainingSST2),axis=0)\n",
    "    dataTraining_u    = np.concatenate((dataTraining_u1,dataTraining_u2),axis=0)\n",
    "    dataTraining_v    = np.concatenate((dataTraining_v1,dataTraining_v2),axis=0)\n",
    "    dataTraining_vv    = np.concatenate((dataTraining_vv1,dataTraining_vv2),axis=0)\n",
    "    dataTrainingObs    = np.concatenate((dataTrainingObs1,dataTrainingObs2),axis=0)\n",
    "\n",
    "# test dataset\n",
    "dataTest     = extract_SpaceTimePatches(qHR,iiTest,jjTest,W,dT,rnd1,rnd2,dx)\n",
    "dataTestMask = extract_SpaceTimePatches(qMask,iiTest,jjTest,W,dT,rnd1,rnd2,dx)\n",
    "dataTestOI   = extract_SpaceTimePatches(qOI,iiTest,jjTest,W,dT,rnd1,rnd2,dx)\n",
    "dataTestSST  = extract_SpaceTimePatches(qSST,iiTest,jjTest,W,dT,rnd1,rnd2,dx)\n",
    "dataTest_u  = extract_SpaceTimePatches(q_u,iiTest,jjTest,W,dT,rnd1,rnd2,dx)\n",
    "dataTest_v  = extract_SpaceTimePatches(q_v,iiTest,jjTest,W,dT,rnd1,rnd2,dx)\n",
    "dataTest_vv  = extract_SpaceTimePatches(q_vv,iiTest,jjTest,W,dT,rnd1,rnd2,dx)\n",
    "dataTestObs   = extract_SpaceTimePatches(qObs,iiTest,jjTest,W,dT,rnd1,rnd2,dx)\n",
    "\n",
    "# validation dataset\n",
    "dataVal     = extract_SpaceTimePatches(qHR,iiVal,jjVal,W,dT,rnd1,rnd2,dx)\n",
    "dataValMask = extract_SpaceTimePatches(qMask,iiVal,jjVal,W,dT,rnd1,rnd2,dx)\n",
    "dataValOI   = extract_SpaceTimePatches(qOI,iiVal,jjVal,W,dT,rnd1,rnd2,dx)\n",
    "dataValSST  = extract_SpaceTimePatches(qSST,iiVal,jjVal,W,dT,rnd1,rnd2,dx)\n",
    "dataVal_u  = extract_SpaceTimePatches(q_u,iiVal,jjVal,W,dT,rnd1,rnd2,dx)\n",
    "dataVal_v  = extract_SpaceTimePatches(q_v,iiVal,jjVal,W,dT,rnd1,rnd2,dx)\n",
    "dataVal_vv  = extract_SpaceTimePatches(q_vv,iiVal,jjVal,W,dT,rnd1,rnd2,dx)\n",
    "dataValObs   = extract_SpaceTimePatches(qObs,iiVal,jjVal,W,dT,rnd1,rnd2,dx)\n",
    "\n",
    "meanTr     = np.mean(dataTraining)\n",
    "x_train    = dataTraining - meanTr\n",
    "stdTr      = np.sqrt( np.mean( x_train**2 ) )\n",
    "x_train    = x_train / stdTr\n",
    "\n",
    "meanSST      = np.mean(dataTrainingSST)\n",
    "ySST_train   = dataTrainingSST - meanSST\n",
    "stdSST       = np.sqrt( np.mean( ySST_train**2 ) )\n",
    "ySST_train   = ySST_train / stdSST\n",
    "\n",
    "print('... mean current magntude: %.2e'%np.mean( np.sqrt(dataTraining_u**2 + dataTraining_v**2 ) ) )\n",
    "std_uv = np.sqrt( np.mean( dataTraining_u**2 + dataTraining_v**2 ) )\n",
    "std_vv = np.sqrt( np.mean( dataTraining_vv**2) )\n",
    "print('... std_vv = %f'%std_vv)\n",
    "print('... std_vv_val = %f'% np.sqrt( np.mean( dataVal_vv**2) ))\n",
    "print('... std_vv_test = %f'% np.sqrt( np.mean( dataTest_vv**2) ))\n",
    "\n",
    "u_train      = dataTraining_u / std_uv     \n",
    "v_train      = dataTraining_v / std_uv     \n",
    "vv_train     = dataTraining_vv / std_vv     \n",
    "\n",
    "x_trainOI   = (dataTrainingOI - meanTr) / stdTr\n",
    "x_trainObs  = (dataTrainingObs - meanTr) / stdTr\n",
    "x_trainMask = dataTrainingMask\n",
    "\n",
    "x_test     = (dataTest  - meanTr )\n",
    "stdTt      = np.sqrt( np.mean( x_test**2 ) )\n",
    "x_test     = x_test / stdTr\n",
    "x_testOI   = (dataTestOI - meanTr) / stdTr\n",
    "x_testObs  = (dataTestObs - meanTr) / stdTr\n",
    "x_testMask  = dataTestMask\n",
    "ySST_test  = (dataTestSST - meanSST ) / stdSST\n",
    "u_test      = dataTest_u / std_uv     \n",
    "v_test      = dataTest_v / std_uv     \n",
    "vv_test     = dataTest_vv / std_vv     \n",
    "\n",
    "x_val     = (dataVal  - meanTr )\n",
    "stdVal    = np.sqrt( np.mean( x_val**2 ) )\n",
    "x_val     = x_val / stdTr\n",
    "x_valOI   = (dataValOI - meanTr) / stdTr\n",
    "x_valObs   = (dataValObs - meanTr) / stdTr\n",
    "x_valMask = dataValMask\n",
    "ySST_val  = (dataValSST - meanSST ) / stdSST\n",
    "u_val      = dataVal_u / std_uv     \n",
    "v_val      = dataVal_v / std_uv     \n",
    "vv_val     = dataVal_vv / std_vv     \n",
    "\n",
    "x_trainObs[ x_trainMask == 0 ] = 0.\n",
    "x_testObs[ x_testMask == 0 ] = 0.\n",
    "x_valObs[ x_valMask == 0 ] = 0.\n",
    "\n",
    "\n",
    "if flag_augment_training_data == True :  \n",
    "    k_augment = 1\n",
    "    \n",
    "    for kk in range(0,k_augment):\n",
    "        # shuffle observation mask for the training data\n",
    "        ind = np.random.permutation(x_trainOI.shape[0])\n",
    "   \n",
    "        x_trainMask_new = x_trainMask[ind,:,:,:]\n",
    "        x_trainOI_new   = x_trainOI[ind,:,:,:]\n",
    "        x_train_new = x_train[ind,:,:,:]\n",
    "        x_trainObs_new = x_train_new * x_trainMask_new\n",
    "      \n",
    "        u_train_new = u_train [ind,:,:,:]    \n",
    "        v_train_new = v_train [ind,:,:,:]    \n",
    "        vv_train_new = vv_train [ind,:,:,:]    \n",
    "\n",
    "        ySST_train_new = ySST_train[ind,:,:,:]  \n",
    "        \n",
    "        x_trainOI = np.concatenate((x_trainOI,x_trainOI_new),axis=0)\n",
    "        x_trainObs = np.concatenate((x_trainObs,x_trainObs_new),axis=0)\n",
    "        x_trainMask = np.concatenate((x_trainMask,x_trainMask_new),axis=0)\n",
    "        x_train = np.concatenate((x_train,x_train_new),axis=0)\n",
    "\n",
    "        u_train = np.concatenate((u_train,u_train_new),axis=0)\n",
    "        v_train = np.concatenate((v_train,v_train_new),axis=0)\n",
    "        vv_train = np.concatenate((vv_train,vv_train_new),axis=0)\n",
    "\n",
    "        ySST_train = np.concatenate((ySST_train,ySST_train_new),axis=0)\n",
    "\n",
    "\n",
    "print('----- MSE Tr OI: %.6f'%np.mean((dataTrainingOI[:,int(dT/2),:,:]-dataTraining[:,int(dT/2),:,:])**2))\n",
    "print('----- MSE Tt OI: %.6f'%np.mean((dataTestOI[:,int(dT/2),:,:]-dataTest[:,int(dT/2),:,:])**2))\n",
    "\n",
    "print('..... Training dataset: %dx%dx%dx%d'%(x_train.shape[0],x_train.shape[1],x_train.shape[2],x_train.shape[3]))\n",
    "print('..... Test dataset    : %dx%dx%dx%d'%(x_test.shape[0],x_test.shape[1],x_test.shape[2],x_test.shape[3]))\n",
    "\n",
    "print('..... Masked points (Tr)) : %.3f'%(np.sum(x_trainMask)/(x_trainMask.shape[0]*x_trainMask.shape[1]*x_trainMask.shape[2]*x_trainMask.shape[3])))\n",
    "print('..... Masked points (Tt)) : %.3f'%(np.sum(x_testMask)/(x_testMask.shape[0]*x_testMask.shape[1]*x_testMask.shape[2]*x_testMask.shape[3])) )\n",
    "\n",
    "print('----- MSE Tr OI: %.6f'%np.mean(stdTr**2 * (x_trainOI[:,int(dT/2),:,:]-x_train[:,int(dT/2),:,:])**2))\n",
    "print('----- MSE Tt OI: %.6f'%np.mean(stdTr**2 * (x_testOI[:,int(dT/2),:,:]-x_test[:,int(dT/2),:,:])**2))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0478b4ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/VM-Nicolas-Lafon/Danube\n"
     ]
    }
   ],
   "source": [
    "training_dataset     = torch.utils.data.TensorDataset(torch.Tensor(x_train_InitD),torch.Tensor(x_train_obsD),torch.Tensor(mask_trainD),torch.Tensor(x_trainD)) # create your datset\n",
    "\n",
    "val_dataset         = torch.utils.data.TensorDataset(torch.Tensor(x_val_InitD),torch.Tensor(x_val_obsD),torch.Tensor(mask_valD),torch.Tensor(x_valD)) # create your datset\n",
    "     \n",
    "test_dataset         = torch.utils.data.TensorDataset(torch.Tensor(x_test_InitD),torch.Tensor(x_test_obsD),torch.Tensor(mask_testD),torch.Tensor(x_testD)) # create your datset\n",
    "\n",
    "        \n",
    "dataloaders = {\n",
    "                'train': torch.utils.data.DataLoader(training_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True),\n",
    "                'val': torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True),\n",
    "                'test': torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True),\n",
    "            }            \n",
    "dataset_sizes = {'train': len(training_dataset),'val': len(val_dataset), 'test': len(test_dataset)}\n",
    "        \n",
    "\n",
    "\"\"\"\n",
    "######################### data loaders\n",
    "training_dataset   = torch.utils.data.TensorDataset(torch.Tensor(x_trainOI),torch.Tensor(x_trainObs),torch.Tensor(x_trainMask),torch.Tensor(ySST_train),torch.Tensor(x_train),torch.Tensor(u_train),torch.Tensor(v_train),torch.Tensor(vv_train)) # create your datset\n",
    "val_dataset        = torch.utils.data.TensorDataset(torch.Tensor(x_valOI),torch.Tensor(x_valObs),torch.Tensor(x_valMask),torch.Tensor(ySST_val),torch.Tensor(x_val),torch.Tensor(u_val),torch.Tensor(v_val),torch.Tensor(vv_val)) # create your datset\n",
    "test_dataset       = torch.utils.data.TensorDataset(torch.Tensor(x_testOI),torch.Tensor(x_testObs),torch.Tensor(x_testMask),torch.Tensor(ySST_test),torch.Tensor(x_test),torch.Tensor(u_test),torch.Tensor(v_test),torch.Tensor(vv_test))  # create your datset\n",
    "\n",
    "dataloaders = {\n",
    "    'train': torch.utils.data.DataLoader(training_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True),\n",
    "    'val': torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True),\n",
    "    'test': torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True),\n",
    "}            \n",
    "\n",
    "var_Tr    = np.var( x_train )\n",
    "var_Tt    = np.var( x_test )\n",
    "var_Val   = np.var( x_val )\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9a8b10b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.XrDataset object at 0x7f7ca17e3460>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "###############################################################\n",
    "        ## AE architecture        \n",
    "print('........ Define AE architecture')\n",
    "            \n",
    "shapeData = np.ones(3).astype(int)\n",
    "shapeData[1:] =  x_trainD.shape[1:]\n",
    "                   \n",
    "elif flagAEType == 1: ## Conv model with no use of the central point\n",
    "    dW = 2\n",
    "    genSuffixModel = '_GENN_%d_%02d_%02d'%(flagAEType,DimAE,dW)\n",
    "    class Encoder(torch.nn.Module):\n",
    "        def __init__(self):\n",
    "            super(Encoder, self).__init__()\n",
    "            self.conv1  = NN_4DVar.ConstrainedConv2d(1,DimAE,(2*dW+1,2*dW+1),padding=dW,bias=False)                      \n",
    "            self.conv21 = torch.nn.Conv2d(DimAE,DimAE,(1,1),padding=0,bias=False)\n",
    "            self.conv22 = torch.nn.Conv2d(DimAE,DimAE,(1,1),padding=0,bias=False)\n",
    "            self.conv23 = torch.nn.Conv2d(DimAE,DimAE,(1,1),padding=0,bias=False)\n",
    "            self.conv3  = torch.nn.Conv2d(2*DimAE,1,(1,1),padding=0,bias=False)\n",
    "            #self.conv4 = torch.nn.Conv1d(4*shapeData[0]*DimAE,8*shapeData[0]*DimAE,1,padding=0,bias=False)\n",
    "            \n",
    "            #self.conv2Tr = torch.nn.ConvTranspose1d(4*shapeData[0]*DimAE,8*shapeData[0]*DimAE,4,stride=4,bias=False)          \n",
    "            #self.conv5 = torch.nn.Conv1d(8*shapeData[0]*DimAE,16*shapeData[0]*DimAE,3,padding=1,bias=False)\n",
    "            #self.conv6 = torch.nn.Conv1d(16*shapeData[0]*DimAE,shapeData[0],3,padding=1,bias=False)\n",
    "            \n",
    "        def forward(self, xin):\n",
    "            x_1 = torch.cat((xin[:,:,xin.size(2)-dW:,:],xin,xin[:,:,0:dW,:]),dim=2)\n",
    "            #x_1 = x_1.view(-1,1,xin.size(1)+2*dW,xin.size(2))\n",
    "            x   = self.conv1( x_1 )\n",
    "            x   = x[:,:,dW:xin.size(2)+dW,:]\n",
    "            x   = torch.cat((self.conv21(x), self.conv22(x) * self.conv23(x)),dim=1)\n",
    "            x   = self.conv3( x )\n",
    "            #x = self.conv4( F.relu(x) )\n",
    "            x = x.view(-1,shapeData[0],shapeData[1],shapeData[2])\n",
    "            return x\n",
    "    class Decoder(torch.nn.Module):\n",
    "        def __init__(self):\n",
    "            super(Decoder, self).__init__()\n",
    "            \n",
    "        def forward(self, x):\n",
    "            return torch.mul(1.,x)\n",
    "                \n",
    "elif flagAEType == 2: ## Conv model with no use of the central point\n",
    "    dW = 5\n",
    "    genSuffixModel = '_GENN_%d_%02d_%02d'%(flagAEType,DimAE,dW)\n",
    "    class Encoder(torch.nn.Module):\n",
    "        def __init__(self):\n",
    "            super(Encoder, self).__init__()\n",
    "            self.pool1  = torch.nn.AvgPool2d((1,4))\n",
    "            self.conv11 = NN_4DVar.ConstrainedConv2d(1,DimAE,(2*dW+1,2*dW+1),padding=dW,bias=False)                      \n",
    "            self.conv12 = torch.nn.Conv2d(DimAE,DimAE,(1,1),padding=0,bias=False)\n",
    "            self.conv21 = torch.nn.Conv2d(DimAE,DimAE,(1,1),padding=0,bias=False)\n",
    "            self.conv22 = torch.nn.Conv2d(DimAE,DimAE,(1,1),padding=0,bias=False)\n",
    "            self.conv23 = torch.nn.Conv2d(DimAE,DimAE,(1,1),padding=0,bias=False)\n",
    "            self.conv3  = torch.nn.Conv2d(2*DimAE,DimAE,(1,1),padding=0,bias=False)\n",
    "            \n",
    "            self.convTr = torch.nn.ConvTranspose2d(DimAE,DimAE,(1,4),stride=(1,4),bias=False)          \n",
    "            #self.conv5 = torch.nn.Conv1d(8*shapeData[0]*DimAE,16*shapeData[0]*DimAE,3,padding=1,bias=False)\n",
    "            #self.conv6 = torch.nn.Conv1d(16*shapeData[0]*DimAE,shapeData[0],3,padding=1,bias=False)\n",
    "            self.conv11_1 = NN_4DVar.ConstrainedConv2d(1,DimAE,(2*dW+1,2*dW+1),padding=dW,bias=False)                      \n",
    "            self.conv12_1 = torch.nn.Conv2d(DimAE,DimAE,(1,1),padding=0,bias=False)\n",
    "            self.conv21_1 = torch.nn.Conv2d(DimAE,DimAE,(1,1),padding=0,bias=False)\n",
    "            self.conv22_1 = torch.nn.Conv2d(DimAE,DimAE,(1,1),padding=0,bias=False)\n",
    "            self.conv23_1 = torch.nn.Conv2d(DimAE,DimAE,(1,1),padding=0,bias=False)\n",
    "            self.conv3_1  = torch.nn.Conv2d(2*DimAE,DimAE,(1,1),padding=0,bias=False)\n",
    "            \n",
    "            self.convF    = torch.nn.Conv2d(DimAE,1,1,padding=0,bias=False)\n",
    "        def forward(self, xin):\n",
    "                \n",
    "            x_1 = self.pool1(xin)\n",
    "                \n",
    "            x_1 = torch.cat((x_1[:,:,x_1.size(2)-dW:,:],x_1,x_1[:,:,0:dW,:]),dim=2)\n",
    "                \n",
    "            #x_1 = x_1.view(-1,1,xin.size(1)+2*dW,xin.size(2))\n",
    "            x   = self.conv11( x_1 )\n",
    "               \n",
    "            x   = self.conv12( F.relu(x) )\n",
    "                \n",
    "            x   = x[:,:,dW:xin.size(2)+dW,:]\n",
    "                \n",
    "            x   = torch.cat((self.conv21(x), self.conv22(x) * self.conv23(x)),dim=1)\n",
    "                \n",
    "            x   = self.conv3( x )\n",
    "          \n",
    "            x   = self.convTr( x )\n",
    "                \n",
    "                      \n",
    "            x_2 = torch.cat((xin[:,:,xin.size(2)-dW:,:],xin,xin[:,:,0:dW,:]),dim=2)\n",
    "                \n",
    "            dx  = self.conv11_1( x_2 )\n",
    "                \n",
    "            dx  = self.conv12_1( F.relu(dx) )\n",
    "                \n",
    "            dx   = dx[:,:,dW:xin.size(2)+dW,:]\n",
    "                \n",
    "            dx   = torch.cat((self.conv21_1(dx), self.conv22_1(dx) * self.conv23_1(dx)),dim=1)\n",
    "                \n",
    "            dx   = self.conv3_1( dx )\n",
    "                \n",
    "                \n",
    "            y=x+dx\n",
    "            x    = self.convF( x + dx )\n",
    "                \n",
    "            #x = self.conv4( F.relu(x) )\n",
    "            x = x.view(-1,shapeData[0],shapeData[1],shapeData[2])\n",
    "                \n",
    "            return x\n",
    "    class Decoder(torch.nn.Module):\n",
    "        def __init__(self):\n",
    "            super(Decoder, self).__init__()\n",
    "            \n",
    "        def forward(self, x):\n",
    "            return torch.mul(1.,x)\n",
    "            \n",
    "elif flagAEType == 3: ## Same as flagAEType == 2 with no constraint on central point\n",
    "    dW = 5\n",
    "    genSuffixModel = '_GENN_%d_%02d_%02d'%(flagAEType,DimAE,dW)\n",
    "    class Encoder(torch.nn.Module):\n",
    "        def __init__(self):\n",
    "            super(Encoder, self).__init__()\n",
    "            self.pool1  = torch.nn.AvgPool2d((1,4))\n",
    "            self.conv11 = torch.nn.Conv2d(1,DimAE,(2*dW+1,2*dW+1),padding=dW,bias=False)                      \n",
    "            self.conv12 = torch.nn.Conv2d(DimAE,DimAE,(1,1),padding=0,bias=False)\n",
    "            self.conv21 = torch.nn.Conv2d(DimAE,DimAE,(1,1),padding=0,bias=False)\n",
    "            self.conv22 = torch.nn.Conv2d(DimAE,DimAE,(1,1),padding=0,bias=False)\n",
    "            self.conv23 = torch.nn.Conv2d(DimAE,DimAE,(1,1),padding=0,bias=False)\n",
    "            self.conv3  = torch.nn.Conv2d(2*DimAE,DimAE,(1,1),padding=0,bias=False)\n",
    "            \n",
    "            self.convTr = torch.nn.ConvTranspose2d(DimAE,DimAE,(1,4),stride=(1,4),bias=False)          \n",
    "            #self.conv5 = torch.nn.Conv1d(8*shapeData[0]*DimAE,16*shapeData[0]*DimAE,3,padding=1,bias=False)\n",
    "            #self.conv6 = torch.nn.Conv1d(16*shapeData[0]*DimAE,shapeData[0],3,padding=1,bias=False)\n",
    "            self.conv11_1 = torch.nn.Conv2d(1,DimAE,(2*dW+1,2*dW+1),padding=dW,bias=False)                      \n",
    "            self.conv12_1 = torch.nn.Conv2d(DimAE,DimAE,(1,1),padding=0,bias=False)\n",
    "            self.conv21_1 = torch.nn.Conv2d(DimAE,DimAE,(1,1),padding=0,bias=False)\n",
    "            self.conv22_1 = torch.nn.Conv2d(DimAE,DimAE,(1,1),padding=0,bias=False)\n",
    "            self.conv23_1 = torch.nn.Conv2d(DimAE,DimAE,(1,1),padding=0,bias=False)\n",
    "            self.conv3_1  = torch.nn.Conv2d(2*DimAE,DimAE,(1,1),padding=0,bias=False)\n",
    "            \n",
    "            self.convF    = torch.nn.Conv2d(DimAE,1,1,padding=0,bias=False)\n",
    "        def forward(self, xin):\n",
    "            x_1 = self.pool1(xin)\n",
    "            x_1 = torch.cat((x_1[:,:,x_1.size(2)-dW:,:],x_1,x_1[:,:,0:dW,:]),dim=2)\n",
    "            #x_1 = x_1.view(-1,1,xin.size(1)+2*dW,xin.size(2))\n",
    "            x   = self.conv11( x_1 )\n",
    "            x   = self.conv12( F.relu(x) )\n",
    "            x   = x[:,:,dW:xin.size(2)+dW,:]\n",
    "            x   = torch.cat((self.conv21(x), self.conv22(x) * self.conv23(x)),dim=1)\n",
    "            x   = self.conv3( x )\n",
    "            x   = self.convTr( x )\n",
    "                      \n",
    "            x_2 = torch.cat((xin[:,:,xin.size(2)-dW:,:],xin,xin[:,:,0:dW,:]),dim=2)\n",
    "            print(xin.shape)\n",
    "            print(xin.size(2))\n",
    "            dx  = self.conv11_1( x_2 )\n",
    "            dx  = self.conv12_1( F.relu(dx) )\n",
    "            dx   = dx[:,:,dW:xin.size(2)+dW,:]\n",
    "            dx   = torch.cat((self.conv21_1(dx), self.conv22_1(dx) * self.conv23_1(dx)),dim=1)\n",
    "            dx   = self.conv3_1( dx )\n",
    "                      \n",
    "            x    = self.convF( x + dx )\n",
    "            #x = self.conv4( F.relu(x) )\n",
    "            x = x.view(-1,shapeData[0],shapeData[1],shapeData[2])\n",
    "            return x\n",
    "    class Decoder(torch.nn.Module):\n",
    "        def __init__(self):\n",
    "            super(Decoder, self).__init__()\n",
    "            \n",
    "        def forward(self, x):\n",
    "            return torch.mul(1.,x)\n",
    "            \n",
    "elif flagAEType == 4: ## Conv model with no use of the central point\n",
    "    dW = 5\n",
    "    genSuffixModel = '_GENN_%d_%02d_%02d'%(flagAEType,DimAE,dW)\n",
    "    class Encoder(torch.nn.Module):\n",
    "        def __init__(self):\n",
    "            super(Encoder, self).__init__()\n",
    "            self.pool1  = torch.nn.AvgPool2d(4)\n",
    "            self.conv1  = NN_4DVar.ConstrainedConv2d(1,2*DimAE,(2*dW+1,2*dW+1),padding=dW,bias=False)\n",
    "            self.conv2  = torch.nn.Conv1d(2*DimAE,DimAE,(1,1),padding=0,bias=False)\n",
    "                      \n",
    "            self.conv21 = torch.nn.Conv2d(DimAE,DimAE,(1,1),padding=0,bias=False)\n",
    "            self.conv22 = torch.nn.Conv2d(DimAE,DimAE,(1,1),padding=0,bias=False)\n",
    "            self.conv23 = torch.nn.Conv2d(DimAE,DimAE,(1,1),padding=0,bias=False)\n",
    "            self.conv3  = torch.nn.Conv2d(2*DimAE,DimAE,(1,1),padding=0,bias=False)\n",
    "            #self.conv4 = torch.nn.Conv1d(4*shapeData[0]*DimAE,8*shapeData[0]*DimAE,1,padding=0,bias=False)\n",
    "            \n",
    "            self.conv2Tr = torch.nn.ConvTranspose2d(DimAE,1,(4,4),stride=(4,4),bias=False)          \n",
    "            #self.conv5 = torch.nn.Conv1d(2*shapeData[0]*DimAE,2*shapeData[0]*DimAE,3,padding=1,bias=False)\n",
    "            #self.conv6 = torch.nn.Conv1d(2*shapeData[0]*DimAE,shapeData[0],1,padding=0,bias=False)\n",
    "            #self.conv6 = torch.nn.Conv1d(16*shapeData[0]*DimAE,shapeData[0],3,padding=1,bias=False)\n",
    "            \n",
    "            self.convHR1  = NN_4DVar.ConstrainedConv2d(1,2*DimAE,(2*dW+1,2*dW+1),padding=dW,bias=False)\n",
    "            self.convHR2  = torch.nn.Conv2d(2*DimAE,DimAE,(1,1),padding=0,bias=False)\n",
    "                      \n",
    "            self.convHR21 = torch.nn.Conv2d(DimAE,DimAE,(1,1),padding=0,bias=False)\n",
    "            self.convHR22 = torch.nn.Conv2d(DimAE,DimAE,(1,1),padding=0,bias=False)\n",
    "            self.convHR23 = torch.nn.Conv2d(DimAE,DimAE,(1,1),padding=0,bias=False)\n",
    "            self.convHR3  = torch.nn.Conv2d(2*DimAE,1,(1,1),padding=0,bias=False)\n",
    "\n",
    "        def forward(self, xinp):\n",
    "            #x = self.fc1( torch.nn.Flatten(x) )\n",
    "            #x = self.pool1( xinp )\n",
    "            x = self.pool1( xinp )\n",
    "            x = self.conv1( x )\n",
    "            x = self.conv2( F.relu(x) )\n",
    "            x = torch.cat((self.conv21(x), self.conv22(x) * self.conv23(x)),dim=1)\n",
    "            x = self.conv3( x )\n",
    "            x = self.conv2Tr( x )\n",
    "            #x = self.conv5( F.relu(x) )\n",
    "            #x = self.conv6( F.relu(x) )\n",
    "                      \n",
    "            xHR = self.convHR1( xinp )\n",
    "            xHR = self.convHR2( F.relu(xHR) )\n",
    "            xHR = torch.cat((self.convHR21(xHR), self.convHR22(xHR) * self.convHR23(xHR)),dim=1)\n",
    "            xHR = self.convHR3( xHR )\n",
    "                      \n",
    "            x   = torch.add(x,1.,xHR)\n",
    "                      \n",
    "            #x = x.view(-1,shapeData[0],shapeData[1],shapeData[2])\n",
    "            return x\n",
    "            \n",
    "    class Decoder(torch.nn.Module):\n",
    "        def __init__(self):\n",
    "            super(Decoder, self).__init__()\n",
    "            \n",
    "        def forward(self, x):\n",
    "            return torch.mul(1.,x)\n",
    "\n",
    "\n",
    "\n",
    "class Phi_r(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Phi_r, self).__init__()\n",
    "        self.encoder = Encoder(shapeData[0],DimAE,rateDr)\n",
    "        self.decoder = Decoder()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "phi_r = Phi_r()\n",
    "\n",
    "print('Phi Model type: '+genSuffixModel)\n",
    "print(phi_r)\n",
    "print('Number of trainable parameters = %d'%(sum(p.numel() for p in phi_r.parameters() if p.requires_grad)))\n",
    "\n",
    "'''\n",
    "#######################################Phi_r, Model_H, Model_Sampling architectures ################################################\n",
    "\n",
    "print('........ Define AE architecture')\n",
    "shapeData      = np.array(x_train.shape[1:])\n",
    "shapeData_test = np.array(x_test.shape[1:])\n",
    "if flag_aug_state == 1 :\n",
    "    shapeData[0]  += 3*shapeData[0]\n",
    "elif flag_aug_state == 2 :\n",
    "    shapeData[0]  += 4*shapeData[0]\n",
    "else:\n",
    "    shapeData[0]  += 2*shapeData[0]\n",
    "\n",
    "shapeDataSST   = np.array(ySST_train.shape[1:])\n",
    "\n",
    "dW  = 3\n",
    "dW2 = 1\n",
    "sS  = int(4/dx)\n",
    "nbBlocks = 1\n",
    "rateDr   = 0. * rateDropout\n",
    "\n",
    "class BiLinUnit(torch.nn.Module):\n",
    "    def __init__(self,dimIn,dim,dropout=0.):\n",
    "        super(BiLinUnit, self).__init__()\n",
    "        \n",
    "        self.conv1  = torch.nn.Conv2d(dimIn, 2*dim, (2*dW+1,2*dW+1),padding=dW, bias=False)\n",
    "        self.conv2  = torch.nn.Conv2d(2*dim, dim, (2*dW2+1,2*dW2+1), padding=dW2, bias=False)\n",
    "        self.conv3  = torch.nn.Conv2d(2*dim, dimIn, (2*dW2+1,2*dW2+1), padding=dW2, bias=False)\n",
    "        self.bilin0 = torch.nn.Conv2d(dim, dim, (2*dW2+1,2*dW2+1), padding=dW2, bias=False)\n",
    "        self.bilin1 = torch.nn.Conv2d(dim, dim, (2*dW2+1,2*dW2+1), padding=dW2, bias=False)\n",
    "        self.bilin2 = torch.nn.Conv2d(dim, dim, (2*dW2+1,2*dW2+1), padding=dW2, bias=False)\n",
    "        self.dropout  = torch.nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self,xin):\n",
    "        \n",
    "        x = self.conv1(xin)\n",
    "        x = self.dropout(x)\n",
    "        x = self.conv2( F.relu(x) )\n",
    "        x = self.dropout(x)\n",
    "        x = torch.cat((self.bilin0(x), self.bilin1(x) * self.bilin2(x)),dim=1)\n",
    "        x = self.dropout(x)\n",
    "        x = self.conv3( x )\n",
    "        \n",
    "        return x\n",
    "   \n",
    "class Encoder(torch.nn.Module):\n",
    "    def __init__(self,dimInp,dimAE,rateDropout=0.):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.NbBlocks  = nbBlocks\n",
    "        self.DimAE     = dimAE\n",
    "        #self.conv1HR  = torch.nn.Conv2d(dimInp,self.DimAE,(2*dW+1,2*dW+1),padding=dW,bias=False) \n",
    "        #self.conv1LR  = torch.nn.Conv2d(dimInp,self.DimAE,(2*dW+1,2*dW+1),padding=dW,bias=False) \n",
    "        self.pool1   = torch.nn.AvgPool2d(sS)\n",
    "        self.convTr  = torch.nn.ConvTranspose2d(dimInp,dimInp,(sS,sS),stride=(sS,sS),bias=False)          \n",
    "\n",
    "        #self.NNtLR    = self.__make_ResNet(self.DimAE,self.NbBlocks,rateDropout)\n",
    "        #self.NNHR     = self.__make_ResNet(self.DimAE,self.NbBlocks,rateDropout)                      \n",
    "        self.NNLR     = self.__make_BilinNN(dimInp,self.DimAE,self.NbBlocks,rateDropout)\n",
    "        self.NNHR     = self.__make_BilinNN(dimInp,self.DimAE,self.NbBlocks,rateDropout)                      \n",
    "        self.dropout  = torch.nn.Dropout(rateDropout)\n",
    "      \n",
    "    def __make_BilinNN(self,dimInp,dimAE,Nb_Blocks=2,dropout=0.): \n",
    "          layers = []\n",
    "          layers.append( BiLinUnit(dimInp,dimAE,dropout) )\n",
    "          for kk in range(0,Nb_Blocks-1):\n",
    "              layers.append( BiLinUnit(dimAE,dimAE,dropout) )\n",
    "          return torch.nn.Sequential(*layers)\n",
    "      \n",
    "    def forward(self, xinp):\n",
    "        \n",
    "        ## LR comlponent\n",
    "        xLR = self.NNLR( self.pool1(xinp) )\n",
    "        xLR = self.dropout(xLR)\n",
    "        xLR = self.convTr( xLR ) \n",
    "        \n",
    "        # HR component\n",
    "        xHR = self.NNHR( xinp )\n",
    "        \n",
    "        return xLR + xHR\n",
    "  \n",
    "class Decoder(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Decoder, self).__init__()\n",
    "  \n",
    "    def forward(self, x):\n",
    "        return torch.mul(1.,x)\n",
    "\n",
    "\n",
    "class Phi_r(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Phi_r, self).__init__()\n",
    "        self.encoder = Encoder(shapeData[0],DimAE,rateDr)\n",
    "        self.decoder = Decoder()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "phi_r = Phi_r()\n",
    "\n",
    "print(phi_r)\n",
    "print('Number of trainable parameters = %d' % (sum(p.numel() for p in phi_r.parameters() if p.requires_grad)))\n",
    "W_KERNEL_MODEL_H = 5\n",
    "\n",
    "class Model_H(torch.nn.Module):\n",
    "    def __init__(self,width_kernel=3,dim=5):\n",
    "        super(Model_H, self).__init__()\n",
    "\n",
    "        self.DimObs = 2\n",
    "        self.w_kernel = width_kernel\n",
    "        self.dimObsChannel = np.array([shapeData[0],dim])\n",
    "\n",
    "        self.conv11  = torch.nn.Conv2d(shapeData[0],self.dimObsChannel[1],(self.w_kernel,self.w_kernel),padding=int(self.w_kernel/2),bias=False)\n",
    "\n",
    "        self.conv12  = torch.nn.Conv2d(shapeData[0],self.dimObsChannel[1],(self.w_kernel,self.w_kernel),padding=int(self.w_kernel/2),bias=False)\n",
    "        self.conv22  = torch.nn.Conv2d(shapeData[0],self.dimObsChannel[1],(self.w_kernel,self.w_kernel),padding=int(self.w_kernel/2),bias=False)\n",
    "                    \n",
    "        self.conv21  = torch.nn.Conv2d(shapeDataSST[0],self.dimObsChannel[1],(self.w_kernel,self.w_kernel),padding=int(self.w_kernel/2),bias=False)\n",
    "        self.convM   = torch.nn.Conv2d(shapeDataSST[0],self.dimObsChannel[1],(3,3),padding=int(self.w_kernel/2),bias=False)\n",
    "        self.S       = torch.nn.Sigmoid()#torch.nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x , y , mask):\n",
    "        dyout  = (x - y[0]) * mask[0] \n",
    "        \n",
    "        y1     = y[1] * mask[1]\n",
    "        dyout1 = self.conv11(x) - self.conv21(y1)\n",
    "        dyout1 = dyout1 * self.S( self.convM( mask[1] ) )                  \n",
    "        \n",
    "        return [dyout,dyout1]\n",
    "\n",
    "class Gradient_img(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Gradient_img, self).__init__()\n",
    "\n",
    "        a = np.array([[-1., 0., 1.], [-2., 0., 2.], [-1., 0., 1.]])\n",
    "        self.convGx = torch.nn.Conv2d(1, 1, kernel_size=3, stride=1, padding=0, bias=False)\n",
    "        self.convGx.weight = torch.nn.Parameter(torch.from_numpy(a).float().unsqueeze(0).unsqueeze(0), requires_grad=False)\n",
    "\n",
    "        b = np.array([[-1., -2., -1.], [0., 0., 0.], [1., 2., 1.]])\n",
    "        self.convGy = torch.nn.Conv2d(1, 1, kernel_size=3, stride=1, padding=0, bias=False)\n",
    "        self.convGy.weight = torch.nn.Parameter(torch.from_numpy(b).float().unsqueeze(0).unsqueeze(0), requires_grad=False)\n",
    "\n",
    "    def forward(self, im):\n",
    "\n",
    "        if im.size(1) == 1:\n",
    "            G_x = self.convGx(im)\n",
    "            G_y = self.convGy(im)\n",
    "            G = torch.sqrt(torch.pow(0.5 * G_x, 2) + torch.pow(0.5 * G_y, 2))\n",
    "        else:\n",
    "\n",
    "            for kk in range(0, im.size(1)):\n",
    "                G_x_ = self.convGx(im[:, kk, :, :].view(-1, 1, im.size(2), im.size(3)))\n",
    "                G_y_ = self.convGy(im[:, kk, :, :].view(-1, 1, im.size(2), im.size(3)))\n",
    "\n",
    "                G_x_ = G_x_.view(-1, 1, im.size(2) - 2, im.size(2) - 2)\n",
    "                G_y_ = G_y_.view(-1, 1, im.size(2) - 2, im.size(2) - 2)\n",
    "                nG_ = torch.sqrt(torch.pow(0.5 * G_x_, 2) + torch.pow(0.5 * G_y_, 2))\n",
    "\n",
    "                if kk == 0:\n",
    "                    nG = nG_.view(-1, 1, im.size(1) - 2, im.size(2) - 2)\n",
    "                    Gx = G_x_.view(-1, 1, im.size(1) - 2, im.size(2) - 2)\n",
    "                    Gy = G_y_.view(-1, 1, im.size(1) - 2, im.size(2) - 2)\n",
    "                else:\n",
    "                    nG = torch.cat((nG, nG_.view(-1, 1, im.size(1) - 2, im.size(2) - 2)), dim=1)\n",
    "                    Gx = torch.cat((Gx, G_x_.view(-1, 1, im.size(1) - 2, im.size(2) - 2)), dim=1)\n",
    "                    Gy = torch.cat((Gy, G_y_.view(-1, 1, im.size(1) - 2, im.size(2) - 2)), dim=1)\n",
    "        return nG,Gx,Gy\n",
    "gradient_img = Gradient_img()\n",
    "\n",
    "class Div_uv(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Div_uv, self).__init__()\n",
    "\n",
    "        a = np.array([[1., 0., -1.], [2., 0., -2.], [1., 0., -1.]])\n",
    "        self.convGx = torch.nn.Conv2d(1, 1, kernel_size=3, stride=1, padding=0, bias=False)\n",
    "        self.convGx.weight = torch.nn.Parameter(torch.from_numpy(a).float().unsqueeze(0).unsqueeze(0), requires_grad=False)\n",
    "\n",
    "        b = np.array([[1., 2., 1.], [0., 0., 0.], [-1., -2., -1.]])\n",
    "        self.convGy = torch.nn.Conv2d(1, 1, kernel_size=3, stride=1, padding=0, bias=False)\n",
    "        self.convGy.weight = torch.nn.Parameter(torch.from_numpy(b).float().unsqueeze(0).unsqueeze(0), requires_grad=False)\n",
    "\n",
    "    def forward(self, u,v):\n",
    "\n",
    "        if u.size(1) == 1:\n",
    "            G_x = self.convGx(u)\n",
    "            G_y = self.convGy(v)\n",
    "        else:\n",
    "\n",
    "            for kk in range(0, u.size(1)):\n",
    "                G_x = self.convGx(u[:, kk, :, :].view(-1, 1, u.size(2), u.size(3)))\n",
    "                G_y = self.convGy(v[:, kk, :, :].view(-1, 1, u.size(2), u.size(3)))\n",
    "\n",
    "                G_x = G_x.view(-1, 1, u.size(2) - 2, u.size(2) - 2)\n",
    "                G_y = G_y.view(-1, 1, u.size(2) - 2, u.size(2) - 2)\n",
    "\n",
    "                div_ = G_x + G_y \n",
    "                if kk == 0:\n",
    "                    div = div_.view(-1, 1, u.size(1) - 2, u.size(2) - 2)\n",
    "                else:\n",
    "                    div = torch.cat((div, div_.view(-1, 1, u.size(1) - 2, u.size(2) - 2)), dim=1)\n",
    "        return div\n",
    "    \n",
    "model_div = Div_uv()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_div = model_div.to(device)\n",
    "\n",
    "class Compute_graduv(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Compute_graduv, self).__init__()\n",
    "\n",
    "        self.alpha = torch.nn.Conv2d(1, 1, kernel_size=1, stride=1, padding=0, bias=False)\n",
    "        self.alpha.weight = torch.nn.Parameter(torch.from_numpy(np.array([1.0])).float().unsqueeze(0).unsqueeze(0).unsqueeze(0), requires_grad=False)\n",
    "        \n",
    "        #self.alpha_v = torch.nn.Conv2d(1, 1, kernel_size=1, stride=1, padding=0, bias=False)\n",
    "        self.convG = torch.nn.Conv2d(1, 1, kernel_size=(3,3), stride=1, padding=(0,0), bias=False)\n",
    "        \n",
    "        if 1*1 :    \n",
    "            a = 0.25*np.array([[-1., 0., 1.], [-2., 0., 2.], [-1., 0., 1.]])\n",
    "            #self.convGx.weight = torch.nn.Parameter(torch.from_numpy(a).float().unsqueeze(0).unsqueeze(0), requires_grad=False)\n",
    "            self.convG.weight = torch.nn.Parameter(torch.from_numpy(a).float().unsqueeze(0).unsqueeze(0), requires_grad=False)\n",
    "            #self.convGx = torch.nn.Conv2d(1, 1, kernel_size=(1,3), stride=1, padding=(0,1), bias=False)\n",
    "            \n",
    "    def forward(self, im, du=1.,dv=1.):\n",
    "        for kk in range(0, im.size(1)):\n",
    "            _g_u = self.alpha( self.convG(im[:, kk, :, :].view(-1, 1, im.size(2), im.size(3))) )\n",
    "            _g_v = self.convG( torch.transpose( im[:, kk, :, :].view(-1, 1, im.size(2), im.size(3)) , 2 , 3 ) )\n",
    "            _g_v = self.alpha( torch.transpose( _g_v , 2 , 3 ) )\n",
    "\n",
    "            _g_u = _g_u.view(-1, 1, im.size(2) - 2, im.size(3) - 2)\n",
    "            _g_v = _g_v.view(-1, 1, im.size(2) - 2, im.size(3) - 2)\n",
    "            if kk == 0:\n",
    "                d_u = _g_u.view(-1, 1, im.size(2) - 2, im.size(3) - 2)\n",
    "                d_v = _g_v.view(-1, 1, im.size(2) - 2, im.size(3) - 2)\n",
    "            else:\n",
    "                d_u = torch.cat((d_u, _g_u.view(-1, 1, im.size(2) - 2, im.size(3) - 2)), dim=1)\n",
    "                d_v = torch.cat((d_v, _g_v.view(-1, 1, im.size(2) - 2, im.size(3) - 2)), dim=1)\n",
    "        \n",
    "        return [ du * d_u , dv * d_v ]\n",
    "\n",
    "class ModelLR(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ModelLR, self).__init__()\n",
    "\n",
    "        self.pool = torch.nn.AvgPool2d((16, 16))\n",
    "\n",
    "    def forward(self, im):\n",
    "        return self.pool(im)\n",
    "\n",
    "alpha_MSE     = 0.1\n",
    "alpha_Proj    = 0.5\n",
    "alpha_SR      = 0.5\n",
    "alpha_LR      = 0.5  # 1e4\n",
    "\n",
    "# loss weghing wrt time\n",
    "w_ = np.zeros(dT)\n",
    "w_[int(dT / 2)] = 1.\n",
    "wLoss = torch.Tensor(w_)\n",
    "\n",
    "\n",
    "# recompute the MSE for OI on training dataset\n",
    "\n",
    "def save_NetCDF(saved_path1, ind_start,ind_end, ssh_gt , ssh_oi, ssh_obs, sst_gt, u_gt, v_gt, vv_gt, ssh_rec, u_rec_geo, v_rec_geo, u_rec, v_rec, vv_rec, vv_rec_swot, feat_sst ):\n",
    "    \n",
    "    extent = [-65., -55., 30., 40.]\n",
    "    indLat = 200\n",
    "    indLon = 200\n",
    "\n",
    "    lon = np.arange(extent[0], extent[1], 1 / (20 / dwscale))\n",
    "    lat = np.arange(extent[2], extent[3], 1 / (20 / dwscale))\n",
    "    indLat = int(indLat / dwscale)\n",
    "    indLon = int(indLon / dwscale)\n",
    "    lon = lon[:indLon]\n",
    "    lat = lat[:indLat]\n",
    "\n",
    "    mesh_lat, mesh_lon = np.meshgrid(lat, lon)\n",
    "    mesh_lat = mesh_lat.T\n",
    "    mesh_lon = mesh_lon.T\n",
    "\n",
    "    #indN_Tt = np.concatenate([np.arange(60, 80)])\n",
    "    indN_Tt = np.concatenate([np.arange(ind_start, ind_end)])\n",
    "    time_ = [datetime.datetime.strftime(datetime.datetime.strptime(\"2012-10-01\", '%Y-%m-%d') \\\n",
    "                                       + datetime.timedelta(days=np.float64(i)), \"%Y-%m-%d\") for i in indN_Tt]\n",
    "\n",
    "    xrdata = xr.Dataset( \\\n",
    "        data_vars={'longitude': (('lat', 'lon'), mesh_lon), \\\n",
    "                   'latitude': (('lat', 'lon'), mesh_lat), \\\n",
    "                   'Time': (('time'), time_), \\\n",
    "                   'ssh_gt': (('time', 'lat', 'lon'), ssh_gt), \\\n",
    "                   'ssh_oi': (('time', 'lat', 'lon'), ssh_oi), \\\n",
    "                   'ssh_obs': (('time', 'lat', 'lon'), ssh_obs), \\\n",
    "                   'sst_gt': (('time', 'lat', 'lon'), sst_gt), \\\n",
    "                   'u_gt': (('time', 'lat_uv', 'lon_uv'), u_gt), \\\n",
    "                   'v_gt': (('time', 'lat_uv', 'lon_uv'), v_gt), \\\n",
    "                   'vv_gt': (('time', 'lat_uv', 'lon_uv'), vv_gt), \\\n",
    "                   'ssh_rec': (('time', 'lat', 'lon'), ssh_rec), \\\n",
    "                   'u_rec_geo': (('time', 'lat_uv', 'lon_uv'), u_rec_geo), \\\n",
    "                   'v_rec_geo': (('time', 'lat_uv', 'lon_uv'), v_rec_geo), \\\n",
    "                   'u_rec': (('time', 'lat_uv', 'lon_uv'), u_rec), \\\n",
    "                   'v_rec': (('time', 'lat_uv', 'lon_uv'), v_rec), \\\n",
    "                   'vv_rec': (('time', 'lat_uv', 'lon_uv'), vv_rec), \\\n",
    "                   'vv_rec_swot': (('time', 'lat_uv', 'lon_uv'), vv_rec), \\\n",
    "                   'feat_sst': (('time','dT','lat', 'lon'), feat_sst)}, \\\n",
    "        coords={'lon': lon, 'lat': lat,'dT': np.arange(0,feat_sst.shape[1]),'lon_uv': lon[1:-1], 'lat_uv': lat[1:-1], 'time': indN_Tt},)\n",
    "    xrdata.time.attrs['units'] = 'days since 2012-10-01 00:00:00'\n",
    "    xrdata.to_netcdf(path=saved_path1, mode='w')\n",
    "\n",
    "\n",
    "m_NormObs = NN_4DVar.Model_WeightedL2Norm()     \n",
    "m_NormPhi = NN_4DVar.Model_WeightedL2Norm()\n",
    "\n",
    "FLAG_TRAINABLE_NORM = True\n",
    "def compute_Lpqr_numpy(x0,x1,p_=2.,q_=2.,r_=2.,thr_=0.,eps=1e-10):\n",
    "    \n",
    "    if 1*1 : \n",
    "        nx = ( np.sqrt( x0[0]**2 + x0[1]**2 ) > thr_ )\n",
    "        dx = np.sqrt( (x0[0]-x1[0])**2 + (x0[1]-x1[1])**2 )\n",
    "        x =  nx * dx\n",
    "        loss_ = np.nansum( np.power( np.abs(eps + x), p_ )  , axis = 2)\n",
    "        loss_ = np.sum( loss_ , axis = 1)\n",
    "     \n",
    "        N = np.sum( np.sum((~np.isnan(x)) * nx , axis = 2) , axis = 1)\n",
    "        loss_ = np.power( eps + loss_ / N  , q_/p_ )       \n",
    "        \n",
    "        loss_ = np.mean( np.power( np.power( eps + loss_,1./q_) , r_) , axis = 0)\n",
    "    else:\n",
    "        nx = ( np.sqrt( x0[0]**2 + x0[1]**2 ) > thr_ )\n",
    "        dx = np.sqrt( (x0[0]-x1[0])**2 + (x0[1]-x1[1])**2 ) / np.sqrt( (x0[0])**2 + (x0[1])**2 )\n",
    "        x =  nx * dx\n",
    "        loss_ = np.nansum( np.power( np.abs(eps + x), p_ )  , axis = 2)\n",
    "        loss_ = np.sum( loss_ , axis = 1)\n",
    "     \n",
    "        N = np.sum( np.sum( (~np.isnan(x)) * nx , axis = 2) , axis = 1)\n",
    "        loss_ = np.power( eps + loss_ / N  , q_/p_ )       \n",
    "        \n",
    "        loss_ = np.mean( np.power( np.power( eps + loss_,1./q_) , r_) , axis = 0)\n",
    "    \n",
    "    return loss_\n",
    "\n",
    "def compute_WeightedLoss_Lpqr(x,w,p_=2.,q_=2.,r_=2.,eps=1e-10):\n",
    "\n",
    "    loss_ = torch.nansum( torch.pow( torch.abs(eps + x), p_ )  , dim = 3)\n",
    "    loss_ = torch.sum( loss_ , dim = 2)\n",
    " \n",
    "    N = torch.sum( torch.sum(~torch.isnan(x) , dim = 3) , dim = 2)\n",
    "    loss_ = torch.pow( eps + loss_ / N  , q_/p_ )       \n",
    "    \n",
    "    loss_ = torch.sum( loss_ * w.repeat(x.shape[0],1) , dim = 1 )\n",
    "    loss_ = torch.mean( torch.pow(torch.pow( eps + loss_,1./q_) , r_) , dim = 0)\n",
    "    \n",
    "    return loss_\n",
    "\n",
    "class Model_NomrLpqr(torch.nn.Module):\n",
    "    def __init__(self,p=2.,q=2.,r=2.,trainable=False):\n",
    "        super(Model_NomrLpqr, self).__init__()\n",
    "        #self.p = torch.nn.Parameter(torch.Tensor([np.exp(p-1.)-1.]), requires_grad=True)\n",
    "        #self.q = torch.nn.Parameter(torch.Tensor([np.exp(q-1.)-1.]), requires_grad=True)\n",
    "        self.trainable = trainable\n",
    "\n",
    "        if FLAG_TRAINABLE_NORM == True :\n",
    "            self.p = torch.nn.Parameter(torch.Tensor([p]), requires_grad=True)\n",
    "            self.q = torch.nn.Parameter(torch.Tensor([q]), requires_grad=True)\n",
    "            self.r = torch.nn.Parameter(torch.Tensor([r]), requires_grad=True)\n",
    "        else:\n",
    "            self.p = torch.nn.Parameter(torch.Tensor([p]), requires_grad=False)\n",
    "            self.q = torch.nn.Parameter(torch.Tensor([q]), requires_grad=False)\n",
    "            self.r = torch.nn.Parameter(torch.Tensor([r]), requires_grad=False)\n",
    "\n",
    "        self.eps = torch.nn.Parameter(torch.Tensor([1e-10]), requires_grad=False)\n",
    "        \n",
    "    def forward(self,x,w,eps):\n",
    "\n",
    "        p_,q_,r_ = self.compute_pqr()\n",
    "            \n",
    "        loss_ = torch.nansum( torch.pow( torch.abs(self.eps + x), p_ )  , dim = 3)\n",
    "        loss_ = torch.nansum( loss_ , dim = 2)\n",
    " \n",
    "        N = torch.sum(~torch.isnan(x) , dim = 3)\n",
    "        N = torch.sum( N , dim = 2)\n",
    "        loss_ = torch.pow( self.eps + loss_ / N  , q_/p_ )       \n",
    "        \n",
    "        loss_ = torch.nansum( loss_ * w.repeat(x.shape[0],1) , dim = 1 )\n",
    "        loss_ = torch.mean( torch.pow(torch.pow( self.eps + loss_,1./q_) , r_) , dim = 0)\n",
    "\n",
    "        return loss_\n",
    "    def compute_pqr(self):\n",
    "        \n",
    "        if 1*0 :\n",
    "            p_ = 1. + torch.log( 1 + torch.abs(self.eps + self.p ) )\n",
    "            q_ = 1. + torch.log( 1 + torch.abs(self.eps + self.q ) )\n",
    "        else:\n",
    "           p_ = torch.abs(self.eps + self.p )\n",
    "           q_ = torch.abs(self.eps + self.q )\n",
    "           r_ = torch.abs(self.eps + self.r )\n",
    "\n",
    "        return p_,q_,r_\n",
    "\n",
    "alpha_ssh2u  = None#42.20436766972647\n",
    "alpha_ssh2v = None#77.99700321505073\n",
    "\n",
    "if alpha_ssh2u is None or alpha_ssh2v is None:\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    graduv = Compute_graduv().to(device)\n",
    "    running_loss_GOI = 0.\n",
    "    running_loss_OI = 0.\n",
    "    num_loss = 0\n",
    "    \n",
    "    num_ssh2u = 0.\n",
    "    denum_ssh2u = 0.\n",
    "    num_ssh2v = 0.\n",
    "    denum_ssh2v = 0.\n",
    "\n",
    "    for ssh_OI, inputs_obs,inputs_Mask, inputs_SST, ssh_gt, u_gt, v_gt, vv_gt in dataloaders['train']:\n",
    "        ssh_gt   = ssh_gt.to(device)\n",
    "        u_gt     = u_gt.to(device)\n",
    "        v_gt     = v_gt.to(device)\n",
    "        \n",
    "        # gradient norm field\n",
    "        g_u, g_v = graduv(ssh_gt)\n",
    "        \n",
    "        num_ssh2u += torch.sum( -1. * g_v * u_gt[:,:,1:-1,1:-1] )\n",
    "        denum_ssh2u += torch.sum(  g_v * g_v )\n",
    "        \n",
    "        num_ssh2v += torch.sum( g_u * v_gt[:,:,1:-1,1:-1] )\n",
    "        denum_ssh2v += torch.sum( g_u * g_u )\n",
    "                \n",
    "        #print(' Correlation dv_ssh/u : %f'% (torch.mean( g_v * u_gt[:,:,1:-1,1:-1] ) / torch.sqrt( torch.mean( g_v * g_v ) * torch.mean( u_gt[:,:,1:-1,1:-1] * u_gt[:,:,1:-1,1:-1] ) )))\n",
    "        #print(' Correlation du_ssh/v : %f'% (torch.mean( g_u * v_gt[:,:,1:-1,1:-1] ) / torch.sqrt( torch.mean( g_u * g_u ) * torch.mean( v_gt[:,:,1:-1,1:-1] * v_gt[:,:,1:-1,1:-1] ) )))\n",
    "        #print('%f %f '%(torch.mean( -1. * g_v * u_gt[:,:,1:-1,1:-1] )/torch.mean(  g_v * g_v ),torch.mean( g_u * v_gt[:,:,1:-1,1:-1] )/torch.mean( g_u * g_u )))\n",
    "        #print( torch.mean( g_u * u_gt[:,:,1:-1,1:-1] ) / torch.sqrt( torch.mean( g_u * g_u ) * torch.mean( u_gt[:,:,1:-1,1:-1] * u_gt[:,:,1:-1,1:-1] ) ))\n",
    "        #print( torch.mean( g_v * v_gt[:,:,1:-1,1:-1] ) / torch.sqrt( torch.mean( g_v * g_v ) * torch.mean( v_gt[:,:,1:-1,1:-1] * v_gt[:,:,1:-1,1:-1] ) ))\n",
    "\n",
    "    alpha_ssh2u = num_ssh2u / denum_ssh2u\n",
    "    alpha_ssh2v = num_ssh2v / denum_ssh2v\n",
    "    \n",
    "    alpha_ssh2u = alpha_ssh2u.detach().cpu().numpy()\n",
    "    alpha_ssh2v = alpha_ssh2v.detach().cpu().numpy()\n",
    "\n",
    "    print('.... SSH to (u,v) (normalized) : %f / %f '%(  alpha_ssh2u , alpha_ssh2v ))\n",
    "    print('.... SSH to (u,v) : %f / %f '%( std_uv * alpha_ssh2u / stdTr , std_uv * alpha_ssh2v / stdTr ))\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2f6a54cf",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'XrDataset' object has no attribute 'dim'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-66d55893fce3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'XrDataset' object has no attribute 'dim'"
     ]
    }
   ],
   "source": [
    "\n",
    "############################################Lightning Module#######################################################################\n",
    "class HParam:\n",
    "    def __init__(self):\n",
    "        self.iter_update     = []\n",
    "        self.nb_grad_update  = []\n",
    "        self.lr_update       = []\n",
    "        self.n_grad          = 1\n",
    "        self.dim_grad_solver = 10\n",
    "        self.dropout         = 0.25\n",
    "        self.w_loss          = []\n",
    "        self.automatic_optimization = True\n",
    "        self.k_batch         = 1\n",
    "        self.flag_uv_param = \"u-v\"\n",
    "        \n",
    "        self.GradType       = 1 \n",
    "        self.OptimType      = 2 \n",
    "\n",
    "        self.alpha_proj    = 0.5\n",
    "        self.alpha_sr      = 0.5\n",
    "        self.alpha_lr      = 0.5  # 1e4\n",
    "        self.alpha_mse_ssh = 10.\n",
    "        self.alpha_mse_gssh = 1.\n",
    "        #self.alpha_mse_uv = 1.\n",
    "\n",
    "        self.alpha_mse_vv = 1.\n",
    "        self.flag_median_output = False\n",
    "        self.median_filter_width = width_med_filt_spatial\n",
    "        self.dw_loss = 32\n",
    "\n",
    "        self.p_norm_loss = 2. \n",
    "        self.q_norm_loss = 2. \n",
    "        self.r_norm_loss = 2. \n",
    "        self.thr_norm_loss = 2.\n",
    "\n",
    "        self.flagNoSSTObs = False\n",
    "        \n",
    "                          \n",
    "        self.alpha          = np.array([1.,0.1])\n",
    "        self.alpha4DVar     = np.array([0.01,1.])#np.array([0.30,1.60])#\n",
    "\n",
    "        self.flagLearnWithObsOnly = False #True # \n",
    "        self.lambda_LRAE          = 0.5 # 0.5\n",
    "\n",
    "        self.GradType       = 1 # Gradient computation (0: subgradient, 1: true gradient/autograd)\n",
    "        self.OptimType      = 2 # 0: fixed-step gradient descent, 1: ConvNet_step gradient descent, 2: LSTM-based descent\n",
    "        \n",
    "        self.NbProjection   = [0,0,0,0,0,0,0]#[0,0,0,0,0,0]#[5,5,5,5,5]##\n",
    "        self.NBProjCurrent =0\n",
    "         \n",
    "        \n",
    "'''\n",
    "\n",
    "############################################Lightning Module#######################################################################\n",
    "class HParam:\n",
    "    def __init__(self):\n",
    "        self.iter_update     = []\n",
    "        self.nb_grad_update  = []\n",
    "        self.lr_update       = []\n",
    "        self.n_grad          = 1\n",
    "        self.dim_grad_solver = 10\n",
    "        self.dropout         = 0.25\n",
    "        self.w_loss          = []\n",
    "        self.automatic_optimization = True\n",
    "        self.k_batch         = 1\n",
    "        self.flag_uv_param = \"u-v\"\n",
    "\n",
    "        self.alpha_proj    = 0.5\n",
    "        self.alpha_sr      = 0.5\n",
    "        self.alpha_lr      = 0.5  # 1e4\n",
    "        self.alpha_mse_ssh = 10.\n",
    "        self.alpha_mse_gssh = 1.\n",
    "        #self.alpha_mse_uv = 1.\n",
    "\n",
    "        self.alpha_mse_vv = 1.\n",
    "        self.flag_median_output = False\n",
    "        self.median_filter_width = width_med_filt_spatial\n",
    "        self.dw_loss = 32\n",
    "\n",
    "        self.p_norm_loss = 2. \n",
    "        self.q_norm_loss = 2. \n",
    "        self.r_norm_loss = 2. \n",
    "        self.thr_norm_loss = 2.\n",
    "\n",
    "        self.flagNoSSTObs = False\n",
    "        \n",
    " '''       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7246abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "       \n",
    "class LitModel(pl.LightningModule):\n",
    "    def __init__(self,conf=HParam(),*args, **kwargs):\n",
    "        super().__init__()\n",
    "                         \n",
    "        self.hparams.alpha          = np.array([1.,0.1])\n",
    "        self.hparams.alpha4DVar     = np.array([0.01,1.])#np.array([0.30,1.60])#\n",
    "\n",
    "        self.hparams.flagLearnWithObsOnly = False #True # \n",
    "        self.hparams.lambda_LRAE          = 0.5 # 0.5\n",
    "\n",
    "        self.hparams.GradType       = 1 # Gradient computation (0: subgradient, 1: true gradient/autograd)\n",
    "        self.hparams.OptimType      = 2 # 0: fixed-step gradient descent, 1: ConvNet_step gradient descent, 2: LSTM-based descent\n",
    "        \n",
    "        self.hparams.NbProjection   = [0,0,0,0,0,0,0]#[0,0,0,0,0,0]#[5,5,5,5,5]##\n",
    "        self.hparams.iter_update     = [0,30,100,150,200,250,400]  # [0,2,4,6,9,15]\n",
    "        self.hparams.nb_grad_update  = [0,5,10,15,20,20,20]  # [0,0,1,2,3,3]#[0,2,2,4,5,5]#\n",
    "        self.hparams.lr_update       = [1e-3,1e-4,1e-4,1e-4,1e-4,1e-5,1e-6,1e-6,1e-7]\n",
    "        self.hparams.k_batch         = 1\n",
    "        \n",
    "         \n",
    "        self.hparams.n_grad          = self.hparams.nb_grad_update[0]\n",
    "        self.hparams.dim_grad_solver = dimGradSolver\n",
    "        self.hparams.dropout         = rateDropout\n",
    "        \n",
    "        self.hparams.alpha_proj    = 0.5\n",
    "        self.hparams.alpha_sr      = 0.5\n",
    "        self.hparams.alpha_lr      = 0.5  # 1e4\n",
    "        self.hparams.alpha_mse_ssh = 10.\n",
    "        self.hparams.alpha_mse_gssh = 1.\n",
    "        self.hparams.alpha_mse_vv = 1.\n",
    "        \n",
    "        self.hparams.w_loss          = torch.nn.Parameter(torch.Tensor(w_), requires_grad=False)\n",
    "        self.hparams.automatic_optimization = False#True#\n",
    "\n",
    "        self.hparams.alpha_ssh2u = alpha_ssh2u\n",
    "        self.hparams.alpha_ssh2v = alpha_ssh2v\n",
    "        self.hparams.d_du = 1.\n",
    "        self.hparams.d_dv = alpha_ssh2u / alpha_ssh2v\n",
    "        \n",
    "        self.hparams.NBProjCurrent =self.hparams.NbProjection[0]\n",
    "        \n",
    "        #lrCurrent       = lrUpdate[0] ??\n",
    "            \n",
    "        \n",
    "        self.model           = NN_4DVar.Model_4DVarNN_GradFP(Phi_r(),shapeData,self.hparams.NBProjCurrent,self.hparams.n_grad,GradType,self.hparams.OptimType,InterpFlag,UsePriodicBoundary)        \n",
    "        \n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        # hyperparameters\n",
    "       \n",
    "        # main model\n",
    "        self.model        = NN_4DVar.Solver_Grad_4DVarNN(Phi_r(), \n",
    "                                                         NN_4DVar.model_GradUpdateLSTM(shapeData, UsePriodicBoundary, self.hparams.dim_grad_solver, self.hparams.dropout), \n",
    "                                                         None, None, shapeData, self.hparams.n_grad)\n",
    "\n",
    "        #self.gradient_img = Gradient_img()\n",
    "        #self.compute_div = Div_uv()\n",
    "        self.w_loss       = self.hparams.w_loss # duplicate for automatic upload to gpu\n",
    "        self.x_pred        = None # variable to store output of test method\n",
    "        \n",
    "        self.automatic_optimization = self.hparams.automatic_optimization\n",
    "        self.curr = 0\n",
    "        \n",
    "    def forward(self):\n",
    "        return 1\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        #optimizer = optim.Adam(self.model.parameters(), lr= self.lrUpdate[0])\n",
    "        optimizer   = optim.Adam([{'params': self.model.model_Grad.parameters(),'lr': self.hparams.lr_update[0]},\n",
    "                                    {'params': self.model.phi_r.parameters(), 'lr': self.hparams.lr_update[0]}\n",
    "                                    ], lr=0.)       \n",
    "        return optimizer\n",
    "    \n",
    "    def on_epoch_start(self):\n",
    "        # enfore acnd check some hyperparameters \n",
    "        self.model.n_grad   = self.hparams.n_grad \n",
    "        \n",
    "    def on_train_epoch_start(self):\n",
    "        opt = self.optimizers()\n",
    "        if (self.current_epoch in self.hparams.iter_update) & (self.current_epoch > 0):\n",
    "            indx             = self.hparams.iter_update.index(self.current_epoch)\n",
    "            print('... Update Iterations number/learning rate #%d: NGrad = %d -- lr = %f'%(self.current_epoch,self.hparams.nb_grad_update[indx],self.hparams.lr_update[indx]))\n",
    "            \n",
    "            self.hparams.n_grad = self.hparams.nb_grad_update[indx]\n",
    "            self.model.n_grad   = self.hparams.n_grad \n",
    "            \n",
    "            mm = 0\n",
    "            lrCurrent = self.hparams.lr_update[indx]\n",
    "            lr = np.array([lrCurrent,lrCurrent,lrCurrent,0.5*lrCurrent,lrCurrent,0.])            \n",
    "            for pg in opt.param_groups:\n",
    "                pg['lr'] = lr[mm]# * self.hparams.learning_rate\n",
    "                mm += 1\n",
    "        \n",
    "    def training_step(self, train_batch, batch_idx, optimizer_idx=0):\n",
    "        opt = self.optimizers()\n",
    "                    \n",
    "        # compute loss and metrics    \n",
    "        loss, out, metrics = self.compute_loss(train_batch, phase='train')\n",
    "\n",
    "        # log step metric        \n",
    "        #self.log('train_mse', mse)\n",
    "        #self.log(\"dev_loss\", mse / var_Tr , on_step=True, on_epoch=True, prog_bar=True, sync_dist=True)\n",
    "        self.log(\"loss\", loss , on_step=True, on_epoch=True, prog_bar=True, sync_dist=True)\n",
    "        self.log(\"tr_mse\", metrics['mse'] / var_Tr , on_step=False, on_epoch=True, prog_bar=True, sync_dist=True)\n",
    "        self.log(\"tr_mse\", metrics['mse_uv'], on_step=False, on_epoch=True, prog_bar=True, sync_dist=True)\n",
    "        #self.log(\"tr_mseG\", metrics['mse_grad'] / metrics['meanGrad'] , on_step=False, on_epoch=True, prog_bar=True, sync_dist=True)\n",
    "        \n",
    "        # initial grad value\n",
    "        if self.hparams.automatic_optimization == False :\n",
    "            # backward\n",
    "            self.manual_backward(loss)\n",
    "        \n",
    "            if (batch_idx + 1) % self.hparams.k_batch == 0:\n",
    "                # optimisation step\n",
    "                opt.step()\n",
    "                \n",
    "                # grad initialization to zero\n",
    "                opt.zero_grad()\n",
    "         \n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, val_batch, batch_idx):\n",
    "        loss, out, metrics = self.compute_loss(val_batch, phase='val')\n",
    "\n",
    "        self.log('val_loss', loss)\n",
    "        self.log(\"val_mse\", metrics['mse'] / var_Val , on_step=False, on_epoch=True, prog_bar=True, sync_dist=True)\n",
    "        self.log(\"val_uv\", metrics['mse_uv']  , on_step=False, on_epoch=True, prog_bar=True, sync_dist=True)\n",
    "        self.log(\"val_mseG\", metrics['mse_grad'] / metrics['meanGrad'] , on_step=False, on_epoch=True, prog_bar=True, sync_dist=True)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, test_batch, batch_idx):\n",
    "        loss, out, metrics = self.compute_loss(test_batch, phase='test')\n",
    "\n",
    "        self.log('test_loss', loss)\n",
    "        self.log(\"test_mse\", metrics['mse'] / var_Tt , on_step=False, on_epoch=True, prog_bar=True, sync_dist=True)\n",
    "        self.log(\"test_mseG\", metrics['mse_grad'] / metrics['meanGrad'] , on_step=False, on_epoch=True, prog_bar=True, sync_dist=True)\n",
    "        self.log(\"test_vv\", metrics['mse_vv']  , on_step=False, on_epoch=True, prog_bar=True, sync_dist=True)\n",
    "        \n",
    "        out_ssh,out_u_geo,out_v_geo,out_u,out_v, ssh_obs,sst_feat  = out\n",
    "        return {'preds_ssh': out_ssh.detach().cpu(),'preds_u_geo': out_u_geo.detach().cpu(),'preds_v_geo': out_v_geo.detach().cpu(),'preds_u': out_u.detach().cpu(),'preds_v': out_v.detach().cpu(),'obs_ssh': ssh_obs.detach().cpu(),'feat_sst': sst_feat.detach().cpu()}\n",
    "\n",
    "    def training_epoch_end(self, training_step_outputs):\n",
    "        # do something with all training_step outputs\n",
    "        print('.. \\n')\n",
    "    \n",
    "    def test_epoch_end(self, outputs):\n",
    "        x_test_rec = torch.cat([chunk['preds_ssh'] for chunk in outputs]).numpy()\n",
    "        x_test_rec = stdTr * x_test_rec + meanTr\n",
    "        \n",
    "        self.x_rec_ssh = x_test_rec[:,int(dT/2),:,:]\n",
    "\n",
    "        x_test_u = torch.cat([chunk['preds_u_geo'] for chunk in outputs]).numpy()\n",
    "        x_test_u = std_uv * x_test_u\n",
    "        self.x_rec_u_geo = x_test_u[:,int(dT/2),:,:]\n",
    "\n",
    "        x_test_v = torch.cat([chunk['preds_v_geo'] for chunk in outputs]).numpy()\n",
    "        x_test_v = std_uv * x_test_v\n",
    "        self.x_rec_v_geo = x_test_v[:,int(dT/2),:,:]\n",
    "\n",
    "        x_test_u = torch.cat([chunk['preds_u'] for chunk in outputs]).numpy()\n",
    "        x_test_u = std_uv * x_test_u\n",
    "        self.x_rec_u = x_test_u[:,int(dT/2),:,:]\n",
    "\n",
    "        x_test_v = torch.cat([chunk['preds_v'] for chunk in outputs]).numpy()\n",
    "        x_test_v = std_uv * x_test_v\n",
    "        self.x_rec_v = x_test_v[:,int(dT/2),:,:]\n",
    "\n",
    "        x_test_vv = torch.cat([chunk['preds_vv'] for chunk in outputs]).numpy()\n",
    "        x_test_vv = std_vv * x_test_vv\n",
    "        self.x_rec_vv = x_test_vv[:,int(dT/2),:,:]\n",
    "\n",
    "        x_test_vv = torch.cat([chunk['preds_vv_swot'] for chunk in outputs]).numpy()\n",
    "        x_test_vv = std_vv * x_test_vv\n",
    "        self.x_rec_vv_swot = x_test_vv[:,int(dT/2),:,:]\n",
    "\n",
    "        x_test_ssh_obs = torch.cat([chunk['obs_ssh'] for chunk in outputs]).numpy()\n",
    "        x_test_ssh_obs[ x_test_ssh_obs == 0. ] = np.float('NaN')\n",
    "        x_test_ssh_obs = stdTr * x_test_ssh_obs + meanTr\n",
    "        self.x_rec_ssh_obs = x_test_ssh_obs[:,int(dT/2),:,:]\n",
    "\n",
    "        x_test_sst_feat = torch.cat([chunk['feat_sst'] for chunk in outputs]).numpy()\n",
    "        self.x_feat_sst = x_test_sst_feat\n",
    "\n",
    "        return 1.\n",
    "\n",
    "    def compute_loss(self, batch, phase):\n",
    "\n",
    "        ssh_OI, imputs_obs, inputs_Mask, inputs_SST, ssh_GT, u_GT, v_GT, vv_GT = batch\n",
    "\n",
    "        new_masks      = torch.cat((1. + 0. * inputs_Mask, inputs_Mask, 0. * inputs_Mask , 0. * inputs_Mask ), dim=1)\n",
    "        inputs_init    = torch.cat((ssh_OI, inputs_Mask * (ssh_GT - ssh_OI) , 0. * ssh_GT , 0. * ssh_GT ), dim=1)\n",
    "        #inputs_missing = torch.cat((ssh_OI, inputs_Mask * (ssh_GT - ssh_OI) , 0. * ssh_GT , 0. * ssh_GT), dim=1)\n",
    "        inputs_missing = torch.cat((ssh_OI, inputs_Mask * (imputs_obs - ssh_OI) , 0. * ssh_GT , 0. * ssh_GT), dim=1)\n",
    "        \n",
    "        mask_SST       = 1. + 0. * inputs_SST\n",
    "\n",
    "        # gradient norm field\n",
    "        g_targets_GT = self.gradient_img(ssh_GT)\n",
    "        g_targets_GT = g_targets_GT[0]\n",
    "        \n",
    "       # need to evaluate grad/backward during the evaluation and training phase for phi_r\n",
    "        with torch.set_grad_enabled(True):\n",
    "            # with torch.set_grad_enabled(phase == 'train'):\n",
    "            inputs_init = torch.autograd.Variable(inputs_init, requires_grad=True)\n",
    "\n",
    "            outputs, hidden_new, cell_new, normgrad = self.model(inputs_init, [inputs_missing,inputs_SST], [new_masks,mask_SST])\n",
    "\n",
    "            if (phase == 'val') or (phase == 'test'):\n",
    "                outputs = outputs.detach()\n",
    "\n",
    "            outputsSLRHR = outputs\n",
    "            outputs_ssh_lr = outputs[:, 0:dT, :, :]\n",
    "            outputs_ssh = outputs[:, 0:dT, :, :] + outputs[:, dT:2*dT, :, :]\n",
    "            outputs_uv1 = outputs[:, 2*dT:3*dT, :, :]  \n",
    "            outputs_uv2 = outputs[:, 3*dT:4*dT, :, :]  \n",
    "\n",
    "            # reconstruction losses for SSH\n",
    "            g_outputs = self.gradient_img(outputs_ssh)\n",
    "            g_outputs = g_outputs[0]\n",
    "            loss_All   = NN_4DVar.compute_WeightedLoss((outputs_ssh - ssh_GT), self.w_loss)\n",
    "            loss_GAll  = NN_4DVar.compute_WeightedLoss(g_outputs - g_targets_GT, self.w_loss)\n",
    "\n",
    "            loss_OI    = NN_4DVar.compute_WeightedLoss(ssh_GT - ssh_OI, self.w_loss)\n",
    "            \n",
    "            loss_GOI   = NN_4DVar.compute_WeightedLoss(self.gradient_img(ssh_OI)[0] - g_targets_GT, self.w_loss)\n",
    "\n",
    "            # reconstruction losses for current \n",
    "            if self.hparams.flag_uv_param =='div-curl':            \n",
    "                curr_curl = self.model_pot2curr(outputs_ssh)\n",
    "                curr_dcurl = self.model_pot2curr(outputs_uv1)\n",
    "                curr_div = self.model_pot2curr(outputs_uv2)\n",
    "\n",
    "                outputs_u = curr_curl[1] + curr_dcurl[1] + curr_div[0]\n",
    "                outputs_v = -1. * curr_curl[0] - 1. * curr_dcurl[0] + curr_div[1]\n",
    "    \n",
    "                outputs_u_geo = curr_curl[1] + curr_dcurl[1]\n",
    "                outputs_v_geo = -1. * curr_curl[0] - 1. * curr_dcurl[0] \n",
    "\n",
    "                loss_uv = NN_4DVar.compute_WeightedLoss((outputs_u - u_GT[:,:,1:-1,1:-1]), self.w_loss)\n",
    "                loss_uv += NN_4DVar.compute_WeightedLoss((outputs_v - v_GT[:,:,1:-1,1:-1]), self.w_loss)\n",
    "                \n",
    "                loss_uv += NN_4DVar.compute_WeightedLoss((outputs_u_geo - u_GT[:,:,1:-1,1:-1]), self.w_loss)\n",
    "                loss_uv += NN_4DVar.compute_WeightedLoss((outputs_v_geo - v_GT[:,:,1:-1,1:-1]), self.w_loss)\n",
    "            else:\n",
    "                outputs_u = outputs_uv1\n",
    "                outputs_v = outputs_uv2\n",
    "\n",
    "                curr_ssh = self.model_pot2curr(outputs_ssh)\n",
    "                               \n",
    "                outputs_u_geo = curr_ssh[1]\n",
    "                outputs_v_geo = -1. * curr_ssh[0]              \n",
    "                \n",
    "                #loss_uv = NN_4DVar.compute_WeightedLoss((outputs_u - u_GT), self.w_loss)\n",
    "                #loss_uv += NN_4DVar.compute_WeightedLoss((outputs_v - v_GT), self.w_loss)\n",
    "                \n",
    "                #loss_uv += NN_4DVar.compute_WeightedLoss((outputs_u_geo - u_GT[:,:,1:-1,1:-1]), self.w_loss)\n",
    "                #loss_uv += NN_4DVar.compute_WeightedLoss((outputs_v_geo - v_GT[:,:,1:-1,1:-1]), self.w_loss)\n",
    "                loss_uv = torch.sqrt( 1e-10 + (outputs_u - u_GT)**2 + (outputs_v - v_GT) **2 )\n",
    "                loss_uv = compute_WeightedLoss_Lpqr(loss_uv, self.w_loss,self.hparams.p_norm_loss,self.hparams.q_norm_loss,self.hparams.r_norm_loss)\n",
    "                \n",
    "                loss_uv2 = torch.sqrt( 1e-10 + (outputs_u_geo - u_GT[:,:,1:-1,1:-1])**2 + (outputs_v_geo - v_GT[:,:,1:-1,1:-1]) **2 )\n",
    "                loss_uv += compute_WeightedLoss_Lpqr(loss_uv2, self.w_loss,self.hparams.p_norm_loss,self.hparams.q_norm_loss,self.hparams.r_norm_loss)\n",
    "\n",
    "                #loss_uv += NN_4DVar.compute_WeightedLoss((outputs_u_geo - u_GT[:,:,1:-1,1:-1]), self.w_loss)\n",
    "                #loss_uv += NN_4DVar.compute_WeightedLoss((outputs_v_geo - v_GT[:,:,1:-1,1:-1]), self.w_loss)\n",
    "                #div_rec = self.compute_div(outputs_u,outputs_v)\n",
    "                #div_gt = self.compute_div(u_GT,v_GT)\n",
    "                \n",
    "                div_rec = model_div(outputs_u,outputs_v)\n",
    "                div_gt = model_div(u_GT,v_GT)\n",
    "\n",
    "                outputs_u = outputs_u[:,:,1:-1,1:-1]\n",
    "                outputs_v = outputs_v[:,:,1:-1,1:-1]\n",
    "\n",
    "                loss_div = NN_4DVar.compute_WeightedLoss((div_rec - div_gt), self.w_loss)\n",
    "                #loss_uv += 10. * loss_div\n",
    "                \n",
    "                if self.current_epoch == 1:\n",
    "                    self.curr = 1\n",
    "                    \n",
    "                if self.curr == self.current_epoch :\n",
    "                    #print( np.mean( (div_gt.detach().cpu().numpy())**2 ), flush=True )\n",
    "                    #print( np.mean( (div_rec.detach().cpu().numpy())**2 ), flush=True )\n",
    "\n",
    "                    self.curr += 1\n",
    "\n",
    "            # projection losses\n",
    "            loss_AE     = torch.mean((self.model.phi_r(outputsSLRHR) - outputsSLRHR) ** 2)\n",
    "            yGT         = torch.cat((ssh_GT,ssh_GT-ssh_OI,outputs_uv1,outputs_uv2),dim=1)\n",
    "            loss_AE_GT = torch.mean((self.model.phi_r(yGT) - yGT) ** 2)\n",
    "\n",
    "            # low-resolution loss\n",
    "            loss_SR      = NN_4DVar.compute_WeightedLoss(outputs_ssh_lr - ssh_OI, self.w_loss)\n",
    "            targets_GTLR = self.model_LR(ssh_OI)\n",
    "            loss_LR      = NN_4DVar.compute_WeightedLoss(self.model_LR(outputs_ssh) - targets_GTLR, self.w_loss)\n",
    "\n",
    "            # total loss\n",
    "            loss = self.hparams.alpha_mse_ssh * loss_All + self.hparams.alpha_mse_gssh * loss_GAll + self.hparams.alpha_mse_uv * loss_uv\n",
    "            loss += 0.5 * self.hparams.alpha_proj * (loss_AE + loss_AE_GT)\n",
    "            loss    += self.hparams.alpha_lr * loss_LR + self.hparams.alpha_sr * loss_SR\n",
    "            \n",
    "            # metrics\n",
    "            mean_gall = NN_4DVar.compute_WeightedLoss(g_targets_GT,self.w_loss)\n",
    "            mse = loss_All.detach()\n",
    "            mse_grad   = loss_GAll.detach()\n",
    "            mse_uv = loss_uv\n",
    "            metrics   = dict([('mse',mse),('mse_grad',mse_grad),('mse_uv',mse_uv),('meanGrad',mean_gall),('mseOI',loss_OI.detach()),('mseGOI',loss_GOI.detach())])\n",
    "            #print(mse.cpu().detach().numpy())\n",
    "            \n",
    "            outputs = [outputs_ssh,outputs_u_geo,outputs_v_geo,outputs_u,outputs_v,inputs_missing[:,dT:2*dT,:,:]]\n",
    "        return loss,outputs, metrics\n",
    "    \n",
    "    \n",
    "'''\n",
    "      \n",
    "class LitModel(pl.LightningModule):\n",
    "    def __init__(self,conf=HParam(),*args, **kwargs):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        # hyperparameters\n",
    "        self.hparams.iter_update     = [0, 20, 40, 60, 100, 150, 800]  # [0,2,4,6,9,15]\n",
    "        self.hparams.nb_grad_update  = [5, 5, 10, 10, 15, 15, 20, 20, 20]  # [0,0,1,2,3,3]#[0,2,2,4,5,5]#\n",
    "        self.hparams.lr_update       = [1e-3, 1e-3, 1e-3, 1e-4, 1e-4, 1e-5, 1e-5, 1e-6, 1e-7]\n",
    "        self.hparams.k_batch         = 1\n",
    "        \n",
    "        self.hparams.n_grad          = self.hparams.nb_grad_update[0]\n",
    "        self.hparams.dim_grad_solver = dimGradSolver\n",
    "        self.hparams.dropout         = rateDropout\n",
    "        \n",
    "        self.hparams.alpha_proj    = 0.5\n",
    "        self.hparams.alpha_sr      = 0.5\n",
    "        self.hparams.alpha_lr      = 0.5  # 1e4\n",
    "        self.hparams.alpha_mse_ssh = 10.\n",
    "        self.hparams.alpha_mse_gssh = 1.\n",
    "        self.hparams.alpha_mse_vv = 1.\n",
    "        \n",
    "        self.hparams.w_loss          = torch.nn.Parameter(torch.Tensor(w_), requires_grad=False)\n",
    "        self.hparams.automatic_optimization = False#True#\n",
    "\n",
    "        self.hparams.alpha_ssh2u = alpha_ssh2u\n",
    "        self.hparams.alpha_ssh2v = alpha_ssh2v\n",
    "        self.hparams.d_du = 1.\n",
    "        self.hparams.d_dv = alpha_ssh2u / alpha_ssh2v\n",
    "\n",
    "        # main model\n",
    "        self.model        = NN_4DVar.Solver_Grad_4DVarNN(Phi_r(), \n",
    "                                                         Model_H(width_kernel=W_KERNEL_MODEL_H,dim=1), \n",
    "                                                         NN_4DVar.model_GradUpdateLSTM(shapeData, UsePriodicBoundary, self.hparams.dim_grad_solver, self.hparams.dropout), \n",
    "                                                         None, None, shapeData, self.hparams.n_grad)\n",
    "\n",
    "        self.model_LR     = ModelLR()\n",
    "        #self.gradient_img = Gradient_img()\n",
    "        #self.compute_div = Div_uv()\n",
    "        self.compute_graduv = Compute_graduv()\n",
    "        self.w_loss       = self.hparams.w_loss # duplicate for automatic upload to gpu\n",
    "        self.x_rec_ssh        = None # variable to store output of test method\n",
    "        self.x_rec_u        = None # variable to store output of test method\n",
    "        self.x_rec_v        = None # variable to store output of test method\n",
    "        self.x_rec_ssh_obs = None\n",
    "        self.x_rec_u_geo = None # variable to store output of test method\n",
    "        self.x_rec_v_geo = None # variable to store output of test method\n",
    "        self.x_feat_sst = None\n",
    "        \n",
    "        self.automatic_optimization = self.hparams.automatic_optimization\n",
    "        self.curr = 0\n",
    "        \n",
    "    def forward(self):\n",
    "        return 1\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        #optimizer = optim.Adam(self.model.parameters(), lr= self.lrUpdate[0])\n",
    "        optimizer   = optim.Adam([{'params': self.model.model_Grad.parameters(), 'lr': self.hparams.lr_update[0]},\n",
    "                                  {'params': self.model.model_VarCost.parameters(), 'lr': self.hparams.lr_update[0]},\n",
    "                                {'params': self.model.model_H.parameters(), 'lr': self.hparams.lr_update[0]},\n",
    "                                {'params': self.model.phi_r.parameters(), 'lr': 0.5*self.hparams.lr_update[0]},\n",
    "                                {'params': self.compute_graduv.parameters(), 'lr': self.hparams.lr_update[0]},\n",
    "                                ], lr=0.)\n",
    "        return optimizer\n",
    "    \n",
    "    def on_epoch_start(self):\n",
    "        # enfore acnd check some hyperparameters \n",
    "        self.model.n_grad   = self.hparams.n_grad \n",
    "        \n",
    "    def on_train_epoch_start(self):\n",
    "        opt = self.optimizers()\n",
    "        if (self.current_epoch in self.hparams.iter_update) & (self.current_epoch > 0):\n",
    "            indx             = self.hparams.iter_update.index(self.current_epoch)\n",
    "            print('... Update Iterations number/learning rate #%d: NGrad = %d -- lr = %f'%(self.current_epoch,self.hparams.nb_grad_update[indx],self.hparams.lr_update[indx]))\n",
    "            \n",
    "            self.hparams.n_grad = self.hparams.nb_grad_update[indx]\n",
    "            self.model.n_grad   = self.hparams.n_grad \n",
    "            \n",
    "            mm = 0\n",
    "            lrCurrent = self.hparams.lr_update[indx]\n",
    "            lr = np.array([lrCurrent,lrCurrent,lrCurrent,0.5*lrCurrent,lrCurrent,0.])            \n",
    "            for pg in opt.param_groups:\n",
    "                pg['lr'] = lr[mm]# * self.hparams.learning_rate\n",
    "                mm += 1\n",
    "        \n",
    "    def training_step(self, train_batch, batch_idx, optimizer_idx=0):\n",
    "        opt = self.optimizers()\n",
    "                    \n",
    "        # compute loss and metrics    \n",
    "        loss, out, metrics = self.compute_loss(train_batch, phase='train')\n",
    "\n",
    "        # log step metric        \n",
    "        #self.log('train_mse', mse)\n",
    "        #self.log(\"dev_loss\", mse / var_Tr , on_step=True, on_epoch=True, prog_bar=True, sync_dist=True)\n",
    "        self.log(\"loss\", loss , on_step=True, on_epoch=True, prog_bar=True, sync_dist=True)\n",
    "        self.log(\"tr_mse\", metrics['mse'] / var_Tr , on_step=False, on_epoch=True, prog_bar=True, sync_dist=True)\n",
    "        self.log(\"tr_mse\", metrics['mse_uv'], on_step=False, on_epoch=True, prog_bar=True, sync_dist=True)\n",
    "        #self.log(\"tr_mseG\", metrics['mse_grad'] / metrics['meanGrad'] , on_step=False, on_epoch=True, prog_bar=True, sync_dist=True)\n",
    "        \n",
    "        # initial grad value\n",
    "        if self.hparams.automatic_optimization == False :\n",
    "            # backward\n",
    "            self.manual_backward(loss)\n",
    "        \n",
    "            if (batch_idx + 1) % self.hparams.k_batch == 0:\n",
    "                # optimisation step\n",
    "                opt.step()\n",
    "                \n",
    "                # grad initialization to zero\n",
    "                opt.zero_grad()\n",
    "         \n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, val_batch, batch_idx):\n",
    "        loss, out, metrics = self.compute_loss(val_batch, phase='val')\n",
    "\n",
    "        self.log('val_loss', loss)\n",
    "        self.log(\"val_mse\", metrics['mse'] / var_Val , on_step=False, on_epoch=True, prog_bar=True, sync_dist=True)\n",
    "        self.log(\"val_uv\", metrics['mse_uv']  , on_step=False, on_epoch=True, prog_bar=True, sync_dist=True)\n",
    "        self.log(\"val_mseG\", metrics['mse_grad'] / metrics['meanGrad'] , on_step=False, on_epoch=True, prog_bar=True, sync_dist=True)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, test_batch, batch_idx):\n",
    "        loss, out, metrics = self.compute_loss(test_batch, phase='test')\n",
    "\n",
    "        self.log('test_loss', loss)\n",
    "        self.log(\"test_mse\", metrics['mse'] / var_Tt , on_step=False, on_epoch=True, prog_bar=True, sync_dist=True)\n",
    "        self.log(\"test_mseG\", metrics['mse_grad'] / metrics['meanGrad'] , on_step=False, on_epoch=True, prog_bar=True, sync_dist=True)\n",
    "        self.log(\"test_vv\", metrics['mse_vv']  , on_step=False, on_epoch=True, prog_bar=True, sync_dist=True)\n",
    "        \n",
    "        out_ssh,out_u_geo,out_v_geo,out_u,out_v, ssh_obs,sst_feat  = out\n",
    "        return {'preds_ssh': out_ssh.detach().cpu(),'preds_u_geo': out_u_geo.detach().cpu(),'preds_v_geo': out_v_geo.detach().cpu(),'preds_u': out_u.detach().cpu(),'preds_v': out_v.detach().cpu(),'obs_ssh': ssh_obs.detach().cpu(),'feat_sst': sst_feat.detach().cpu()}\n",
    "\n",
    "    def training_epoch_end(self, training_step_outputs):\n",
    "        # do something with all training_step outputs\n",
    "        print('.. \\n')\n",
    "    \n",
    "    def test_epoch_end(self, outputs):\n",
    "        x_test_rec = torch.cat([chunk['preds_ssh'] for chunk in outputs]).numpy()\n",
    "        x_test_rec = stdTr * x_test_rec + meanTr\n",
    "        \n",
    "        self.x_rec_ssh = x_test_rec[:,int(dT/2),:,:]\n",
    "\n",
    "        x_test_u = torch.cat([chunk['preds_u_geo'] for chunk in outputs]).numpy()\n",
    "        x_test_u = std_uv * x_test_u\n",
    "        self.x_rec_u_geo = x_test_u[:,int(dT/2),:,:]\n",
    "\n",
    "        x_test_v = torch.cat([chunk['preds_v_geo'] for chunk in outputs]).numpy()\n",
    "        x_test_v = std_uv * x_test_v\n",
    "        self.x_rec_v_geo = x_test_v[:,int(dT/2),:,:]\n",
    "\n",
    "        x_test_u = torch.cat([chunk['preds_u'] for chunk in outputs]).numpy()\n",
    "        x_test_u = std_uv * x_test_u\n",
    "        self.x_rec_u = x_test_u[:,int(dT/2),:,:]\n",
    "\n",
    "        x_test_v = torch.cat([chunk['preds_v'] for chunk in outputs]).numpy()\n",
    "        x_test_v = std_uv * x_test_v\n",
    "        self.x_rec_v = x_test_v[:,int(dT/2),:,:]\n",
    "\n",
    "        x_test_vv = torch.cat([chunk['preds_vv'] for chunk in outputs]).numpy()\n",
    "        x_test_vv = std_vv * x_test_vv\n",
    "        self.x_rec_vv = x_test_vv[:,int(dT/2),:,:]\n",
    "\n",
    "        x_test_vv = torch.cat([chunk['preds_vv_swot'] for chunk in outputs]).numpy()\n",
    "        x_test_vv = std_vv * x_test_vv\n",
    "        self.x_rec_vv_swot = x_test_vv[:,int(dT/2),:,:]\n",
    "\n",
    "        x_test_ssh_obs = torch.cat([chunk['obs_ssh'] for chunk in outputs]).numpy()\n",
    "        x_test_ssh_obs[ x_test_ssh_obs == 0. ] = np.float('NaN')\n",
    "        x_test_ssh_obs = stdTr * x_test_ssh_obs + meanTr\n",
    "        self.x_rec_ssh_obs = x_test_ssh_obs[:,int(dT/2),:,:]\n",
    "\n",
    "        x_test_sst_feat = torch.cat([chunk['feat_sst'] for chunk in outputs]).numpy()\n",
    "        self.x_feat_sst = x_test_sst_feat\n",
    "\n",
    "        return 1.\n",
    "\n",
    "    def compute_loss(self, batch, phase):\n",
    "\n",
    "        ssh_OI, imputs_obs, inputs_Mask, inputs_SST, ssh_GT, u_GT, v_GT, vv_GT = batch\n",
    "\n",
    "        new_masks      = torch.cat((1. + 0. * inputs_Mask, inputs_Mask, 0. * inputs_Mask , 0. * inputs_Mask ), dim=1)\n",
    "        inputs_init    = torch.cat((ssh_OI, inputs_Mask * (ssh_GT - ssh_OI) , 0. * ssh_GT , 0. * ssh_GT ), dim=1)\n",
    "        #inputs_missing = torch.cat((ssh_OI, inputs_Mask * (ssh_GT - ssh_OI) , 0. * ssh_GT , 0. * ssh_GT), dim=1)\n",
    "        inputs_missing = torch.cat((ssh_OI, inputs_Mask * (imputs_obs - ssh_OI) , 0. * ssh_GT , 0. * ssh_GT), dim=1)\n",
    "        \n",
    "        mask_SST       = 1. + 0. * inputs_SST\n",
    "\n",
    "        # gradient norm field\n",
    "        g_targets_GT = self.gradient_img(ssh_GT)\n",
    "        g_targets_GT = g_targets_GT[0]\n",
    "        \n",
    "       # need to evaluate grad/backward during the evaluation and training phase for phi_r\n",
    "        with torch.set_grad_enabled(True):\n",
    "            # with torch.set_grad_enabled(phase == 'train'):\n",
    "            inputs_init = torch.autograd.Variable(inputs_init, requires_grad=True)\n",
    "\n",
    "            outputs, hidden_new, cell_new, normgrad = self.model(inputs_init, [inputs_missing,inputs_SST], [new_masks,mask_SST])\n",
    "\n",
    "            if (phase == 'val') or (phase == 'test'):\n",
    "                outputs = outputs.detach()\n",
    "\n",
    "            outputsSLRHR = outputs\n",
    "            outputs_ssh_lr = outputs[:, 0:dT, :, :]\n",
    "            outputs_ssh = outputs[:, 0:dT, :, :] + outputs[:, dT:2*dT, :, :]\n",
    "            outputs_uv1 = outputs[:, 2*dT:3*dT, :, :]  \n",
    "            outputs_uv2 = outputs[:, 3*dT:4*dT, :, :]  \n",
    "\n",
    "            # reconstruction losses for SSH\n",
    "            g_outputs = self.gradient_img(outputs_ssh)\n",
    "            g_outputs = g_outputs[0]\n",
    "            loss_All   = NN_4DVar.compute_WeightedLoss((outputs_ssh - ssh_GT), self.w_loss)\n",
    "            loss_GAll  = NN_4DVar.compute_WeightedLoss(g_outputs - g_targets_GT, self.w_loss)\n",
    "\n",
    "            loss_OI    = NN_4DVar.compute_WeightedLoss(ssh_GT - ssh_OI, self.w_loss)\n",
    "            \n",
    "            loss_GOI   = NN_4DVar.compute_WeightedLoss(self.gradient_img(ssh_OI)[0] - g_targets_GT, self.w_loss)\n",
    "\n",
    "            # reconstruction losses for current \n",
    "            if self.hparams.flag_uv_param =='div-curl':            \n",
    "                curr_curl = self.model_pot2curr(outputs_ssh)\n",
    "                curr_dcurl = self.model_pot2curr(outputs_uv1)\n",
    "                curr_div = self.model_pot2curr(outputs_uv2)\n",
    "\n",
    "                outputs_u = curr_curl[1] + curr_dcurl[1] + curr_div[0]\n",
    "                outputs_v = -1. * curr_curl[0] - 1. * curr_dcurl[0] + curr_div[1]\n",
    "    \n",
    "                outputs_u_geo = curr_curl[1] + curr_dcurl[1]\n",
    "                outputs_v_geo = -1. * curr_curl[0] - 1. * curr_dcurl[0] \n",
    "\n",
    "                loss_uv = NN_4DVar.compute_WeightedLoss((outputs_u - u_GT[:,:,1:-1,1:-1]), self.w_loss)\n",
    "                loss_uv += NN_4DVar.compute_WeightedLoss((outputs_v - v_GT[:,:,1:-1,1:-1]), self.w_loss)\n",
    "                \n",
    "                loss_uv += NN_4DVar.compute_WeightedLoss((outputs_u_geo - u_GT[:,:,1:-1,1:-1]), self.w_loss)\n",
    "                loss_uv += NN_4DVar.compute_WeightedLoss((outputs_v_geo - v_GT[:,:,1:-1,1:-1]), self.w_loss)\n",
    "            else:\n",
    "                outputs_u = outputs_uv1\n",
    "                outputs_v = outputs_uv2\n",
    "\n",
    "                curr_ssh = self.model_pot2curr(outputs_ssh)\n",
    "                               \n",
    "                outputs_u_geo = curr_ssh[1]\n",
    "                outputs_v_geo = -1. * curr_ssh[0]              \n",
    "                \n",
    "                #loss_uv = NN_4DVar.compute_WeightedLoss((outputs_u - u_GT), self.w_loss)\n",
    "                #loss_uv += NN_4DVar.compute_WeightedLoss((outputs_v - v_GT), self.w_loss)\n",
    "                \n",
    "                #loss_uv += NN_4DVar.compute_WeightedLoss((outputs_u_geo - u_GT[:,:,1:-1,1:-1]), self.w_loss)\n",
    "                #loss_uv += NN_4DVar.compute_WeightedLoss((outputs_v_geo - v_GT[:,:,1:-1,1:-1]), self.w_loss)\n",
    "                loss_uv = torch.sqrt( 1e-10 + (outputs_u - u_GT)**2 + (outputs_v - v_GT) **2 )\n",
    "                loss_uv = compute_WeightedLoss_Lpqr(loss_uv, self.w_loss,self.hparams.p_norm_loss,self.hparams.q_norm_loss,self.hparams.r_norm_loss)\n",
    "                \n",
    "                loss_uv2 = torch.sqrt( 1e-10 + (outputs_u_geo - u_GT[:,:,1:-1,1:-1])**2 + (outputs_v_geo - v_GT[:,:,1:-1,1:-1]) **2 )\n",
    "                loss_uv += compute_WeightedLoss_Lpqr(loss_uv2, self.w_loss,self.hparams.p_norm_loss,self.hparams.q_norm_loss,self.hparams.r_norm_loss)\n",
    "\n",
    "                #loss_uv += NN_4DVar.compute_WeightedLoss((outputs_u_geo - u_GT[:,:,1:-1,1:-1]), self.w_loss)\n",
    "                #loss_uv += NN_4DVar.compute_WeightedLoss((outputs_v_geo - v_GT[:,:,1:-1,1:-1]), self.w_loss)\n",
    "                #div_rec = self.compute_div(outputs_u,outputs_v)\n",
    "                #div_gt = self.compute_div(u_GT,v_GT)\n",
    "                \n",
    "                div_rec = model_div(outputs_u,outputs_v)\n",
    "                div_gt = model_div(u_GT,v_GT)\n",
    "\n",
    "                outputs_u = outputs_u[:,:,1:-1,1:-1]\n",
    "                outputs_v = outputs_v[:,:,1:-1,1:-1]\n",
    "\n",
    "                loss_div = NN_4DVar.compute_WeightedLoss((div_rec - div_gt), self.w_loss)\n",
    "                #loss_uv += 10. * loss_div\n",
    "                \n",
    "                if self.current_epoch == 1:\n",
    "                    self.curr = 1\n",
    "                    \n",
    "                if self.curr == self.current_epoch :\n",
    "                    #print( np.mean( (div_gt.detach().cpu().numpy())**2 ), flush=True )\n",
    "                    #print( np.mean( (div_rec.detach().cpu().numpy())**2 ), flush=True )\n",
    "\n",
    "                    self.curr += 1\n",
    "\n",
    "            # projection losses\n",
    "            loss_AE     = torch.mean((self.model.phi_r(outputsSLRHR) - outputsSLRHR) ** 2)\n",
    "            yGT         = torch.cat((ssh_GT,ssh_GT-ssh_OI,outputs_uv1,outputs_uv2),dim=1)\n",
    "            loss_AE_GT = torch.mean((self.model.phi_r(yGT) - yGT) ** 2)\n",
    "\n",
    "            # low-resolution loss\n",
    "            loss_SR      = NN_4DVar.compute_WeightedLoss(outputs_ssh_lr - ssh_OI, self.w_loss)\n",
    "            targets_GTLR = self.model_LR(ssh_OI)\n",
    "            loss_LR      = NN_4DVar.compute_WeightedLoss(self.model_LR(outputs_ssh) - targets_GTLR, self.w_loss)\n",
    "\n",
    "            # total loss\n",
    "            loss = self.hparams.alpha_mse_ssh * loss_All + self.hparams.alpha_mse_gssh * loss_GAll + self.hparams.alpha_mse_uv * loss_uv\n",
    "            loss += 0.5 * self.hparams.alpha_proj * (loss_AE + loss_AE_GT)\n",
    "            loss    += self.hparams.alpha_lr * loss_LR + self.hparams.alpha_sr * loss_SR\n",
    "            \n",
    "            # metrics\n",
    "            mean_gall = NN_4DVar.compute_WeightedLoss(g_targets_GT,self.w_loss)\n",
    "            mse = loss_All.detach()\n",
    "            mse_grad   = loss_GAll.detach()\n",
    "            mse_uv = loss_uv\n",
    "            metrics   = dict([('mse',mse),('mse_grad',mse_grad),('mse_uv',mse_uv),('meanGrad',mean_gall),('mseOI',loss_OI.detach()),('mseGOI',loss_GOI.detach())])\n",
    "            #print(mse.cpu().detach().numpy())\n",
    "            \n",
    "            outputs = [outputs_ssh,outputs_u_geo,outputs_v_geo,outputs_u,outputs_v,inputs_missing[:,dT:2*dT,:,:]]\n",
    "        return loss,outputs, metrics\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "4dvarnet",
   "language": "python",
   "name": "4dvarnet"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
