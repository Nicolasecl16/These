{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-18 12:17:23.237944: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "tfk = tf.keras\n",
    "tfkl = tf.keras.layers\n",
    "tfpl = tfp.layers\n",
    "tfd = tfp.distributions\n",
    "import six"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = 2*tfd.Uniform().sample(1000)\n",
    "X = tfd.InverseGamma(concentration =1.5, scale = 0.6 ).sample(1000)\n",
    "R1 = A*X\n",
    "\n",
    "train_dataset = R1[:250]\n",
    "train_dataset = tf.reshape(train_dataset,[250,1])\n",
    "eval_dataset = R1[250:]\n",
    "eval_dataset=tf.reshape(eval_dataset,[750,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetricWrapper(tfk.metrics.Mean):\n",
    "    def __init__(self, fn, name=None, dtype=None, **kwargs):\n",
    "        super(MetricWrapper, self).__init__(name=name)\n",
    "        self._fn = fn\n",
    "        self._fn_kwargs = kwargs\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        matches = self._fn(y_true, y_pred, **self._fn_kwargs)\n",
    "        return super(MetricWrapper, self).update_state(\n",
    "        matches, sample_weight=sample_weight)\n",
    "    def get_config(self):\n",
    "        config = {}\n",
    "        for k, v in six.iteritems(self._fn_kwargs):\n",
    "            config[k] = K.eval(v) if is_tensor_or_variable(v) else v\n",
    "        base_config = super(MetricWrapper, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ZerosConstraint(tfk.constraints.Constraint):\n",
    "    def __init__(self, ref_value):\n",
    "        self.ref_value = ref_value\n",
    "\n",
    "    def __call__(self, w):\n",
    "        return 0*w\n",
    "\n",
    "    def get_config(self):\n",
    "        return {'ref_value': self.ref_value}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_mvn_prior(ndim, trainable=False):\n",
    "        if trainable:\n",
    "            loc = tf.Variable(tf.random.normal([ndim], stddev=0.1, dtype=tf.float32), name='prior_loc')\n",
    "            scale = tfp.util.TransformedVariable(\n",
    "            tf.random.normal([ndim], mean=1.0, stddev=0.1, dtype=tf.float32),\n",
    "            bijector=tfb.Chain([tfb.Shift(1e-5), tfb.Softplus(), tfb.Shift(0.5413)]), name='prior_scale')\n",
    "        else:\n",
    "            loc = tf.zeros(ndim)\n",
    "            scale = 1\n",
    "        prior = tfd.Independent(tfd.Normal(loc=loc, scale=scale), reinterpreted_batch_ndims=1)\n",
    "    return prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "class U_Encoder(tfk.Model):    \n",
    "    def __init__(self):      \n",
    "        super(U_Encoder,self).__init__()\n",
    "        self.alpha_layer  = tfkl.Dense(1, bias_initializer=tfk.initializers.Constant(2))\n",
    "        self.dense1       = tfkl.Dense(5, activation ='relu',kernel_initializer =tfk.initializers.RandomUniform(minval=0.01, maxval=0.02))\n",
    "        self.dense2       = tfkl.Dense(5, activation ='relu',kernel_initializer =tfk.initializers.RandomUniform(minval=0.01, maxval=0.02))\n",
    "        self.dense3       = tfkl.Dense(2, activation='relu',bias_initializer = tfk.initializers.RandomUniform(minval=1, maxval=2))\n",
    "        self.lambda1      = tfkl.Lambda(lambda x: tf.abs(x)+0.001)\n",
    "        self.dist_lambda1 = tfpl.DistributionLambda(\n",
    "                            make_distribution_fn=lambda t: tfd.InverseGamma(\n",
    "                                concentration=t[...,0], scale=t[...,1]))  \n",
    "        self.dist_lambda2 = tfpl.DistributionLambda(\n",
    "                            make_distribution_fn=lambda t: tfd.Gamma(\n",
    "                                concentration=t[...,0], rate=1))  \n",
    "        self.dist_lambda3 = tfpl.DistributionLambda(\n",
    "                            make_distribution_fn=lambda t: tfd.Gamma(\n",
    "                                concentration=t[...,0], rate=t[...,1]))  \n",
    "        self.concat1 = tfkl.Concatenate()\n",
    "        #self.KL_Loss      = tfpl.KLDivergenceAddLoss()\n",
    "        \n",
    "## passer prior en gamma et voir ensuite\n",
    "    def precall(self,inputs):\n",
    "        x      = self.dense1(inputs)\n",
    "        x      = self.dense2(x)\n",
    "        x      = self.dense3(x)\n",
    "        param  = self.lambda1(x)\n",
    "        return param        \n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x = self.precall(inputs)\n",
    "        alpha = self.alpha_layer(0*inputs)\n",
    "        prior = tfd.InverseGamma(concentration=alpha, scale=1)\n",
    "        return tfpl.DistributionLambda(\n",
    "                            make_distribution_fn=lambda t: \n",
    "            tfd.InverseGamma(\n",
    "                                concentration=t[...,0], scale=t[...,1]),\n",
    "            activity_regularizer = tfpl.KLDivergenceRegularizer(prior))(x)\n",
    "    \n",
    "    def call2(self,inputs):\n",
    "        param = self.precall(inputs)\n",
    "        alpha  = self.alpha_layer(0*inputs)\n",
    "        #params = self.concat1([alpha,param])\n",
    "        return self.dist_lambda2(alpha)\n",
    "    \n",
    "    def call3(self,inputs):\n",
    "        param = self.precall(inputs)\n",
    "        return self.dist_lambda3(param)\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "class U_Decoder(tfk.Model):\n",
    "    def __init__(self):\n",
    "        super(U_Decoder,self).__init__()\n",
    "        \n",
    "        self.dense1  = tfkl.Dense(5, use_bias=True, activation='relu')\n",
    "        self.dense2  = tfkl.Dense(5, use_bias=True, activation='relu')\n",
    "        self.dense31 = tfkl.Dense(1, use_bias=True)\n",
    "        self.dense32 = tfkl.Dense(1, use_bias=True)\n",
    "        self.lambda1 = tfkl.Lambda(lambda x: tf.abs(x)+0.001)\n",
    "        self.concat1 = tfkl.Concatenate()\n",
    "        self.dist_lambda1 = tfpl.DistributionLambda(\n",
    "            make_distribution_fn=lambda t: tfd.Gamma(\n",
    "                concentration=t[...,0], rate=t[...,1]))\n",
    "        \n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x     = tfkl.Reshape(target_shape=[1])(inputs)\n",
    "        x     = self.dense1(x)\n",
    "        x     = self.dense2(x)\n",
    "        alpha = self.dense31(x)\n",
    "        alpha = self.lambda1(alpha)\n",
    "        beta  = self.dense32(x)\n",
    "        beta  = self.lambda1(beta/inputs**2)\n",
    "        x     = self.concat1([alpha,beta])\n",
    "        x     = self.dist_lambda1(x)\n",
    "        return x\n",
    "    \n",
    "class U_Ext_VAE(tfk.Model):\n",
    "    def __init__(self):      \n",
    "        super(U_Ext_VAE,self).__init__()\n",
    "        self.encoder = U_Encoder()\n",
    "        self.decoder = U_Decoder()\n",
    "        #self.Block   = tfpl.DistributionLambda(\n",
    "         #   make_distribution_fn=lambda d: tfd.Blockwise(\n",
    "        #        d))\n",
    "    \n",
    "    def call(self,inputs):\n",
    "        res = {\"output_1\" : self.decoder(self.encoder(inputs)), \"output_2\" : self.decoder(self.encoder(inputs))}\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = U_Ext_VAE()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfp.distributions._TensorCoercible(\"u__encoder_42_distribution_lambda_129_tensor_coercible\", batch_shape=[2], event_shape=[], dtype=float32)\n",
      "Model: \"u__encoder_42\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_347 (Dense)           multiple                  2         \n",
      "                                                                 \n",
      " dense_348 (Dense)           multiple                  10        \n",
      "                                                                 \n",
      " dense_349 (Dense)           multiple                  30        \n",
      "                                                                 \n",
      " dense_350 (Dense)           multiple                  12        \n",
      "                                                                 \n",
      " lambda_84 (Lambda)          multiple                  0         \n",
      "                                                                 \n",
      " distribution_lambda_125 (Di  multiple                 0 (unused)\n",
      " stributionLambda)                                               \n",
      "                                                                 \n",
      " distribution_lambda_126 (Di  multiple                 0 (unused)\n",
      " stributionLambda)                                               \n",
      "                                                                 \n",
      " distribution_lambda_127 (Di  multiple                 0 (unused)\n",
      " stributionLambda)                                               \n",
      "                                                                 \n",
      " concatenate_83 (Concatenate  multiple                 0 (unused)\n",
      " )                                                               \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 54\n",
      "Trainable params: 54\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "This model has not yet been built. Build the model first by calling `build()` or by calling the model on a batch of data.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [256]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#print(vae(eval_dataset[2:4,:]))\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(vae\u001b[38;5;241m.\u001b[39mencoder\u001b[38;5;241m.\u001b[39msummary())\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mvae\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msummary\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/.conda/envs/tf/lib/python3.9/site-packages/keras/engine/training.py:3214\u001b[0m, in \u001b[0;36mModel.summary\u001b[0;34m(self, line_length, positions, print_fn, expand_nested, show_trainable, layer_range)\u001b[0m\n\u001b[1;32m   3184\u001b[0m \u001b[38;5;124;03m\"\"\"Prints a string summary of the network.\u001b[39;00m\n\u001b[1;32m   3185\u001b[0m \n\u001b[1;32m   3186\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3211\u001b[0m \u001b[38;5;124;03m    ValueError: if `summary()` is called before the model is built.\u001b[39;00m\n\u001b[1;32m   3212\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   3213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuilt:\n\u001b[0;32m-> 3214\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   3215\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis model has not yet been built. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3216\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBuild the model first by calling `build()` or by calling \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3217\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe model on a batch of data.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3218\u001b[0m     )\n\u001b[1;32m   3219\u001b[0m layer_utils\u001b[38;5;241m.\u001b[39mprint_summary(\n\u001b[1;32m   3220\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   3221\u001b[0m     line_length\u001b[38;5;241m=\u001b[39mline_length,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3226\u001b[0m     layer_range\u001b[38;5;241m=\u001b[39mlayer_range,\n\u001b[1;32m   3227\u001b[0m )\n",
      "\u001b[0;31mValueError\u001b[0m: This model has not yet been built. Build the model first by calling `build()` or by calling the model on a batch of data."
     ]
    }
   ],
   "source": [
    "print(vae.encoder(eval_dataset[2:4,:]))\n",
    "#print(vae(eval_dataset[2:4,:]))\n",
    "print(vae.encoder.summary())\n",
    "print(vae.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_log_likelihood = lambda x, rv_x: -rv_x.log_prob(x)\n",
    "def VAE_cost(x,rv_x):\n",
    "    print('VAE1')\n",
    "    print(rv_x)\n",
    "    print(type(rv_x))\n",
    "    #for i in rv_x:\n",
    "        #print(i)\n",
    "    return(-rv_x.log_prob(x))\n",
    "\n",
    "def VAE_cost2(x,rv_x):\n",
    "    print('VAE2')\n",
    "    print(rv_x)\n",
    "    print(type(rv_x))\n",
    "    #for i in rv_x:\n",
    "        #print(i)\n",
    "    return(-rv_x.log_prob(x))\n",
    "\n",
    "\n",
    "def kl_reg(x,y):\n",
    "    return(tfd.kl_divergence(tfd.Gamma(concentration = y[0], rate = 1),tfd.Gamma(concentration = y[1], rate = y[2])))\n",
    "losses = {\n",
    "\"output_1\": VAE_cost,\n",
    "\"output2\": VAE_cost,\n",
    "}\n",
    "#lossWeights = {\"category_output\": 1.0, \"color_output\": 1.0}\n",
    "\n",
    "#VAE_cost = lambda x,rv_x: -rv_x['dist1'].log_prob(x) #+ tfd.kl_divergence(rv_x[1],rv_x[2])##\n",
    "vae.compile(optimizer=tf.optimizers.Adam(learning_rate=1e-3),\n",
    "            loss=losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfp.distributions._TensorCoercible(\"u__encoder_36_distribution_lambda_96_tensor_coercible\", batch_shape=[5], event_shape=[], dtype=float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-09 19:36:55.467571: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at strided_slice_op.cc:105 : INVALID_ARGUMENT: Index out of range using input dim 0; input has only 0 dims\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "{{function_node __wrapped__StridedSlice_device_/job:localhost/replica:0/task:0/device:CPU:0}} Index out of range using input dim 0; input has only 0 dims [Op:StridedSlice] name: strided_slice/",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Input \u001b[0;32mIn [227]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mkl_reg\u001b[39m(x,y):\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m(tfd\u001b[38;5;241m.\u001b[39mkl_divergence(tfd\u001b[38;5;241m.\u001b[39mGamma(concentration \u001b[38;5;241m=\u001b[39m y[:,\u001b[38;5;241m0\u001b[39m], rate \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m),tfd\u001b[38;5;241m.\u001b[39mGamma(concentration \u001b[38;5;241m=\u001b[39m y[:,\u001b[38;5;241m1\u001b[39m], rate \u001b[38;5;241m=\u001b[39m y[:,\u001b[38;5;241m2\u001b[39m])))\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mkl_reg\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mvae\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m)\n",
      "Input \u001b[0;32mIn [227]\u001b[0m, in \u001b[0;36mkl_reg\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mkl_reg\u001b[39m(x,y):\n\u001b[0;32m----> 4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m(tfd\u001b[38;5;241m.\u001b[39mkl_divergence(tfd\u001b[38;5;241m.\u001b[39mGamma(concentration \u001b[38;5;241m=\u001b[39m \u001b[43my\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m, rate \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m),tfd\u001b[38;5;241m.\u001b[39mGamma(concentration \u001b[38;5;241m=\u001b[39m y[:,\u001b[38;5;241m1\u001b[39m], rate \u001b[38;5;241m=\u001b[39m y[:,\u001b[38;5;241m2\u001b[39m])))\n",
      "File \u001b[0;32m~/.conda/envs/tf/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/.conda/envs/tf/lib/python3.9/site-packages/tensorflow/python/framework/ops.py:7209\u001b[0m, in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   7207\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mraise_from_not_ok_status\u001b[39m(e, name):\n\u001b[1;32m   7208\u001b[0m   e\u001b[38;5;241m.\u001b[39mmessage \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m name: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m name \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 7209\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: {{function_node __wrapped__StridedSlice_device_/job:localhost/replica:0/task:0/device:CPU:0}} Index out of range using input dim 0; input has only 0 dims [Op:StridedSlice] name: strided_slice/"
     ]
    }
   ],
   "source": [
    "#print(train_dataset[:5])\n",
    "print(vae.encoder(train_dataset[:5]))\n",
    "def kl_reg(x,y):\n",
    "    return(tfd.kl_divergence(tfd.Gamma(concentration = y[:,0], rate = 1),tfd.Gamma(concentration = y[:,1], rate = y[:,2])))\n",
    "print(kl_reg(train_dataset[:5],vae.encoder(train_dataset[:5])[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "VAE1\n",
      "tfp.distributions._TensorCoercible(\"u__ext_vae_42_u__decoder_42_distribution_lambda_128_tensor_coercible\", batch_shape=[?], event_shape=[], dtype=float32)\n",
      "<class 'tensorflow_probability.python.layers.internal.distribution_tensor_coercible._TensorCoercible'>\n",
      "VAE1\n",
      "tfp.distributions._TensorCoercible(\"u__ext_vae_42_u__decoder_42_distribution_lambda_128_tensor_coercible_1\", batch_shape=[?], event_shape=[], dtype=float32)\n",
      "<class 'tensorflow_probability.python.layers.internal.distribution_tensor_coercible._TensorCoercible'>\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['u__encoder_42/dense_347/kernel:0', 'u__encoder_42/dense_347/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "VAE1\n",
      "tfp.distributions._TensorCoercible(\"u__ext_vae_42_u__decoder_42_distribution_lambda_128_tensor_coercible\", batch_shape=[?], event_shape=[], dtype=float32)\n",
      "<class 'tensorflow_probability.python.layers.internal.distribution_tensor_coercible._TensorCoercible'>\n",
      "VAE1\n",
      "tfp.distributions._TensorCoercible(\"u__ext_vae_42_u__decoder_42_distribution_lambda_128_tensor_coercible_1\", batch_shape=[?], event_shape=[], dtype=float32)\n",
      "<class 'tensorflow_probability.python.layers.internal.distribution_tensor_coercible._TensorCoercible'>\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['u__encoder_42/dense_347/kernel:0', 'u__encoder_42/dense_347/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "8/8 [==============================] - 2s 6ms/step - loss: 7.3051 - output_1_loss: 3.6345 - output_2_loss: 3.6706\n",
      "Epoch 2/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 6.0626 - output_1_loss: 2.9533 - output_2_loss: 3.1092\n",
      "Epoch 3/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 5.4893 - output_1_loss: 2.7848 - output_2_loss: 2.7045\n",
      "Epoch 4/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 5.3437 - output_1_loss: 2.6216 - output_2_loss: 2.7221\n",
      "Epoch 5/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 5.5715 - output_1_loss: 2.7634 - output_2_loss: 2.8081\n",
      "Epoch 6/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 4.8285 - output_1_loss: 2.3472 - output_2_loss: 2.4814\n",
      "Epoch 7/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 4.7565 - output_1_loss: 2.2967 - output_2_loss: 2.4598\n",
      "Epoch 8/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 4.7932 - output_1_loss: 2.3376 - output_2_loss: 2.4556\n",
      "Epoch 9/500\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 4.4754 - output_1_loss: 2.2640 - output_2_loss: 2.2115\n",
      "Epoch 10/500\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 4.6666 - output_1_loss: 2.1708 - output_2_loss: 2.4958\n",
      "Epoch 11/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 4.4443 - output_1_loss: 2.2156 - output_2_loss: 2.2287\n",
      "Epoch 12/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 4.1983 - output_1_loss: 2.1514 - output_2_loss: 2.0469\n",
      "Epoch 13/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 4.3445 - output_1_loss: 2.0874 - output_2_loss: 2.2571\n",
      "Epoch 14/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 4.2956 - output_1_loss: 2.0733 - output_2_loss: 2.2223\n",
      "Epoch 15/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 4.4020 - output_1_loss: 2.2643 - output_2_loss: 2.1377\n",
      "Epoch 16/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 4.0398 - output_1_loss: 2.0378 - output_2_loss: 2.0020\n",
      "Epoch 17/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.9170 - output_1_loss: 1.9592 - output_2_loss: 1.9578\n",
      "Epoch 18/500\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 3.9879 - output_1_loss: 2.0878 - output_2_loss: 1.9001\n",
      "Epoch 19/500\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 4.0836 - output_1_loss: 2.0057 - output_2_loss: 2.0780\n",
      "Epoch 20/500\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 4.1454 - output_1_loss: 2.0074 - output_2_loss: 2.1380\n",
      "Epoch 21/500\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 4.5354 - output_1_loss: 2.2142 - output_2_loss: 2.3212\n",
      "Epoch 22/500\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 3.8042 - output_1_loss: 1.9745 - output_2_loss: 1.8297\n",
      "Epoch 23/500\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 3.9608 - output_1_loss: 1.9034 - output_2_loss: 2.0575\n",
      "Epoch 24/500\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 3.8640 - output_1_loss: 1.9986 - output_2_loss: 1.8654\n",
      "Epoch 25/500\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 3.8629 - output_1_loss: 1.9066 - output_2_loss: 1.9563\n",
      "Epoch 26/500\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 3.7949 - output_1_loss: 1.9708 - output_2_loss: 1.8242\n",
      "Epoch 27/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 3.8958 - output_1_loss: 2.0377 - output_2_loss: 1.8581\n",
      "Epoch 28/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 4.2922 - output_1_loss: 2.3582 - output_2_loss: 1.9339\n",
      "Epoch 29/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 4.0994 - output_1_loss: 1.9772 - output_2_loss: 2.1222\n",
      "Epoch 30/500\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 3.8873 - output_1_loss: 1.9558 - output_2_loss: 1.9314\n",
      "Epoch 31/500\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 3.6176 - output_1_loss: 1.8367 - output_2_loss: 1.7809\n",
      "Epoch 32/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.6642 - output_1_loss: 1.8920 - output_2_loss: 1.7722\n",
      "Epoch 33/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 4.2729 - output_1_loss: 1.9623 - output_2_loss: 2.3105\n",
      "Epoch 34/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 4.1631 - output_1_loss: 2.4182 - output_2_loss: 1.7449\n",
      "Epoch 35/500\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 3.7318 - output_1_loss: 1.8478 - output_2_loss: 1.8841\n",
      "Epoch 36/500\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 3.6583 - output_1_loss: 1.7481 - output_2_loss: 1.9103\n",
      "Epoch 37/500\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 4.1117 - output_1_loss: 2.0104 - output_2_loss: 2.1013\n",
      "Epoch 38/500\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 3.9348 - output_1_loss: 1.8319 - output_2_loss: 2.1029\n",
      "Epoch 39/500\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 4.1394 - output_1_loss: 1.9001 - output_2_loss: 2.2393\n",
      "Epoch 40/500\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 3.7873 - output_1_loss: 1.8038 - output_2_loss: 1.9835\n",
      "Epoch 41/500\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 3.7872 - output_1_loss: 1.9202 - output_2_loss: 1.8671\n",
      "Epoch 42/500\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 4.1294 - output_1_loss: 2.2151 - output_2_loss: 1.9143\n",
      "Epoch 43/500\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 3.7883 - output_1_loss: 1.9078 - output_2_loss: 1.8805\n",
      "Epoch 44/500\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 3.9757 - output_1_loss: 2.0065 - output_2_loss: 1.9692\n",
      "Epoch 45/500\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 3.5844 - output_1_loss: 1.7576 - output_2_loss: 1.8268\n",
      "Epoch 46/500\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 3.8639 - output_1_loss: 1.8673 - output_2_loss: 1.9966\n",
      "Epoch 47/500\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 3.8132 - output_1_loss: 1.8902 - output_2_loss: 1.9230\n",
      "Epoch 48/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 4.1896 - output_1_loss: 2.3839 - output_2_loss: 1.8057\n",
      "Epoch 49/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 3.6565 - output_1_loss: 1.9004 - output_2_loss: 1.7562\n",
      "Epoch 50/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.6969 - output_1_loss: 1.8088 - output_2_loss: 1.8882\n",
      "Epoch 51/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.5804 - output_1_loss: 1.8414 - output_2_loss: 1.7390\n",
      "Epoch 52/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.6539 - output_1_loss: 1.9655 - output_2_loss: 1.6884\n",
      "Epoch 53/500\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 3.5955 - output_1_loss: 1.7088 - output_2_loss: 1.8868\n",
      "Epoch 54/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.5760 - output_1_loss: 1.9064 - output_2_loss: 1.6697\n",
      "Epoch 55/500\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 3.3103 - output_1_loss: 1.6599 - output_2_loss: 1.6503\n",
      "Epoch 56/500\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 3.5933 - output_1_loss: 1.7795 - output_2_loss: 1.8138\n",
      "Epoch 57/500\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 3.5967 - output_1_loss: 1.9474 - output_2_loss: 1.6493\n",
      "Epoch 58/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 3.4704 - output_1_loss: 1.7566 - output_2_loss: 1.7139\n",
      "Epoch 59/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 4.9339 - output_1_loss: 1.7934 - output_2_loss: 3.1406\n",
      "Epoch 60/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.9222 - output_1_loss: 2.1457 - output_2_loss: 1.7765\n",
      "Epoch 61/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.6451 - output_1_loss: 1.9870 - output_2_loss: 1.6582\n",
      "Epoch 62/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.6926 - output_1_loss: 1.9862 - output_2_loss: 1.7064\n",
      "Epoch 63/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.5481 - output_1_loss: 1.7252 - output_2_loss: 1.8229\n",
      "Epoch 64/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.7337 - output_1_loss: 1.7381 - output_2_loss: 1.9956\n",
      "Epoch 65/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.5943 - output_1_loss: 1.8694 - output_2_loss: 1.7249\n",
      "Epoch 66/500\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 3.6702 - output_1_loss: 2.0073 - output_2_loss: 1.6628\n",
      "Epoch 67/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.6459 - output_1_loss: 1.9214 - output_2_loss: 1.7245\n",
      "Epoch 68/500\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 3.8031 - output_1_loss: 1.7754 - output_2_loss: 2.0278\n",
      "Epoch 69/500\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 3.8693 - output_1_loss: 1.9502 - output_2_loss: 1.9191\n",
      "Epoch 70/500\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 3.4097 - output_1_loss: 1.8114 - output_2_loss: 1.5983\n",
      "Epoch 71/500\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 3.5083 - output_1_loss: 1.7642 - output_2_loss: 1.7441\n",
      "Epoch 72/500\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 3.6790 - output_1_loss: 1.8431 - output_2_loss: 1.8360\n",
      "Epoch 73/500\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 3.7047 - output_1_loss: 1.9740 - output_2_loss: 1.7307\n",
      "Epoch 74/500\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 3.3879 - output_1_loss: 1.7317 - output_2_loss: 1.6562\n",
      "Epoch 75/500\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 3.3521 - output_1_loss: 1.7407 - output_2_loss: 1.6115\n",
      "Epoch 76/500\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 3.5476 - output_1_loss: 1.8329 - output_2_loss: 1.7147\n",
      "Epoch 77/500\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 3.5486 - output_1_loss: 1.7639 - output_2_loss: 1.7847\n",
      "Epoch 78/500\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 3.6049 - output_1_loss: 1.8050 - output_2_loss: 1.7999\n",
      "Epoch 79/500\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 3.5806 - output_1_loss: 1.6724 - output_2_loss: 1.9083\n",
      "Epoch 80/500\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 3.2540 - output_1_loss: 1.6112 - output_2_loss: 1.6428\n",
      "Epoch 81/500\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 3.4470 - output_1_loss: 1.7658 - output_2_loss: 1.6813\n",
      "Epoch 82/500\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 3.6071 - output_1_loss: 1.9734 - output_2_loss: 1.6337\n",
      "Epoch 83/500\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 3.7946 - output_1_loss: 2.0211 - output_2_loss: 1.7735\n",
      "Epoch 84/500\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 3.3320 - output_1_loss: 1.6690 - output_2_loss: 1.6630\n",
      "Epoch 85/500\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 3.3831 - output_1_loss: 1.7408 - output_2_loss: 1.6423\n",
      "Epoch 86/500\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 3.4300 - output_1_loss: 1.6082 - output_2_loss: 1.8218\n",
      "Epoch 87/500\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 3.6013 - output_1_loss: 1.7814 - output_2_loss: 1.8199\n",
      "Epoch 88/500\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 3.3365 - output_1_loss: 1.6989 - output_2_loss: 1.6376\n",
      "Epoch 89/500\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 3.5071 - output_1_loss: 1.9155 - output_2_loss: 1.5917\n",
      "Epoch 90/500\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 3.4707 - output_1_loss: 1.7401 - output_2_loss: 1.7306\n",
      "Epoch 91/500\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 3.3604 - output_1_loss: 1.6683 - output_2_loss: 1.6921\n",
      "Epoch 92/500\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 3.2644 - output_1_loss: 1.7520 - output_2_loss: 1.5125\n",
      "Epoch 93/500\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 3.7298 - output_1_loss: 1.8066 - output_2_loss: 1.9232\n",
      "Epoch 94/500\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 3.3354 - output_1_loss: 1.6199 - output_2_loss: 1.7155\n",
      "Epoch 95/500\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 3.2554 - output_1_loss: 1.6896 - output_2_loss: 1.5658\n",
      "Epoch 96/500\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 3.7743 - output_1_loss: 1.6521 - output_2_loss: 2.1222\n",
      "Epoch 97/500\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 3.3456 - output_1_loss: 1.7092 - output_2_loss: 1.6364\n",
      "Epoch 98/500\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 3.3408 - output_1_loss: 1.6281 - output_2_loss: 1.7127\n",
      "Epoch 99/500\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 3.5152 - output_1_loss: 1.6206 - output_2_loss: 1.8946\n",
      "Epoch 100/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.3893 - output_1_loss: 1.6507 - output_2_loss: 1.7386\n",
      "Epoch 101/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 3.2934 - output_1_loss: 1.6204 - output_2_loss: 1.6730\n",
      "Epoch 102/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 3.2073 - output_1_loss: 1.4927 - output_2_loss: 1.7146\n",
      "Epoch 103/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 3.2645 - output_1_loss: 1.6711 - output_2_loss: 1.5934\n",
      "Epoch 104/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.3758 - output_1_loss: 1.6388 - output_2_loss: 1.7370\n",
      "Epoch 105/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.4724 - output_1_loss: 1.6535 - output_2_loss: 1.8189\n",
      "Epoch 106/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.2597 - output_1_loss: 1.6264 - output_2_loss: 1.6333\n",
      "Epoch 107/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.8039 - output_1_loss: 2.0416 - output_2_loss: 1.7624\n",
      "Epoch 108/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.6350 - output_1_loss: 1.7856 - output_2_loss: 1.8494\n",
      "Epoch 109/500\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 3.6442 - output_1_loss: 1.9813 - output_2_loss: 1.6630\n",
      "Epoch 110/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 3.4482 - output_1_loss: 1.5432 - output_2_loss: 1.9049\n",
      "Epoch 111/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.5329 - output_1_loss: 1.8689 - output_2_loss: 1.6640\n",
      "Epoch 112/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.5671 - output_1_loss: 1.7874 - output_2_loss: 1.7796\n",
      "Epoch 113/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.5863 - output_1_loss: 1.6367 - output_2_loss: 1.9497\n",
      "Epoch 114/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.3312 - output_1_loss: 1.6247 - output_2_loss: 1.7065\n",
      "Epoch 115/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.5866 - output_1_loss: 1.7111 - output_2_loss: 1.8755\n",
      "Epoch 116/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.1693 - output_1_loss: 1.5940 - output_2_loss: 1.5753\n",
      "Epoch 117/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.6312 - output_1_loss: 1.6582 - output_2_loss: 1.9731\n",
      "Epoch 118/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.1081 - output_1_loss: 1.5857 - output_2_loss: 1.5223\n",
      "Epoch 119/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.4917 - output_1_loss: 1.7309 - output_2_loss: 1.7608\n",
      "Epoch 120/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.2510 - output_1_loss: 1.5947 - output_2_loss: 1.6562\n",
      "Epoch 121/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.4968 - output_1_loss: 1.8854 - output_2_loss: 1.6114\n",
      "Epoch 122/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.2767 - output_1_loss: 1.6213 - output_2_loss: 1.6555\n",
      "Epoch 123/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 4.1100 - output_1_loss: 2.5102 - output_2_loss: 1.5998\n",
      "Epoch 124/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.1332 - output_1_loss: 1.5361 - output_2_loss: 1.5971\n",
      "Epoch 125/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.3733 - output_1_loss: 1.6544 - output_2_loss: 1.7189\n",
      "Epoch 126/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.4613 - output_1_loss: 1.8564 - output_2_loss: 1.6048\n",
      "Epoch 127/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.1311 - output_1_loss: 1.4695 - output_2_loss: 1.6616\n",
      "Epoch 128/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.2645 - output_1_loss: 1.5620 - output_2_loss: 1.7025\n",
      "Epoch 129/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.3944 - output_1_loss: 1.7392 - output_2_loss: 1.6552\n",
      "Epoch 130/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.1083 - output_1_loss: 1.5286 - output_2_loss: 1.5798\n",
      "Epoch 131/500\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 3.7201 - output_1_loss: 1.8014 - output_2_loss: 1.9187\n",
      "Epoch 132/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 4.1160 - output_1_loss: 2.1277 - output_2_loss: 1.9883\n",
      "Epoch 133/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 3.1543 - output_1_loss: 1.7016 - output_2_loss: 1.4527\n",
      "Epoch 134/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 3.1912 - output_1_loss: 1.6387 - output_2_loss: 1.5525\n",
      "Epoch 135/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.3404 - output_1_loss: 1.5479 - output_2_loss: 1.7926\n",
      "Epoch 136/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 3.1180 - output_1_loss: 1.4764 - output_2_loss: 1.6417\n",
      "Epoch 137/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.2142 - output_1_loss: 1.6468 - output_2_loss: 1.5674\n",
      "Epoch 138/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.2803 - output_1_loss: 1.6845 - output_2_loss: 1.5957\n",
      "Epoch 139/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.4250 - output_1_loss: 1.8308 - output_2_loss: 1.5942\n",
      "Epoch 140/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.3195 - output_1_loss: 1.5513 - output_2_loss: 1.7682\n",
      "Epoch 141/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.0214 - output_1_loss: 1.4366 - output_2_loss: 1.5848\n",
      "Epoch 142/500\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 3.1468 - output_1_loss: 1.5412 - output_2_loss: 1.6056\n",
      "Epoch 143/500\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 3.0355 - output_1_loss: 1.4792 - output_2_loss: 1.5563\n",
      "Epoch 144/500\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 3.2426 - output_1_loss: 1.5702 - output_2_loss: 1.6724\n",
      "Epoch 145/500\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 3.1934 - output_1_loss: 1.5972 - output_2_loss: 1.5962\n",
      "Epoch 146/500\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 3.3574 - output_1_loss: 1.8088 - output_2_loss: 1.5485\n",
      "Epoch 147/500\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 3.2305 - output_1_loss: 1.5193 - output_2_loss: 1.7112\n",
      "Epoch 148/500\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 3.1782 - output_1_loss: 1.5397 - output_2_loss: 1.6386\n",
      "Epoch 149/500\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 3.2334 - output_1_loss: 1.6604 - output_2_loss: 1.5730\n",
      "Epoch 150/500\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 3.1506 - output_1_loss: 1.5590 - output_2_loss: 1.5917\n",
      "Epoch 151/500\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 3.3508 - output_1_loss: 1.6070 - output_2_loss: 1.7438\n",
      "Epoch 152/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 2.9585 - output_1_loss: 1.5525 - output_2_loss: 1.4060\n",
      "Epoch 153/500\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 3.2271 - output_1_loss: 1.6133 - output_2_loss: 1.6139\n",
      "Epoch 154/500\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 3.7560 - output_1_loss: 2.0375 - output_2_loss: 1.7184\n",
      "Epoch 155/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.2710 - output_1_loss: 1.6938 - output_2_loss: 1.5772\n",
      "Epoch 156/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 2.9826 - output_1_loss: 1.5146 - output_2_loss: 1.4679\n",
      "Epoch 157/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.4659 - output_1_loss: 1.6013 - output_2_loss: 1.8646\n",
      "Epoch 158/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.1376 - output_1_loss: 1.4969 - output_2_loss: 1.6406\n",
      "Epoch 159/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.5029 - output_1_loss: 1.7471 - output_2_loss: 1.7558\n",
      "Epoch 160/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.2787 - output_1_loss: 1.6498 - output_2_loss: 1.6289\n",
      "Epoch 161/500\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 3.1337 - output_1_loss: 1.5659 - output_2_loss: 1.5678\n",
      "Epoch 162/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.3952 - output_1_loss: 1.8290 - output_2_loss: 1.5663\n",
      "Epoch 163/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.6886 - output_1_loss: 2.0874 - output_2_loss: 1.6012\n",
      "Epoch 164/500\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 3.0409 - output_1_loss: 1.5115 - output_2_loss: 1.5294\n",
      "Epoch 165/500\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 2.9281 - output_1_loss: 1.4521 - output_2_loss: 1.4760\n",
      "Epoch 166/500\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 3.0314 - output_1_loss: 1.4559 - output_2_loss: 1.5754\n",
      "Epoch 167/500\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 2.9999 - output_1_loss: 1.5596 - output_2_loss: 1.4403\n",
      "Epoch 168/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.7201 - output_1_loss: 1.9554 - output_2_loss: 1.7647\n",
      "Epoch 169/500\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 3.4400 - output_1_loss: 1.7215 - output_2_loss: 1.7185\n",
      "Epoch 170/500\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 3.2307 - output_1_loss: 1.5908 - output_2_loss: 1.6399\n",
      "Epoch 171/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.7825 - output_1_loss: 1.7971 - output_2_loss: 1.9854\n",
      "Epoch 172/500\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 3.0375 - output_1_loss: 1.5491 - output_2_loss: 1.4884\n",
      "Epoch 173/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.5750 - output_1_loss: 1.9075 - output_2_loss: 1.6674\n",
      "Epoch 174/500\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 3.4547 - output_1_loss: 1.7080 - output_2_loss: 1.7467\n",
      "Epoch 175/500\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 4.0426 - output_1_loss: 2.3355 - output_2_loss: 1.7071\n",
      "Epoch 176/500\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 3.9287 - output_1_loss: 1.9330 - output_2_loss: 1.9957\n",
      "Epoch 177/500\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 3.2739 - output_1_loss: 1.7495 - output_2_loss: 1.5243\n",
      "Epoch 178/500\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 3.4338 - output_1_loss: 1.8514 - output_2_loss: 1.5824\n",
      "Epoch 179/500\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 3.2833 - output_1_loss: 1.6305 - output_2_loss: 1.6528\n",
      "Epoch 180/500\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 3.3228 - output_1_loss: 1.7854 - output_2_loss: 1.5374\n",
      "Epoch 181/500\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 3.2364 - output_1_loss: 1.6101 - output_2_loss: 1.6263\n",
      "Epoch 182/500\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 3.2621 - output_1_loss: 1.5520 - output_2_loss: 1.7101\n",
      "Epoch 183/500\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 3.1119 - output_1_loss: 1.5007 - output_2_loss: 1.6111\n",
      "Epoch 184/500\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 3.2358 - output_1_loss: 1.7204 - output_2_loss: 1.5154\n",
      "Epoch 185/500\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 3.1962 - output_1_loss: 1.4813 - output_2_loss: 1.7149\n",
      "Epoch 186/500\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 3.1996 - output_1_loss: 1.5374 - output_2_loss: 1.6622\n",
      "Epoch 187/500\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 3.2657 - output_1_loss: 1.4800 - output_2_loss: 1.7857\n",
      "Epoch 188/500\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 3.1745 - output_1_loss: 1.6757 - output_2_loss: 1.4988\n",
      "Epoch 189/500\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 3.8520 - output_1_loss: 1.5677 - output_2_loss: 2.2843\n",
      "Epoch 190/500\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 3.6798 - output_1_loss: 1.6251 - output_2_loss: 2.0548\n",
      "Epoch 191/500\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 3.0693 - output_1_loss: 1.4542 - output_2_loss: 1.6151\n",
      "Epoch 192/500\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 3.3884 - output_1_loss: 1.6907 - output_2_loss: 1.6977\n",
      "Epoch 193/500\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 3.1496 - output_1_loss: 1.5725 - output_2_loss: 1.5771\n",
      "Epoch 194/500\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 3.3492 - output_1_loss: 1.7913 - output_2_loss: 1.5579\n",
      "Epoch 195/500\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 3.3614 - output_1_loss: 1.6697 - output_2_loss: 1.6917\n",
      "Epoch 196/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.0145 - output_1_loss: 1.4479 - output_2_loss: 1.5666\n",
      "Epoch 197/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.3425 - output_1_loss: 1.6641 - output_2_loss: 1.6784\n",
      "Epoch 198/500\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 3.4780 - output_1_loss: 1.7842 - output_2_loss: 1.6937\n",
      "Epoch 199/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.8608 - output_1_loss: 2.0234 - output_2_loss: 1.8374\n",
      "Epoch 200/500\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 3.0614 - output_1_loss: 1.5930 - output_2_loss: 1.4684\n",
      "Epoch 201/500\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 3.4669 - output_1_loss: 1.7588 - output_2_loss: 1.7081\n",
      "Epoch 202/500\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 3.6168 - output_1_loss: 1.7783 - output_2_loss: 1.8385\n",
      "Epoch 203/500\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 3.1690 - output_1_loss: 1.5200 - output_2_loss: 1.6490\n",
      "Epoch 204/500\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 3.9997 - output_1_loss: 2.4756 - output_2_loss: 1.5241\n",
      "Epoch 205/500\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 3.0340 - output_1_loss: 1.4770 - output_2_loss: 1.5569\n",
      "Epoch 206/500\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 3.1849 - output_1_loss: 1.5834 - output_2_loss: 1.6015\n",
      "Epoch 207/500\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 3.3254 - output_1_loss: 1.5740 - output_2_loss: 1.7514\n",
      "Epoch 208/500\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 3.1485 - output_1_loss: 1.6026 - output_2_loss: 1.5460\n",
      "Epoch 209/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.6967 - output_1_loss: 2.0629 - output_2_loss: 1.6338\n",
      "Epoch 210/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.2715 - output_1_loss: 1.7037 - output_2_loss: 1.5678\n",
      "Epoch 211/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.2205 - output_1_loss: 1.7037 - output_2_loss: 1.5168\n",
      "Epoch 212/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 3.1791 - output_1_loss: 1.7034 - output_2_loss: 1.4757\n",
      "Epoch 213/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.2663 - output_1_loss: 1.6255 - output_2_loss: 1.6408\n",
      "Epoch 214/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 3.0158 - output_1_loss: 1.4882 - output_2_loss: 1.5276\n",
      "Epoch 215/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.6510 - output_1_loss: 1.5181 - output_2_loss: 2.1329\n",
      "Epoch 216/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 3.8228 - output_1_loss: 2.0099 - output_2_loss: 1.8128\n",
      "Epoch 217/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.3416 - output_1_loss: 1.7191 - output_2_loss: 1.6225\n",
      "Epoch 218/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.2495 - output_1_loss: 1.6694 - output_2_loss: 1.5802\n",
      "Epoch 219/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.3175 - output_1_loss: 1.6182 - output_2_loss: 1.6992\n",
      "Epoch 220/500\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 3.4577 - output_1_loss: 1.6469 - output_2_loss: 1.8108\n",
      "Epoch 221/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.2078 - output_1_loss: 1.5665 - output_2_loss: 1.6413\n",
      "Epoch 222/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.3657 - output_1_loss: 1.6838 - output_2_loss: 1.6820\n",
      "Epoch 223/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.2112 - output_1_loss: 1.5491 - output_2_loss: 1.6621\n",
      "Epoch 224/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 3.2846 - output_1_loss: 1.7124 - output_2_loss: 1.5722\n",
      "Epoch 225/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.3565 - output_1_loss: 1.7045 - output_2_loss: 1.6520\n",
      "Epoch 226/500\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 3.0062 - output_1_loss: 1.5154 - output_2_loss: 1.4908\n",
      "Epoch 227/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.1402 - output_1_loss: 1.6188 - output_2_loss: 1.5214\n",
      "Epoch 228/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.0726 - output_1_loss: 1.6864 - output_2_loss: 1.3861\n",
      "Epoch 229/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 2.9136 - output_1_loss: 1.4336 - output_2_loss: 1.4800\n",
      "Epoch 230/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.2891 - output_1_loss: 1.6122 - output_2_loss: 1.6769\n",
      "Epoch 231/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.1349 - output_1_loss: 1.5630 - output_2_loss: 1.5719\n",
      "Epoch 232/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.0393 - output_1_loss: 1.5923 - output_2_loss: 1.4470\n",
      "Epoch 233/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.2177 - output_1_loss: 1.5417 - output_2_loss: 1.6760\n",
      "Epoch 234/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.4402 - output_1_loss: 1.6682 - output_2_loss: 1.7720\n",
      "Epoch 235/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.1415 - output_1_loss: 1.5969 - output_2_loss: 1.5446\n",
      "Epoch 236/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.2487 - output_1_loss: 1.5020 - output_2_loss: 1.7468\n",
      "Epoch 237/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.0453 - output_1_loss: 1.3654 - output_2_loss: 1.6799\n",
      "Epoch 238/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.1505 - output_1_loss: 1.5522 - output_2_loss: 1.5984\n",
      "Epoch 239/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.2501 - output_1_loss: 1.6981 - output_2_loss: 1.5519\n",
      "Epoch 240/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.1229 - output_1_loss: 1.4556 - output_2_loss: 1.6673\n",
      "Epoch 241/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.4108 - output_1_loss: 1.4607 - output_2_loss: 1.9501\n",
      "Epoch 242/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.1244 - output_1_loss: 1.6948 - output_2_loss: 1.4296\n",
      "Epoch 243/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.1960 - output_1_loss: 1.4191 - output_2_loss: 1.7769\n",
      "Epoch 244/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.3756 - output_1_loss: 1.6035 - output_2_loss: 1.7721\n",
      "Epoch 245/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 2.9385 - output_1_loss: 1.4998 - output_2_loss: 1.4388\n",
      "Epoch 246/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.6566 - output_1_loss: 1.7438 - output_2_loss: 1.9129\n",
      "Epoch 247/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.0275 - output_1_loss: 1.5195 - output_2_loss: 1.5080\n",
      "Epoch 248/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.8175 - output_1_loss: 2.0518 - output_2_loss: 1.7657\n",
      "Epoch 249/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.0114 - output_1_loss: 1.5654 - output_2_loss: 1.4460\n",
      "Epoch 250/500\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 3.1372 - output_1_loss: 1.5571 - output_2_loss: 1.5802\n",
      "Epoch 251/500\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 3.0743 - output_1_loss: 1.4185 - output_2_loss: 1.6559\n",
      "Epoch 252/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 2.9514 - output_1_loss: 1.4129 - output_2_loss: 1.5385\n",
      "Epoch 253/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.1143 - output_1_loss: 1.4679 - output_2_loss: 1.6464\n",
      "Epoch 254/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.3749 - output_1_loss: 1.6368 - output_2_loss: 1.7381\n",
      "Epoch 255/500\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 3.4697 - output_1_loss: 1.7630 - output_2_loss: 1.7067\n",
      "Epoch 256/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.4758 - output_1_loss: 1.4472 - output_2_loss: 2.0285\n",
      "Epoch 257/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.3532 - output_1_loss: 1.7058 - output_2_loss: 1.6474\n",
      "Epoch 258/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.1222 - output_1_loss: 1.6120 - output_2_loss: 1.5103\n",
      "Epoch 259/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.1988 - output_1_loss: 1.5882 - output_2_loss: 1.6106\n",
      "Epoch 260/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.3468 - output_1_loss: 1.8240 - output_2_loss: 1.5228\n",
      "Epoch 261/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.5405 - output_1_loss: 1.9134 - output_2_loss: 1.6271\n",
      "Epoch 262/500\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 3.2271 - output_1_loss: 1.6651 - output_2_loss: 1.5620\n",
      "Epoch 263/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.3778 - output_1_loss: 1.6079 - output_2_loss: 1.7699\n",
      "Epoch 264/500\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 4.0790 - output_1_loss: 1.6615 - output_2_loss: 2.4175\n",
      "Epoch 265/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.0509 - output_1_loss: 1.5877 - output_2_loss: 1.4632\n",
      "Epoch 266/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.0759 - output_1_loss: 1.6075 - output_2_loss: 1.4684\n",
      "Epoch 267/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.0083 - output_1_loss: 1.5119 - output_2_loss: 1.4964\n",
      "Epoch 268/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.3110 - output_1_loss: 1.6243 - output_2_loss: 1.6867\n",
      "Epoch 269/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.0287 - output_1_loss: 1.4690 - output_2_loss: 1.5597\n",
      "Epoch 270/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.2527 - output_1_loss: 1.5938 - output_2_loss: 1.6589\n",
      "Epoch 271/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.1443 - output_1_loss: 1.7036 - output_2_loss: 1.4407\n",
      "Epoch 272/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.1669 - output_1_loss: 1.6691 - output_2_loss: 1.4979\n",
      "Epoch 273/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.2508 - output_1_loss: 1.6389 - output_2_loss: 1.6119\n",
      "Epoch 274/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.3906 - output_1_loss: 1.7064 - output_2_loss: 1.6842\n",
      "Epoch 275/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.3933 - output_1_loss: 1.5565 - output_2_loss: 1.8368\n",
      "Epoch 276/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.2195 - output_1_loss: 1.5789 - output_2_loss: 1.6406\n",
      "Epoch 277/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.3902 - output_1_loss: 1.6774 - output_2_loss: 1.7128\n",
      "Epoch 278/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.2691 - output_1_loss: 1.7058 - output_2_loss: 1.5633\n",
      "Epoch 279/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 2.9355 - output_1_loss: 1.4187 - output_2_loss: 1.5168\n",
      "Epoch 280/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.8529 - output_1_loss: 2.1696 - output_2_loss: 1.6832\n",
      "Epoch 281/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.3601 - output_1_loss: 1.8436 - output_2_loss: 1.5165\n",
      "Epoch 282/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.2704 - output_1_loss: 1.7002 - output_2_loss: 1.5701\n",
      "Epoch 283/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.0670 - output_1_loss: 1.5910 - output_2_loss: 1.4760\n",
      "Epoch 284/500\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 3.1056 - output_1_loss: 1.5508 - output_2_loss: 1.5548\n",
      "Epoch 285/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 2.9721 - output_1_loss: 1.5873 - output_2_loss: 1.3848\n",
      "Epoch 286/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.0120 - output_1_loss: 1.4797 - output_2_loss: 1.5323\n",
      "Epoch 287/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.2195 - output_1_loss: 1.5174 - output_2_loss: 1.7021\n",
      "Epoch 288/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.0032 - output_1_loss: 1.5931 - output_2_loss: 1.4101\n",
      "Epoch 289/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.3624 - output_1_loss: 1.5048 - output_2_loss: 1.8576\n",
      "Epoch 290/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.4057 - output_1_loss: 1.7446 - output_2_loss: 1.6611\n",
      "Epoch 291/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.5103 - output_1_loss: 1.6410 - output_2_loss: 1.8693\n",
      "Epoch 292/500\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 3.0119 - output_1_loss: 1.5003 - output_2_loss: 1.5116\n",
      "Epoch 293/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.2836 - output_1_loss: 1.7195 - output_2_loss: 1.5641\n",
      "Epoch 294/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.9182 - output_1_loss: 2.1192 - output_2_loss: 1.7990\n",
      "Epoch 295/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.3813 - output_1_loss: 1.6856 - output_2_loss: 1.6957\n",
      "Epoch 296/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.5176 - output_1_loss: 1.6869 - output_2_loss: 1.8307\n",
      "Epoch 297/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.5206 - output_1_loss: 1.6170 - output_2_loss: 1.9037\n",
      "Epoch 298/500\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 3.1098 - output_1_loss: 1.5774 - output_2_loss: 1.5324\n",
      "Epoch 299/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.1818 - output_1_loss: 1.6251 - output_2_loss: 1.5567\n",
      "Epoch 300/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 2.9404 - output_1_loss: 1.4683 - output_2_loss: 1.4721\n",
      "Epoch 301/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.4064 - output_1_loss: 1.7459 - output_2_loss: 1.6605\n",
      "Epoch 302/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.1298 - output_1_loss: 1.4554 - output_2_loss: 1.6743\n",
      "Epoch 303/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.1504 - output_1_loss: 1.5001 - output_2_loss: 1.6503\n",
      "Epoch 304/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.4883 - output_1_loss: 1.5389 - output_2_loss: 1.9494\n",
      "Epoch 305/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.2448 - output_1_loss: 1.5696 - output_2_loss: 1.6752\n",
      "Epoch 306/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.2531 - output_1_loss: 1.5439 - output_2_loss: 1.7092\n",
      "Epoch 307/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.1492 - output_1_loss: 1.5958 - output_2_loss: 1.5533\n",
      "Epoch 308/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.1627 - output_1_loss: 1.6196 - output_2_loss: 1.5431\n",
      "Epoch 309/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.2314 - output_1_loss: 1.6317 - output_2_loss: 1.5997\n",
      "Epoch 310/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.0968 - output_1_loss: 1.5326 - output_2_loss: 1.5642\n",
      "Epoch 311/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.3829 - output_1_loss: 1.6746 - output_2_loss: 1.7083\n",
      "Epoch 312/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.0692 - output_1_loss: 1.4139 - output_2_loss: 1.6553\n",
      "Epoch 313/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.2685 - output_1_loss: 1.5787 - output_2_loss: 1.6899\n",
      "Epoch 314/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.3060 - output_1_loss: 1.5514 - output_2_loss: 1.7546\n",
      "Epoch 315/500\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 2.8592 - output_1_loss: 1.3572 - output_2_loss: 1.5020\n",
      "Epoch 316/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.1907 - output_1_loss: 1.5626 - output_2_loss: 1.6281\n",
      "Epoch 317/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.4104 - output_1_loss: 1.6933 - output_2_loss: 1.7171\n",
      "Epoch 318/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.4257 - output_1_loss: 1.6323 - output_2_loss: 1.7935\n",
      "Epoch 319/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.6932 - output_1_loss: 1.7156 - output_2_loss: 1.9776\n",
      "Epoch 320/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 2.9318 - output_1_loss: 1.4475 - output_2_loss: 1.4844\n",
      "Epoch 321/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.0832 - output_1_loss: 1.5187 - output_2_loss: 1.5645\n",
      "Epoch 322/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.1857 - output_1_loss: 1.7324 - output_2_loss: 1.4532\n",
      "Epoch 323/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.2382 - output_1_loss: 1.5609 - output_2_loss: 1.6773\n",
      "Epoch 324/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.4905 - output_1_loss: 1.6765 - output_2_loss: 1.8140\n",
      "Epoch 325/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.3121 - output_1_loss: 1.7051 - output_2_loss: 1.6069\n",
      "Epoch 326/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.1907 - output_1_loss: 1.5866 - output_2_loss: 1.6040\n",
      "Epoch 327/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.1836 - output_1_loss: 1.6646 - output_2_loss: 1.5190\n",
      "Epoch 328/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.2396 - output_1_loss: 1.6663 - output_2_loss: 1.5734\n",
      "Epoch 329/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.0608 - output_1_loss: 1.5617 - output_2_loss: 1.4991\n",
      "Epoch 330/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.0781 - output_1_loss: 1.5600 - output_2_loss: 1.5181\n",
      "Epoch 331/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.0295 - output_1_loss: 1.4784 - output_2_loss: 1.5511\n",
      "Epoch 332/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.0763 - output_1_loss: 1.4715 - output_2_loss: 1.6048\n",
      "Epoch 333/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.5356 - output_1_loss: 2.0416 - output_2_loss: 1.4940\n",
      "Epoch 334/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.2781 - output_1_loss: 1.5828 - output_2_loss: 1.6953\n",
      "Epoch 335/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.0213 - output_1_loss: 1.6296 - output_2_loss: 1.3917\n",
      "Epoch 336/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 2.9045 - output_1_loss: 1.4669 - output_2_loss: 1.4377\n",
      "Epoch 337/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.1139 - output_1_loss: 1.6040 - output_2_loss: 1.5099\n",
      "Epoch 338/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.5615 - output_1_loss: 1.8235 - output_2_loss: 1.7380\n",
      "Epoch 339/500\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 3.1617 - output_1_loss: 1.5438 - output_2_loss: 1.6179\n",
      "Epoch 340/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.0526 - output_1_loss: 1.5748 - output_2_loss: 1.4778\n",
      "Epoch 341/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 2.9592 - output_1_loss: 1.5456 - output_2_loss: 1.4136\n",
      "Epoch 342/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.2931 - output_1_loss: 1.8057 - output_2_loss: 1.4873\n",
      "Epoch 343/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.6225 - output_1_loss: 1.8395 - output_2_loss: 1.7831\n",
      "Epoch 344/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.2411 - output_1_loss: 1.6025 - output_2_loss: 1.6386\n",
      "Epoch 345/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.4269 - output_1_loss: 1.6613 - output_2_loss: 1.7656\n",
      "Epoch 346/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.0658 - output_1_loss: 1.4767 - output_2_loss: 1.5891\n",
      "Epoch 347/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.1413 - output_1_loss: 1.5958 - output_2_loss: 1.5455\n",
      "Epoch 348/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.2147 - output_1_loss: 1.7216 - output_2_loss: 1.4932\n",
      "Epoch 349/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.6423 - output_1_loss: 1.7853 - output_2_loss: 1.8570\n",
      "Epoch 350/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.1037 - output_1_loss: 1.5593 - output_2_loss: 1.5444\n",
      "Epoch 351/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.0317 - output_1_loss: 1.5654 - output_2_loss: 1.4662\n",
      "Epoch 352/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 2.8941 - output_1_loss: 1.4707 - output_2_loss: 1.4234\n",
      "Epoch 353/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.3774 - output_1_loss: 1.5587 - output_2_loss: 1.8186\n",
      "Epoch 354/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.1808 - output_1_loss: 1.6616 - output_2_loss: 1.5192\n",
      "Epoch 355/500\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 3.1368 - output_1_loss: 1.5093 - output_2_loss: 1.6275\n",
      "Epoch 356/500\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 2.9468 - output_1_loss: 1.4798 - output_2_loss: 1.4671\n",
      "Epoch 357/500\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 3.1487 - output_1_loss: 1.6176 - output_2_loss: 1.5310\n",
      "Epoch 358/500\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 3.7600 - output_1_loss: 1.6161 - output_2_loss: 2.1439\n",
      "Epoch 359/500\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 3.3242 - output_1_loss: 1.6956 - output_2_loss: 1.6286\n",
      "Epoch 360/500\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 2.9432 - output_1_loss: 1.6297 - output_2_loss: 1.3135\n",
      "Epoch 361/500\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 3.4256 - output_1_loss: 1.8804 - output_2_loss: 1.5453\n",
      "Epoch 362/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 2.9191 - output_1_loss: 1.4368 - output_2_loss: 1.4823\n",
      "Epoch 363/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.3925 - output_1_loss: 1.7971 - output_2_loss: 1.5954\n",
      "Epoch 364/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.8609 - output_1_loss: 1.7676 - output_2_loss: 2.0933\n",
      "Epoch 365/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 2.8688 - output_1_loss: 1.3776 - output_2_loss: 1.4912\n",
      "Epoch 366/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 3.2142 - output_1_loss: 1.8163 - output_2_loss: 1.3979"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [258]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mvae\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m\t\u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moutput_1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moutput_2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m#validation_data=(eval_dataset,eval_dataset), \u001b[39;49;00m\n\u001b[1;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/tf/lib/python3.9/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.conda/envs/tf/lib/python3.9/site-packages/keras/engine/training.py:1564\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1556\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1557\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1558\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1561\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   1562\u001b[0m ):\n\u001b[1;32m   1563\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1564\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1565\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1566\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/.conda/envs/tf/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.conda/envs/tf/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    912\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    914\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 915\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    917\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    918\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/.conda/envs/tf/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    944\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    945\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    946\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 947\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stateless_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    948\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    949\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    950\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    951\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/.conda/envs/tf/lib/python3.9/site-packages/tensorflow/python/eager/function.py:2496\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2493\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m   2494\u001b[0m   (graph_function,\n\u001b[1;32m   2495\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2496\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2497\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/tf/lib/python3.9/site-packages/tensorflow/python/eager/function.py:1862\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1858\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1859\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1860\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1861\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1862\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1863\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1864\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1865\u001b[0m     args,\n\u001b[1;32m   1866\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1867\u001b[0m     executing_eagerly)\n\u001b[1;32m   1868\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/.conda/envs/tf/lib/python3.9/site-packages/tensorflow/python/eager/function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    497\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    498\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 499\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    505\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    506\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    507\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[1;32m    508\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    511\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[1;32m    512\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m~/.conda/envs/tf/lib/python3.9/site-packages/tensorflow/python/eager/execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "vae.fit(x=train_dataset,\n",
    "\ty={\"output_1\": train_dataset, \"output_2\": train_dataset}, \n",
    "        #validation_data=(eval_dataset,eval_dataset), \n",
    "        batch_size=32,\n",
    "        epochs=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder weights\n",
      "[<tf.Variable 'u__encoder_42/dense_347/kernel:0' shape=(1, 1) dtype=float32, numpy=array([[0.3935958]], dtype=float32)>, <tf.Variable 'u__encoder_42/dense_347/bias:0' shape=(1,) dtype=float32, numpy=array([2.], dtype=float32)>, <tf.Variable 'u__encoder_42/dense_348/kernel:0' shape=(1, 5) dtype=float32, numpy=\n",
      "array([[-0.00096554,  0.02641617,  0.02482235,  0.02492729,  0.02716741]],\n",
      "      dtype=float32)>, <tf.Variable 'u__encoder_42/dense_348/bias:0' shape=(5,) dtype=float32, numpy=\n",
      "array([-8.476215e-05, -8.611680e-03, -8.712660e-03, -8.377144e-03,\n",
      "       -7.816047e-03], dtype=float32)>, <tf.Variable 'u__encoder_42/dense_349/kernel:0' shape=(5, 5) dtype=float32, numpy=\n",
      "array([[ 0.00435882, -0.00183346, -0.00116824,  0.0359099 ,  0.02937343],\n",
      "       [ 0.00308287, -0.00574021, -0.00332049,  0.05047046,  0.03118267],\n",
      "       [ 0.0019932 ,  0.00014373, -0.00117087,  0.04485384,  0.02776554],\n",
      "       [-0.00078366, -0.00197054,  0.00031014,  0.04394756,  0.03121982],\n",
      "       [ 0.00370167,  0.00166461, -0.00780858,  0.05041174,  0.03730459]],\n",
      "      dtype=float32)>, <tf.Variable 'u__encoder_42/dense_349/bias:0' shape=(5,) dtype=float32, numpy=\n",
      "array([-0.02186695, -0.01465068, -0.00950775, -0.01242055, -0.01075963],\n",
      "      dtype=float32)>, <tf.Variable 'u__encoder_42/dense_350/kernel:0' shape=(5, 2) dtype=float32, numpy=\n",
      "array([[-0.786623  , -0.24753575],\n",
      "       [ 0.8345251 ,  0.5437053 ],\n",
      "       [ 0.580156  , -0.8658264 ],\n",
      "       [-0.14906876,  0.66916585],\n",
      "       [-0.2803642 ,  0.7762693 ]], dtype=float32)>, <tf.Variable 'u__encoder_42/dense_350/bias:0' shape=(2,) dtype=float32, numpy=array([2.5682542, 1.7907226], dtype=float32)>]\n",
      "decoder weights\n",
      "[<tf.Variable 'u__ext_vae_42/u__decoder_42/dense_351/kernel:0' shape=(1, 5) dtype=float32, numpy=\n",
      "array([[ 0.33636084,  0.5280555 , -0.8783467 , -0.9533849 , -0.12827802]],\n",
      "      dtype=float32)>, <tf.Variable 'u__ext_vae_42/u__decoder_42/dense_351/bias:0' shape=(5,) dtype=float32, numpy=\n",
      "array([-0.16606122,  0.23488136,  0.        ,  0.        ,  0.        ],\n",
      "      dtype=float32)>, <tf.Variable 'u__ext_vae_42/u__decoder_42/dense_352/kernel:0' shape=(5, 5) dtype=float32, numpy=\n",
      "array([[ 0.4977411 ,  0.04483342,  0.5523778 ,  0.07839476, -0.6297316 ],\n",
      "       [-0.36032203, -0.33521596, -0.5446749 , -0.02390312,  0.4235016 ],\n",
      "       [-0.41428426,  0.47326863, -0.07142669,  0.6342139 ,  0.04893583],\n",
      "       [ 0.26549697,  0.11568189,  0.02258211,  0.03998595,  0.422042  ],\n",
      "       [ 0.5652281 ,  0.36782575, -0.09269184, -0.04855007,  0.3857559 ]],\n",
      "      dtype=float32)>, <tf.Variable 'u__ext_vae_42/u__decoder_42/dense_352/bias:0' shape=(5,) dtype=float32, numpy=\n",
      "array([ 0.        ,  0.        ,  0.        , -0.00803324,  0.27030772],\n",
      "      dtype=float32)>, <tf.Variable 'u__ext_vae_42/u__decoder_42/dense_353/kernel:0' shape=(5, 1) dtype=float32, numpy=\n",
      "array([[-0.9045439 ],\n",
      "       [-0.5542748 ],\n",
      "       [-0.9918716 ],\n",
      "       [-0.7897471 ],\n",
      "       [ 0.41929594]], dtype=float32)>, <tf.Variable 'u__ext_vae_42/u__decoder_42/dense_353/bias:0' shape=(1,) dtype=float32, numpy=array([0.16163687], dtype=float32)>, <tf.Variable 'u__ext_vae_42/u__decoder_42/dense_354/kernel:0' shape=(5, 1) dtype=float32, numpy=\n",
      "array([[-0.82449126],\n",
      "       [ 0.6487162 ],\n",
      "       [-0.55051017],\n",
      "       [-0.3842963 ],\n",
      "       [ 0.21043497]], dtype=float32)>, <tf.Variable 'u__ext_vae_42/u__decoder_42/dense_354/bias:0' shape=(1,) dtype=float32, numpy=array([-0.01456102], dtype=float32)>]\n",
      "Model: \"u__ext_vae_42\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " u__encoder_42 (U_Encoder)   multiple                  54        \n",
      "                                                                 \n",
      " u__decoder_42 (U_Decoder)   multiple                  52        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 106\n",
      "Trainable params: 106\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "vae(eval_dataset[2:4,:])\n",
    "print('encoder weights')\n",
    "print(vae.encoder.weights)\n",
    "print('decoder weights')\n",
    "print(vae.decoder.weights)\n",
    "vae.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[2.5749981]\n",
      " [2.1107657]\n",
      " [2.0994234]\n",
      " [2.1205082]\n",
      " [2.4401367]\n",
      " [2.1991394]\n",
      " [2.7691572]\n",
      " [4.1062613]\n",
      " [2.201768 ]\n",
      " [8.237326 ]], shape=(10, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(vae.encoder.alpha_layer(eval_dataset[:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "jd = tfd.JointDistributionSequential([\n",
    "    tfd.Normal(0., 1., name='z'),\n",
    "    tfd.Normal(0., 1., name='y'),\n",
    "    lambda y, z: tfd.Normal(y + z, 1., name='x')\n",
    "], validate_args=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfp.distributions.JointDistributionSequential(\"JointDistributionSequential\", batch_shape=[[], [], []], event_shape=[[], [], []], dtype=[float32, float32, float32])\n"
     ]
    }
   ],
   "source": [
    "print(jd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 1 required positional argument: 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [34]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m L \u001b[38;5;241m=\u001b[39m\u001b[43mtfpl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mVariableLayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43minitializer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtfk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minitializers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mConstant\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(L(eval_dataset))\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() missing 1 required positional argument: 'shape'"
     ]
    }
   ],
   "source": [
    "L =tfpl.VariableLayer(initializer=tfk.initializers.Constant(2))\n",
    "print(L(eval_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = tfkl.Dense(1,kernel_constraint = \n",
    "               tfk.constraints.MinMaxNorm(\n",
    "            min_value=0.0, max_value=0.0000001, rate=1.0, axis=0), \n",
    "               bias_initializer=tfk.initializers.Constant(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[2.]\n",
      " [2.]\n",
      " [2.]\n",
      " [2.]\n",
      " [2.]], shape=(5, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(G(eval_dataset[:5]*0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.layers as tfkl\n",
    "from tensorflow.keras import backend as K\n",
    "import tensorflow_probability as tfp\n",
    "tfd = tfp.distributions\n",
    "tfpl = tfp.layers\n",
    "tfb = tfp.bijectors\n",
    "\n",
    "\n",
    "# Constants #\n",
    "#############\n",
    "BATCH_SIZE = 8\n",
    "N_EPOCHS = 500\n",
    "PRIOR_TRAINABLE = True\n",
    "\n",
    "\n",
    "# Setup data #\n",
    "##############\n",
    "true_dist = tfd.MultivariateNormalDiag(\n",
    "    loc=[-1., 1., 5],  # must have length == NDIM\n",
    "    scale_diag=[0.5, 0.5, 0.9]\n",
    ")\n",
    "NDIM = true_dist.event_shape[0]\n",
    "print(NDIM)\n",
    "def gen_ds(n_iters=1e2):\n",
    "    iter_ix = 0\n",
    "    while iter_ix < n_iters:\n",
    "        y_out = true_dist.sample()\n",
    "        yield np.ones((1,), dtype=np.float32), y_out.numpy()\n",
    "        iter_ix += 1\n",
    "ds = tf.data.Dataset.from_generator(gen_ds, args=[1e2], output_types=(tf.float32, tf.float32),\n",
    "                                    output_shapes=((1,), (NDIM,))).batch(BATCH_SIZE)\n",
    "\n",
    "\n",
    "def make_mvn_prior(ndim, trainable=False):\n",
    "    if trainable:\n",
    "        loc = tf.Variable(tf.random.normal([ndim], stddev=0.1, dtype=tf.float32), name='prior_loc')\n",
    "        scale = tfp.util.TransformedVariable(\n",
    "            tf.random.normal([ndim], mean=1.0, stddev=0.1, dtype=tf.float32),\n",
    "            bijector=tfb.Chain([tfb.Shift(1e-5), tfb.Softplus(), tfb.Shift(0.5413)]), name='prior_scale')\n",
    "    else:\n",
    "        loc = tf.zeros(ndim)\n",
    "        scale = 1\n",
    "    prior = tfd.Independent(tfd.Normal(loc=loc, scale=scale), reinterpreted_batch_ndims=1)\n",
    "    return prior\n",
    "\n",
    "\n",
    "def make_mvn_dist_fn(_x_, ndim):\n",
    "    _loc = tfkl.Dense(ndim, name=\"loc_params\")(_x_)\n",
    "    _scale = tfkl.Dense(ndim, name=\"untransformed_scale_params\")(_x_)\n",
    "    _scale = tf.math.softplus(_scale + np.log(np.exp(1) - 1)) + 1e-5\n",
    "    make_dist_fn = lambda t: tfd.Independent(tfd.Normal(loc=t[0], scale=t[1]))\n",
    "    return make_dist_fn, [_loc, _scale]\n",
    "\n",
    "\n",
    "# Setup Model(s) #\n",
    "##################\n",
    "def make_input_output(prior):\n",
    "    _input = tfkl.Input(shape=(1,))\n",
    "    make_dist_fn, dist_inputs = make_mvn_dist_fn(_input, NDIM)\n",
    "    output = tfpl.DistributionLambda(\n",
    "        name=\"out_dist\",\n",
    "        make_distribution_fn=make_dist_fn,\n",
    "        activity_regularizer=tfpl.KLDivergenceRegularizer(prior, use_exact_kl=True, weight=0.1)\n",
    "    )(dist_inputs)\n",
    "    return _input, output\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([ 0.03274685 -0.08426258  0.03194337], shape=(3,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Make and Train #\n",
    "##################\n",
    "K.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "prior = make_mvn_prior(NDIM, trainable=PRIOR_TRAINABLE)\n",
    "print(prior.mean())\n",
    "_in, _out = make_input_output(prior)\n",
    "model = tf.keras.Model(_in, _out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([-0.6294408  -0.30990827  0.73676753], shape=(3,), dtype=float32)\n",
      "Epoch 1/500\n",
      "13/13 - 3s - loss: 7.4467 - 3s/epoch - 230ms/step\n",
      "Epoch 2/500\n",
      "13/13 - 2s - loss: 7.2316 - 2s/epoch - 135ms/step\n",
      "Epoch 3/500\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(prior\u001b[38;5;241m.\u001b[39mmean())\n\u001b[1;32m      2\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m y_true, model_out: \u001b[38;5;241m-\u001b[39mmodel_out\u001b[38;5;241m.\u001b[39mlog_prob(y_true))\n\u001b[0;32m----> 3\u001b[0m hist \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mN_EPOCHS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m loc_params \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mget_layer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloc_params\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mweights\n\u001b[1;32m      5\u001b[0m out_locs \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mones((\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)) \u001b[38;5;241m@\u001b[39m loc_params[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mnumpy() \u001b[38;5;241m+\u001b[39m loc_params[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "File \u001b[0;32m~/.conda/envs/tf/lib/python3.9/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.conda/envs/tf/lib/python3.9/site-packages/keras/engine/training.py:1564\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1556\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1557\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1558\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1561\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   1562\u001b[0m ):\n\u001b[1;32m   1563\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1564\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1565\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1566\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/.conda/envs/tf/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.conda/envs/tf/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    912\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    914\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 915\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    917\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    918\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/.conda/envs/tf/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    944\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    945\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    946\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 947\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stateless_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    948\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    949\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    950\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    951\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/.conda/envs/tf/lib/python3.9/site-packages/tensorflow/python/eager/function.py:2496\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2493\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m   2494\u001b[0m   (graph_function,\n\u001b[1;32m   2495\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2496\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2497\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/tf/lib/python3.9/site-packages/tensorflow/python/eager/function.py:1862\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1858\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1859\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1860\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1861\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1862\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1863\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1864\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1865\u001b[0m     args,\n\u001b[1;32m   1866\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1867\u001b[0m     executing_eagerly)\n\u001b[1;32m   1868\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/.conda/envs/tf/lib/python3.9/site-packages/tensorflow/python/eager/function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    497\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    498\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 499\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    505\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    506\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    507\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[1;32m    508\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    511\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[1;32m    512\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m~/.conda/envs/tf/lib/python3.9/site-packages/tensorflow/python/eager/execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(prior.mean())\n",
    "model.compile(optimizer='adam', loss=lambda y_true, model_out: -model_out.log_prob(y_true))\n",
    "hist = model.fit(ds, epochs=N_EPOCHS, verbose=2)\n",
    "loc_params = model.get_layer(\"loc_params\").weights\n",
    "out_locs = np.ones((1, 1)) @ loc_params[0].numpy() + loc_params[1].numpy()\n",
    "print(f\"Model est dist mean: {out_locs}\")\n",
    "print(f\"prior mean: {prior.mean()}\")\n",
    "\n",
    "\n",
    "# Plot Loss #\n",
    "#############\n",
    "plt.plot(hist.history['loss'])\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss (neg.log.lik)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "()\n",
      "<BatchDataset element_spec=(TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), TensorSpec(shape=(None, 1), dtype=tf.float32, name=None))>\n",
      "tfp.distributions.Independent(\"IndependentGamma\", batch_shape=[], event_shape=[1], dtype=float32)\n",
      "tf.Tensor([2.3291242], shape=(1,), dtype=float32)\n",
      "tfp.distributions.Independent(\"IndependentGamma\", batch_shape=[?], event_shape=[1], dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.layers as tfkl\n",
    "from tensorflow.keras import backend as K\n",
    "import tensorflow_probability as tfp\n",
    "tfd = tfp.distributions\n",
    "tfpl = tfp.layers\n",
    "tfb = tfp.bijectors\n",
    "\n",
    "\n",
    "# Constants #\n",
    "#############\n",
    "BATCH_SIZE = 8\n",
    "N_EPOCHS = 500\n",
    "PRIOR_TRAINABLE = True\n",
    "\n",
    "\n",
    "# Setup data #\n",
    "##############\n",
    "true_dist = tfd.Gamma(\n",
    "    concentration=[2],  # must have length == NDIM\n",
    "    rate=[1]\n",
    ")\n",
    "NDIM = true_dist.event_shape\n",
    "print(NDIM)\n",
    "def gen_ds(n_iters=1e2):\n",
    "    iter_ix = 0\n",
    "    while iter_ix < n_iters:\n",
    "        y_out = true_dist.sample()\n",
    "        yield np.ones((1,), dtype=np.float32), y_out.numpy()\n",
    "        iter_ix += 1\n",
    "ds = tf.data.Dataset.from_generator(gen_ds, args=[1e2], output_types=(tf.float32, tf.float32),\n",
    "                                    output_shapes=((1,), (1,))).batch(BATCH_SIZE)\n",
    "\n",
    "\n",
    "print(ds)\n",
    "\n",
    "def make_mvn_prior(ndim, trainable=False):\n",
    "    if trainable:\n",
    "        c = tf.Variable(tf.random.uniform([ndim], minval=1,maxval = 3, dtype=tf.float32), name='prior_c')\n",
    "        scale = 1\n",
    "    else:\n",
    "        loc = tf.zeros(ndim)\n",
    "        scale = 1\n",
    "    prior = tfd.Independent(tfd.Gamma(concentration=c, rate=scale), reinterpreted_batch_ndims=1)\n",
    "    print(prior)\n",
    "    return prior\n",
    "\n",
    "\n",
    "def make_mvn_dist_fn(_x_, ndim):\n",
    "    _c = tf.abs(tfkl.Dense(1, name=\"c_params\")(_x_))\n",
    "    _scale = tfkl.Dense(1, name=\"untransformed_scale_params\")(_x_)\n",
    "    _scale = tf.math.softplus(_scale + np.log(np.exp(1) - 1)) + 1e-5\n",
    "    make_dist_fn = lambda t: tfd.Independent(tfd.Gamma(concentration=t[0], rate=t[1]))\n",
    "    print(make_dist_fn([_c,_scale]))\n",
    "    return make_dist_fn, [_c, _scale]\n",
    "\n",
    "\n",
    "# Setup Model(s) #\n",
    "##################\n",
    "def make_input_output(prior):\n",
    "    _input = tfkl.Input(shape=(1,))\n",
    "    make_dist_fn, dist_inputs = make_mvn_dist_fn(_input, 1)\n",
    "    output = tfpl.DistributionLambda(\n",
    "        name=\"out_dist\",\n",
    "        make_distribution_fn=make_dist_fn,\n",
    "        activity_regularizer=tfpl.KLDivergenceRegularizer(prior, use_exact_kl=True, weight=0.1)\n",
    "    )(dist_inputs)\n",
    "    return _input, output\n",
    "\n",
    "# Make and Train #\n",
    "##################\n",
    "K.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "prior = make_mvn_prior(1, trainable=PRIOR_TRAINABLE)\n",
    "print(prior.mean())\n",
    "_in, _out = make_input_output(prior)\n",
    "model = tf.keras.Model(_in, _out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([2.2163537], shape=(1,), dtype=float32)\n",
      "Epoch 1/10\n",
      "13/13 - 2s - loss: 2.3797 - 2s/epoch - 140ms/step\n",
      "Epoch 2/10\n",
      "13/13 - 1s - loss: 2.3990 - 806ms/epoch - 62ms/step\n",
      "Epoch 3/10\n",
      "13/13 - 1s - loss: 2.3434 - 791ms/epoch - 61ms/step\n",
      "Epoch 4/10\n",
      "13/13 - 1s - loss: 2.2476 - 788ms/epoch - 61ms/step\n",
      "Epoch 5/10\n",
      "13/13 - 1s - loss: 2.2625 - 868ms/epoch - 67ms/step\n",
      "Epoch 6/10\n",
      "13/13 - 1s - loss: 2.1750 - 858ms/epoch - 66ms/step\n",
      "Epoch 7/10\n",
      "13/13 - 1s - loss: 2.2022 - 840ms/epoch - 65ms/step\n",
      "Epoch 8/10\n",
      "13/13 - 1s - loss: 2.0928 - 798ms/epoch - 61ms/step\n",
      "Epoch 9/10\n",
      "13/13 - 1s - loss: 2.0739 - 823ms/epoch - 63ms/step\n",
      "Epoch 10/10\n",
      "13/13 - 1s - loss: 2.1358 - 832ms/epoch - 64ms/step\n",
      "Model est dist mean: [[-0.70195439]]\n",
      "prior mean: [2.104279]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(prior.mean())\n",
    "model.compile(optimizer='adam', loss=lambda y_true, model_out: -model_out.log_prob(y_true))\n",
    "hist = model.fit(ds, epochs=10, verbose=2)\n",
    "loc_params = model.get_layer(\"c_params\").weights\n",
    "out_locs = np.ones((1, 1)) @ loc_params[0].numpy() + loc_params[1].numpy()\n",
    "print(f\"Model est dist mean: {out_locs}\")\n",
    "print(f\"prior mean: {prior.mean()}\")\n",
    "\n",
    "\n",
    "# Plot Loss #\n",
    "#############\n",
    "plt.plot(hist.history['loss'])\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss (neg.log.lik)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class U_Encoder(tfk.Model):    \n",
    "    def __init__(self):      \n",
    "        super(U_Encoder,self).__init__()\n",
    "        self.alpha        = 1.5\n",
    "        self.prior        = self.make_mvn_prior(1,True)\n",
    "        self.dense1       = tfkl.Dense(5, activation ='relu',kernel_initializer =tfk.initializers.RandomUniform(minval=0.01, maxval=0.02))\n",
    "        self.dense2       = tfkl.Dense(5, activation ='relu',kernel_initializer =tfk.initializers.RandomUniform(minval=0.01, maxval=0.02))\n",
    "        self.dense3       = tfkl.Dense(2, activation='relu',bias_initializer = tfk.initializers.RandomUniform(minval=1, maxval=2))\n",
    "        self.lambda1      = tfkl.Lambda(lambda x: tf.abs(x)+0.001)\n",
    "        self.dist_lambda3 = tfpl.DistributionLambda(\n",
    "                            make_distribution_fn=lambda t: (tfd.Gamma(\n",
    "                                concentration=t[...,0], rate=t[...,1])),\n",
    "            activity_regularizer=tfpl.KLDivergenceRegularizer(self.prior, use_exact_kl=True)\n",
    "        )  \n",
    "        \n",
    "        #self.KL_Loss      = tfpl.KLDivergenceAddLoss()\n",
    "        \n",
    "    def make_mvn_prior(self,ndim, trainable):       \n",
    "        if trainable:\n",
    "            c = tf.Variable(tf.random.uniform([ndim], minval=2.1,maxval = 2.101, dtype=tf.float32), name='prior_c')\n",
    "            print(c)\n",
    "            rate = 1\n",
    "        else:\n",
    "            c = self.alpha\n",
    "            rate = 1\n",
    "        prior = (tfd.Gamma(concentration=c, rate=rate))\n",
    "        return prior\n",
    "    \n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x = self.dense1(inputs)\n",
    "        x = self.dense2(x)\n",
    "        x = self.dense3(x)\n",
    "        x = self.dist_lambda3(x)\n",
    "        return(x)\n",
    "    \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'prior_c:0' shape=(1,) dtype=float32, numpy=array([2.100589], dtype=float32)>\n",
      "tfp.distributions._TensorCoercible(\"u__encoder_distribution_lambda_10_tensor_coercible\", batch_shape=[5], event_shape=[], dtype=float32)\n",
      "Model: \"u__encoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_32 (Dense)            multiple                  10        \n",
      "                                                                 \n",
      " dense_33 (Dense)            multiple                  30        \n",
      "                                                                 \n",
      " dense_34 (Dense)            multiple                  12        \n",
      "                                                                 \n",
      " lambda_12 (Lambda)          multiple                  0 (unused)\n",
      "                                                                 \n",
      " distribution_lambda_10 (Dis  multiple                 1         \n",
      " tributionLambda)                                                \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 53\n",
      "Trainable params: 53\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "enc = U_Encoder()\n",
    "\n",
    "print(enc(train_dataset[:5]))\n",
    "enc.summary()\n",
    "negative_log_likelihood = lambda x, rv_x: -rv_x.log_prob(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "8/8 [==============================] - 1s 5ms/step - loss: 2.3009\n",
      "Epoch 2/5\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 2.2391\n",
      "Epoch 3/5\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 2.1780\n",
      "Epoch 4/5\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 2.1136\n",
      "Epoch 5/5\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 2.0500\n",
      "<tf.Variable 'prior_c:0' shape=(1,) dtype=float32, numpy=array([2.061332], dtype=float32)>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "enc.compile(optimizer=tf.optimizers.Adam(learning_rate=1e-3),\n",
    "            loss=negative_log_likelihood)\n",
    "\n",
    "enc.fit(x=train_dataset,\n",
    "\ty=train_dataset, \n",
    "        #validation_data=(eval_dataset,eval_dataset), \n",
    "        batch_size=32,\n",
    "        epochs=5)\n",
    "print(enc.prior.concentration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "class U_Decoder(tfk.Model):\n",
    "    def __init__(self):\n",
    "        super(U_Decoder,self).__init__()\n",
    "        \n",
    "        self.dense1  = tfkl.Dense(5, use_bias=True, activation='relu')\n",
    "        self.lambda1 = tfkl.Lambda(lambda x: 1/x)\n",
    "        self.dense2  = tfkl.Dense(5, use_bias=True, activation='relu')\n",
    "        self.dense31 = tfkl.Dense(1, use_bias=True)\n",
    "        self.dense32 = tfkl.Dense(1, use_bias=True)\n",
    "        self.lambda2 = tfkl.Lambda(lambda x: tf.abs(x)+0.001)\n",
    "        self.concat1 = tfkl.Concatenate()\n",
    "        self.dist_lambda1 = tfpl.DistributionLambda(\n",
    "            make_distribution_fn=lambda t: tfd.Gamma(\n",
    "                concentration=t[...,0], rate=t[...,1]))\n",
    "        \n",
    "        \n",
    "    def call(self, inputs):\n",
    "        y1     = tfkl.Reshape(target_shape=[1])(inputs)\n",
    "        y     = self.lambda1(y1)\n",
    "        x     = self.dense1(y)\n",
    "        x     = self.dense2(x)\n",
    "        alpha = self.dense31(x*y1)\n",
    "        alpha = self.lambda2(alpha)\n",
    "        beta  = self.dense32(x)\n",
    "        beta  = self.lambda2(beta/y**2)\n",
    "        x     = self.concat1([alpha,beta])\n",
    "        x     = self.dist_lambda1(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "   \n",
    "class U_Ext_VAE(tfk.Model):\n",
    "    def __init__(self):      \n",
    "        super(U_Ext_VAE,self).__init__()\n",
    "        self.encoder = U_Encoder()\n",
    "        self.decoder = U_Decoder()\n",
    "        #self.Block   = tfpl.DistributionLambda(\n",
    "         #   make_distribution_fn=lambda d: tfd.Blockwise(\n",
    "        #        d))\n",
    "    \n",
    "    def call(self,inputs):\n",
    "        res =  self.decoder(self.encoder(inputs))\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'prior_c:0' shape=(1,) dtype=float32, numpy=array([2.1002862], dtype=float32)>\n",
      "tfp.distributions._TensorCoercible(\"u__ext_vae_2_u__decoder_2_distribution_lambda_16_tensor_coercible\", batch_shape=[5], event_shape=[], dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "vae = U_Ext_VAE()\n",
    "\n",
    "print(vae(train_dataset[:5]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "8/8 [==============================] - 2s 85ms/step - loss: 2.2905 - val_loss: nan\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 42ms/step - loss: 2.3243 - val_loss: nan\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 47ms/step - loss: 2.2797 - val_loss: nan\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 2.2825 - val_loss: nan\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 2.2838 - val_loss: nan\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 2.2494 - val_loss: nan\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 2.2327 - val_loss: nan\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 2.1874 - val_loss: nan\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 2.2195 - val_loss: nan\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 2.2630 - val_loss: nan\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 2.1986 - val_loss: nan\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 2.2009 - val_loss: nan\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 2.1630 - val_loss: nan\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 2.1904 - val_loss: nan\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 2.1878 - val_loss: nan\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 2.1898 - val_loss: nan\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 2.1575 - val_loss: nan\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 2.1285 - val_loss: nan\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 2.1377 - val_loss: nan\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.1488 - val_loss: nan\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 2.1584 - val_loss: nan\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 2.2310 - val_loss: nan\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 2.1053 - val_loss: nan\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.1310 - val_loss: nan\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.1032 - val_loss: nan\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 2.1115 - val_loss: nan\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 39ms/step - loss: 2.0635 - val_loss: nan\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 2.1092 - val_loss: nan\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 2.0673 - val_loss: nan\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 2.0771 - val_loss: nan\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 25ms/step - loss: 2.0502 - val_loss: nan\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.0646 - val_loss: nan\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 2.0625 - val_loss: nan\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 2.0050 - val_loss: nan\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 2.0516 - val_loss: nan\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 2.0685 - val_loss: nan\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 2.0265 - val_loss: nan\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 2.0009 - val_loss: nan\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.0139 - val_loss: nan\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 2.0214 - val_loss: nan\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.9922 - val_loss: nan\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 25ms/step - loss: 1.9928 - val_loss: nan\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 2.0038 - val_loss: nan\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 2.0353 - val_loss: nan\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.9561 - val_loss: nan\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 1.9702 - val_loss: nan\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 38ms/step - loss: 1.9417 - val_loss: nan\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 25ms/step - loss: 1.9460 - val_loss: nan\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 1.9164 - val_loss: nan\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 1.9489 - val_loss: nan\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 1.9511 - val_loss: nan\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 1.9384 - val_loss: nan\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 36ms/step - loss: 1.9145 - val_loss: nan\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.9917 - val_loss: nan\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 1.9436 - val_loss: nan\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.8791 - val_loss: nan\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 1.9200 - val_loss: nan\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.8840 - val_loss: nan\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 1.8956 - val_loss: nan\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.9231 - val_loss: nan\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 1.8775 - val_loss: nan\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 1.8653 - val_loss: nan\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 1.8766 - val_loss: nan\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.8520 - val_loss: nan\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.9051 - val_loss: nan\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 36ms/step - loss: 1.8483 - val_loss: nan\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 1.8478 - val_loss: nan\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.8806 - val_loss: nan\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 1.8323 - val_loss: nan\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 1.8335 - val_loss: nan\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.9153 - val_loss: nan\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 1.8174 - val_loss: nan\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 25ms/step - loss: 1.8239 - val_loss: nan\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 1.8253 - val_loss: nan\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 1.7828 - val_loss: nan\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 25ms/step - loss: 1.7832 - val_loss: nan\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.7900 - val_loss: nan\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.7723 - val_loss: nan\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.7822 - val_loss: nan\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.8115 - val_loss: nan\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.7876 - val_loss: nan\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.7315 - val_loss: nan\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.7393 - val_loss: nan\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.7222 - val_loss: nan\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.7300 - val_loss: nan\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 1.7397 - val_loss: nan\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 36ms/step - loss: 1.7273 - val_loss: nan\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.7482 - val_loss: nan\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.7425 - val_loss: nan\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.7737 - val_loss: nan\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.7157 - val_loss: nan\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 36ms/step - loss: 1.7485 - val_loss: nan\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.6892 - val_loss: nan\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 1.7262 - val_loss: nan\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.7136 - val_loss: nan\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 1.6903 - val_loss: nan\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 1.7386 - val_loss: nan\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.7267 - val_loss: nan\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 1.6828 - val_loss: nan\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 1.6779 - val_loss: nan\n",
      "<tf.Variable 'prior_c:0' shape=(1,) dtype=float32, numpy=array([1.8771465], dtype=float32)>\n"
     ]
    }
   ],
   "source": [
    "negative_log_likelihood = lambda x, rv_x: -rv_x.log_prob(x)\n",
    "vae.compile(optimizer=tf.optimizers.Adam(learning_rate=1e-4),\n",
    "            loss=negative_log_likelihood)\n",
    "\n",
    "vae.fit(x=train_dataset,\n",
    "\ty=train_dataset, \n",
    "        validation_data=(eval_dataset,eval_dataset), \n",
    "        batch_size=32,\n",
    "        epochs=100)\n",
    "print(vae.encoder.prior.concentration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tfk.Model):\n",
    "    \n",
    "    def __init__(self):      \n",
    "        super(Encoder,self).__init__()   \n",
    "        self.prior        = tfd.InverseGamma(concentration=1.5,scale=1)\n",
    "        self.dense1       = tfkl.Dense(5, activation ='relu',kernel_initializer =tfk.initializers.Zeros())\n",
    "        self.dense2       = tfkl.Dense(5, activation ='relu',kernel_initializer =tfk.initializers.Zeros())\n",
    "        self.dense3       = tfkl.Dense(2, activation='relu',bias_initializer = tfk.initializers.RandomUniform(minval=1, maxval=2))\n",
    "        self.lambda1      = tfkl.Lambda(lambda x: tf.abs(x)+0.001, name='posterior_params')\n",
    "        self.dist_lambda1 = tfpl.DistributionLambda(\n",
    "                            make_distribution_fn=lambda t: tfd.InverseGamma(\n",
    "                                concentration=t[...,0], scale=t[...,1]),\n",
    "                                    activity_regularizer=tfpl.KLDivergenceRegularizer(self.prior))  \n",
    "        \n",
    "        \n",
    "    def call(self, inputs):\n",
    "        print('encoder')\n",
    "        print(inputs)\n",
    "        x = self.dense1(inputs)\n",
    "        print(x)\n",
    "        x = self.dense2(x)\n",
    "        print(x)\n",
    "        x = self.dense3(x)\n",
    "        print(x)\n",
    "        x = self.lambda1(x)\n",
    "        print(x)\n",
    "        x = self.dist_lambda1(x)\n",
    "        print(x)\n",
    "        return x\n",
    "    \n",
    "class Decoder(tfk.Model):\n",
    "    def __init__(self):\n",
    "        super(Decoder,self).__init__()\n",
    "        self.dense1  = tfkl.Dense(5, use_bias=True, activation='relu')\n",
    "        self.dense2  = tfkl.Dense(5, use_bias=True, activation='relu')\n",
    "        self.dense31 = tfkl.Dense(1, use_bias=True)\n",
    "        self.dense32 = tfkl.Dense(1, use_bias=True)\n",
    "        self.lambda1 = tfkl.Lambda(lambda x: tf.abs(x)+0.001)\n",
    "        self.concat1 = tfkl.Concatenate()\n",
    "        self.dist_lambda1 = tfpl.DistributionLambda(\n",
    "            make_distribution_fn=lambda t: tfd.Gamma(\n",
    "                concentration=t[...,0], rate=t[...,1]))\n",
    "        \n",
    "        \n",
    "    def call(self, inputs):\n",
    "        print('decoder')\n",
    "        print(inputs)\n",
    "        x     = tfkl.Reshape(target_shape=[1])(inputs)\n",
    "        print(x)\n",
    "        x     = self.dense1(x)\n",
    "        print(x)\n",
    "        x     = self.dense2(x)\n",
    "        alpha = self.dense31(x)\n",
    "        alpha = self.lambda1(alpha)\n",
    "        beta  = self.dense32(x)\n",
    "        beta  = self.lambda1(beta/inputs**2)\n",
    "        x     = self.concat1([alpha,beta])\n",
    "        x     = self.dist_lambda1(x)\n",
    "        return x\n",
    "    \n",
    "class Ext_VAE(tfk.Model):\n",
    "    def __init__(self):      \n",
    "        super(Ext_VAE,self).__init__()\n",
    "        self.encoder = Encoder()\n",
    "        self.decoder = Decoder()\n",
    "    \n",
    "    def call(self,inputs):\n",
    "        return self.decoder(self.encoder(inputs))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae_k = Ext_VAE()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder\n",
      "tf.Tensor(\n",
      "[[ 0.5922747 ]\n",
      " [ 0.26953256]\n",
      " [ 0.6041775 ]\n",
      " [27.71446   ]\n",
      " [ 0.4887664 ]], shape=(5, 1), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]], shape=(5, 5), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]], shape=(5, 5), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[1.2782958 1.9192564]\n",
      " [1.2782958 1.9192564]\n",
      " [1.2782958 1.9192564]\n",
      " [1.2782958 1.9192564]\n",
      " [1.2782958 1.9192564]], shape=(5, 2), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[1.2792958 1.9202565]\n",
      " [1.2792958 1.9202565]\n",
      " [1.2792958 1.9202565]\n",
      " [1.2792958 1.9202565]\n",
      " [1.2792958 1.9202565]], shape=(5, 2), dtype=float32)\n",
      "tfp.distributions._TensorCoercible(\"ext_vae_6_encoder_6_distribution_lambda_48_tensor_coercible\", batch_shape=[5], event_shape=[], dtype=float32)\n",
      "decoder\n",
      "tfp.distributions._TensorCoercible(\"ext_vae_6_encoder_6_distribution_lambda_48_tensor_coercible\", batch_shape=[5], event_shape=[], dtype=float32)\n",
      "tf.Tensor(\n",
      "[[0.56478024]\n",
      " [2.273512  ]\n",
      " [1.5238252 ]\n",
      " [1.2870331 ]\n",
      " [0.7005551 ]], shape=(5, 1), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[0.         0.15894382 0.         0.44786963 0.33630413]\n",
      " [0.         0.6398252  0.         1.8028905  1.3537858 ]\n",
      " [0.         0.42884395 0.         1.2083905  0.9073772 ]\n",
      " [0.         0.36220452 0.         1.0206147  0.766377  ]\n",
      " [0.         0.19715439 0.         0.55553883 0.41715264]], shape=(5, 5), dtype=float32)\n",
      "tfp.distributions._TensorCoercible(\"ext_vae_6_decoder_6_distribution_lambda_49_tensor_coercible\", batch_shape=[5], event_shape=[], dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(vae_k(train_dataset[:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model with unknown prior\n",
    "class U_Encoder2(tfk.Model):    \n",
    "    def __init__(self):      \n",
    "        super(U_Encoder2,self).__init__()\n",
    "        self.alpha        = 1.5\n",
    "        self.c            = tf.Variable(tf.random.uniform([1], minval=1,maxval = 2, dtype=tf.float32), name='prior_c')\n",
    "        self.prior        = self.make_mvn_prior(1,True)\n",
    "        self.dense1       = tfkl.Dense(5, activation ='relu',kernel_initializer =tfk.initializers.RandomUniform(minval=0.01, maxval=0.02))\n",
    "        self.dense2       = tfkl.Dense(5, activation ='relu',kernel_initializer =tfk.initializers.RandomUniform(minval=0.01, maxval=0.02))\n",
    "        self.dense3       = tfkl.Dense(2, activation='relu',bias_initializer = tfk.initializers.RandomUniform(minval=1, maxval=2))\n",
    "        self.lambda1      = tfkl.Lambda(lambda x: tf.abs(x)+0.001)\n",
    "        self.dist_lambda3 = tfpl.DistributionLambda(\n",
    "                            make_distribution_fn=lambda t: (tfd.Gamma(\n",
    "                                concentration=t[...,0], rate=t[...,1])),\n",
    "            activity_regularizer=tfpl.KLDivergenceRegularizer(self.prior, use_exact_kl=True)\n",
    "        )  \n",
    "        \n",
    "        #self.KL_Loss      = tfpl.KLDivergenceAddLoss()\n",
    "        \n",
    "    def make_mvn_prior(self,ndim, trainable):       \n",
    "        if trainable:\n",
    "            c = self.c\n",
    "            print(c)\n",
    "            rate = 1\n",
    "        else:\n",
    "            c = self.alpha\n",
    "            rate = 1\n",
    "        prior = (tfd.Gamma(concentration=c, rate=rate))\n",
    "        return prior\n",
    "    \n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x = self.dense1(inputs)\n",
    "        x = self.dense2(x)\n",
    "        x = self.dense3(x)\n",
    "        x = self.dist_lambda3(x)\n",
    "        return(x)\n",
    "    \n",
    " \n",
    " \n",
    "class U_Decoder2(tfk.Model):\n",
    "    def __init__(self):\n",
    "        super(U_Decoder2,self).__init__()\n",
    "        \n",
    "        self.dense1  = tfkl.Dense(5, use_bias=True, activation='relu')\n",
    "        self.lambda1 = tfkl.Lambda(lambda x: 1/x)\n",
    "        self.dense2  = tfkl.Dense(5, use_bias=True, activation='relu')\n",
    "        self.dense31 = tfkl.Dense(1, use_bias=True)\n",
    "        self.dense32 = tfkl.Dense(1, use_bias=True)\n",
    "        self.lambda2 = tfkl.Lambda(lambda x: tf.abs(x)+0.001)\n",
    "        self.concat1 = tfkl.Concatenate()\n",
    "        self.dist_lambda1 = tfpl.DistributionLambda(\n",
    "            make_distribution_fn=lambda t: tfd.Gamma(\n",
    "                concentration=t[...,0], rate=t[...,1]))\n",
    "        \n",
    "        \n",
    "    def call(self, inputs):\n",
    "        y     = tfkl.Reshape(target_shape=[1])(inputs)\n",
    "        y     = self.lambda1(y)\n",
    "        x     = self.dense1(y)\n",
    "        x     = self.dense2(x)\n",
    "        alpha = self.dense31(x/y)\n",
    "        alpha = self.lambda1(alpha)\n",
    "        beta  = self.dense32(x)\n",
    "        beta  = self.lambda2(beta/y**2)\n",
    "        x     = self.concat1([alpha,beta])\n",
    "        x     = self.dist_lambda1(x)\n",
    "        return x\n",
    "\n",
    "    \n",
    "class U_Ext_VAE2(tfk.Model):\n",
    "    def __init__(self):      \n",
    "        super(U_Ext_VAE2,self).__init__()\n",
    "        self.encoder = U_Encoder2()\n",
    "        self.decoder = U_Decoder2()\n",
    "        #self.Block   = tfpl.DistributionLambda(\n",
    "         #   make_distribution_fn=lambda d: tfd.Blockwise(\n",
    "        #        d))\n",
    "    \n",
    "    def call(self,inputs):\n",
    "        res =  self.decoder(self.encoder(inputs))\n",
    "        return res\n",
    "    \n",
    "# Standard VAE as competitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([1. 1. 1.], shape=(3,), dtype=float32)\n",
      "<tf.Variable 'prior_c:0' shape=(1,) dtype=float32, numpy=array([1.3221647], dtype=float32)>\n",
      "tfp.distributions._TensorCoercible(\"u__encoder2_18_distribution_lambda_91_tensor_coercible\", batch_shape=[5], event_shape=[], dtype=float32)\n",
      "<tf.Variable 'prior_c:0' shape=(1,) dtype=float32, numpy=array([1.3221647], dtype=float32)>\n"
     ]
    }
   ],
   "source": [
    "G = tfpl.VariableLayer(shape=[3], initializer = 'ones')\n",
    "print(G([3,5,10]))\n",
    "enco = U_Encoder2()\n",
    "print(enco(train_dataset[:5]))\n",
    "print(enco.c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"u__encoder2_18\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_303 (Dense)           multiple                  10        \n",
      "                                                                 \n",
      " dense_304 (Dense)           multiple                  30        \n",
      "                                                                 \n",
      " dense_305 (Dense)           multiple                  12        \n",
      "                                                                 \n",
      " lambda_105 (Lambda)         multiple                  0 (unused)\n",
      "                                                                 \n",
      " distribution_lambda_91 (Dis  multiple                 1         \n",
      " tributionLambda)                                                \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 53\n",
      "Trainable params: 53\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([1.3221647], dtype=float32),\n",
       " array([[0.01622628, 0.0197575 , 0.01816324, 0.01926297, 0.01248426]],\n",
       "       dtype=float32),\n",
       " array([0., 0., 0., 0., 0.], dtype=float32),\n",
       " array([[0.01893114, 0.01302198, 0.01395604, 0.01376069, 0.0197444 ],\n",
       "        [0.0129853 , 0.01052482, 0.01933458, 0.01453449, 0.01571166],\n",
       "        [0.01918814, 0.01771098, 0.01112029, 0.01351321, 0.01090526],\n",
       "        [0.01297861, 0.01755456, 0.01433524, 0.01858643, 0.01950116],\n",
       "        [0.01476115, 0.01123269, 0.01905973, 0.0194897 , 0.01717541]],\n",
       "       dtype=float32),\n",
       " array([0., 0., 0., 0., 0.], dtype=float32),\n",
       " array([[ 0.813419  , -0.5900625 ],\n",
       "        [-0.65594196, -0.75153774],\n",
       "        [ 0.20350325,  0.8429414 ],\n",
       "        [-0.35190642,  0.53639483],\n",
       "        [-0.88005733, -0.1692347 ]], dtype=float32),\n",
       " array([1.2374572, 1.3579409], dtype=float32)]"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "enco.summary()\n",
    "enco.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['constant:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['constant:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "8/8 [==============================] - 1s 4ms/step - loss: 1.1720\n",
      "Epoch 2/5\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.1660\n",
      "Epoch 3/5\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.1604\n",
      "Epoch 4/5\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.1532\n",
      "Epoch 5/5\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.1464\n",
      "tf.Tensor([1.], shape=(1,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "enco.compile(optimizer=tf.optimizers.Adam(learning_rate=1e-3),\n",
    "            loss=negative_log_likelihood)\n",
    "\n",
    "enco.fit(x=train_dataset,\n",
    "\ty=train_dataset, \n",
    "        #validation_data=(eval_dataset,eval_dataset), \n",
    "        batch_size=32,\n",
    "        epochs=5)\n",
    "print(enco.var1([0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = '/home/nlafon/These/Extreme_VAE/tmp/radius1/Ext_VAE/Learnable_prior2/U_Ext_VAE_checkpoint'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('_CHECKPOINTABLE_OBJECT_GRAPH', []), ('decoder/dense1/bias/.ATTRIBUTES/VARIABLE_VALUE', [5]), ('decoder/dense1/bias/.OPTIMIZER_SLOT/optimizer/m/.ATTRIBUTES/VARIABLE_VALUE', [5]), ('decoder/dense1/bias/.OPTIMIZER_SLOT/optimizer/v/.ATTRIBUTES/VARIABLE_VALUE', [5]), ('decoder/dense1/kernel/.ATTRIBUTES/VARIABLE_VALUE', [1, 5]), ('decoder/dense1/kernel/.OPTIMIZER_SLOT/optimizer/m/.ATTRIBUTES/VARIABLE_VALUE', [1, 5]), ('decoder/dense1/kernel/.OPTIMIZER_SLOT/optimizer/v/.ATTRIBUTES/VARIABLE_VALUE', [1, 5]), ('decoder/dense2/bias/.ATTRIBUTES/VARIABLE_VALUE', [5]), ('decoder/dense2/bias/.OPTIMIZER_SLOT/optimizer/m/.ATTRIBUTES/VARIABLE_VALUE', [5]), ('decoder/dense2/bias/.OPTIMIZER_SLOT/optimizer/v/.ATTRIBUTES/VARIABLE_VALUE', [5]), ('decoder/dense2/kernel/.ATTRIBUTES/VARIABLE_VALUE', [5, 5]), ('decoder/dense2/kernel/.OPTIMIZER_SLOT/optimizer/m/.ATTRIBUTES/VARIABLE_VALUE', [5, 5]), ('decoder/dense2/kernel/.OPTIMIZER_SLOT/optimizer/v/.ATTRIBUTES/VARIABLE_VALUE', [5, 5]), ('decoder/dense31/bias/.ATTRIBUTES/VARIABLE_VALUE', [1]), ('decoder/dense31/bias/.OPTIMIZER_SLOT/optimizer/m/.ATTRIBUTES/VARIABLE_VALUE', [1]), ('decoder/dense31/bias/.OPTIMIZER_SLOT/optimizer/v/.ATTRIBUTES/VARIABLE_VALUE', [1]), ('decoder/dense31/kernel/.ATTRIBUTES/VARIABLE_VALUE', [5, 1]), ('decoder/dense31/kernel/.OPTIMIZER_SLOT/optimizer/m/.ATTRIBUTES/VARIABLE_VALUE', [5, 1]), ('decoder/dense31/kernel/.OPTIMIZER_SLOT/optimizer/v/.ATTRIBUTES/VARIABLE_VALUE', [5, 1]), ('decoder/dense32/bias/.ATTRIBUTES/VARIABLE_VALUE', [1]), ('decoder/dense32/bias/.OPTIMIZER_SLOT/optimizer/m/.ATTRIBUTES/VARIABLE_VALUE', [1]), ('decoder/dense32/bias/.OPTIMIZER_SLOT/optimizer/v/.ATTRIBUTES/VARIABLE_VALUE', [1]), ('decoder/dense32/kernel/.ATTRIBUTES/VARIABLE_VALUE', [5, 1]), ('decoder/dense32/kernel/.OPTIMIZER_SLOT/optimizer/m/.ATTRIBUTES/VARIABLE_VALUE', [5, 1]), ('decoder/dense32/kernel/.OPTIMIZER_SLOT/optimizer/v/.ATTRIBUTES/VARIABLE_VALUE', [5, 1]), ('encoder/dense1/bias/.ATTRIBUTES/VARIABLE_VALUE', [5]), ('encoder/dense1/bias/.OPTIMIZER_SLOT/encoder/optimizer/m/.ATTRIBUTES/VARIABLE_VALUE', [5]), ('encoder/dense1/bias/.OPTIMIZER_SLOT/encoder/optimizer/v/.ATTRIBUTES/VARIABLE_VALUE', [5]), ('encoder/dense1/bias/.OPTIMIZER_SLOT/optimizer/m/.ATTRIBUTES/VARIABLE_VALUE', [5]), ('encoder/dense1/bias/.OPTIMIZER_SLOT/optimizer/v/.ATTRIBUTES/VARIABLE_VALUE', [5]), ('encoder/dense1/kernel/.ATTRIBUTES/VARIABLE_VALUE', [1, 5]), ('encoder/dense1/kernel/.OPTIMIZER_SLOT/encoder/optimizer/m/.ATTRIBUTES/VARIABLE_VALUE', [1, 5]), ('encoder/dense1/kernel/.OPTIMIZER_SLOT/encoder/optimizer/v/.ATTRIBUTES/VARIABLE_VALUE', [1, 5]), ('encoder/dense1/kernel/.OPTIMIZER_SLOT/optimizer/m/.ATTRIBUTES/VARIABLE_VALUE', [1, 5]), ('encoder/dense1/kernel/.OPTIMIZER_SLOT/optimizer/v/.ATTRIBUTES/VARIABLE_VALUE', [1, 5]), ('encoder/dense2/bias/.ATTRIBUTES/VARIABLE_VALUE', [5]), ('encoder/dense2/bias/.OPTIMIZER_SLOT/encoder/optimizer/m/.ATTRIBUTES/VARIABLE_VALUE', [5]), ('encoder/dense2/bias/.OPTIMIZER_SLOT/encoder/optimizer/v/.ATTRIBUTES/VARIABLE_VALUE', [5]), ('encoder/dense2/bias/.OPTIMIZER_SLOT/optimizer/m/.ATTRIBUTES/VARIABLE_VALUE', [5]), ('encoder/dense2/bias/.OPTIMIZER_SLOT/optimizer/v/.ATTRIBUTES/VARIABLE_VALUE', [5]), ('encoder/dense2/kernel/.ATTRIBUTES/VARIABLE_VALUE', [5, 5]), ('encoder/dense2/kernel/.OPTIMIZER_SLOT/encoder/optimizer/m/.ATTRIBUTES/VARIABLE_VALUE', [5, 5]), ('encoder/dense2/kernel/.OPTIMIZER_SLOT/encoder/optimizer/v/.ATTRIBUTES/VARIABLE_VALUE', [5, 5]), ('encoder/dense2/kernel/.OPTIMIZER_SLOT/optimizer/m/.ATTRIBUTES/VARIABLE_VALUE', [5, 5]), ('encoder/dense2/kernel/.OPTIMIZER_SLOT/optimizer/v/.ATTRIBUTES/VARIABLE_VALUE', [5, 5]), ('encoder/dense3/bias/.ATTRIBUTES/VARIABLE_VALUE', [2]), ('encoder/dense3/bias/.OPTIMIZER_SLOT/encoder/optimizer/m/.ATTRIBUTES/VARIABLE_VALUE', [2]), ('encoder/dense3/bias/.OPTIMIZER_SLOT/encoder/optimizer/v/.ATTRIBUTES/VARIABLE_VALUE', [2]), ('encoder/dense3/bias/.OPTIMIZER_SLOT/optimizer/m/.ATTRIBUTES/VARIABLE_VALUE', [2]), ('encoder/dense3/bias/.OPTIMIZER_SLOT/optimizer/v/.ATTRIBUTES/VARIABLE_VALUE', [2]), ('encoder/dense3/kernel/.ATTRIBUTES/VARIABLE_VALUE', [5, 2]), ('encoder/dense3/kernel/.OPTIMIZER_SLOT/encoder/optimizer/m/.ATTRIBUTES/VARIABLE_VALUE', [5, 2]), ('encoder/dense3/kernel/.OPTIMIZER_SLOT/encoder/optimizer/v/.ATTRIBUTES/VARIABLE_VALUE', [5, 2]), ('encoder/dense3/kernel/.OPTIMIZER_SLOT/optimizer/m/.ATTRIBUTES/VARIABLE_VALUE', [5, 2]), ('encoder/dense3/kernel/.OPTIMIZER_SLOT/optimizer/v/.ATTRIBUTES/VARIABLE_VALUE', [5, 2]), ('encoder/optimizer/beta_1/.ATTRIBUTES/VARIABLE_VALUE', []), ('encoder/optimizer/beta_2/.ATTRIBUTES/VARIABLE_VALUE', []), ('encoder/optimizer/decay/.ATTRIBUTES/VARIABLE_VALUE', []), ('encoder/optimizer/iter/.ATTRIBUTES/VARIABLE_VALUE', []), ('encoder/optimizer/learning_rate/.ATTRIBUTES/VARIABLE_VALUE', []), ('encoder/prior/_concentration/.ATTRIBUTES/VARIABLE_VALUE', [1]), ('encoder/prior/_concentration/.OPTIMIZER_SLOT/encoder/optimizer/m/.ATTRIBUTES/VARIABLE_VALUE', [1]), ('encoder/prior/_concentration/.OPTIMIZER_SLOT/encoder/optimizer/v/.ATTRIBUTES/VARIABLE_VALUE', [1]), ('encoder/prior/_concentration/.OPTIMIZER_SLOT/optimizer/m/.ATTRIBUTES/VARIABLE_VALUE', [1]), ('encoder/prior/_concentration/.OPTIMIZER_SLOT/optimizer/v/.ATTRIBUTES/VARIABLE_VALUE', [1]), ('optimizer/beta_1/.ATTRIBUTES/VARIABLE_VALUE', []), ('optimizer/beta_2/.ATTRIBUTES/VARIABLE_VALUE', []), ('optimizer/decay/.ATTRIBUTES/VARIABLE_VALUE', []), ('optimizer/iter/.ATTRIBUTES/VARIABLE_VALUE', []), ('optimizer/learning_rate/.ATTRIBUTES/VARIABLE_VALUE', [])]\n",
      "[1.1216836]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.training import checkpoint_utils as cp\n",
    "\n",
    "print(cp.list_variables(checkpoint_path))\n",
    "print(cp.load_variable(checkpoint_path,name = 'encoder/prior/_concentration/.ATTRIBUTES/VARIABLE_VALUE'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Prob_models as PM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'prior_c:0' shape=(1,) dtype=float32, numpy=array([1.4716179], dtype=float32)>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x7fc150b3cfd0>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vae_test = PM.U_Ext_VAE()\n",
    "vae_test.load_weights(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'prior_c:0' shape=(1,) dtype=float32, numpy=array([1.2315114], dtype=float32)>\n"
     ]
    }
   ],
   "source": [
    "print(vae_test.encoder.prior.concentration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class U_Encoder2(tfk.Model):    \n",
    "    def __init__(self):      \n",
    "        super(U_Encoder2,self).__init__()\n",
    "        self.alpha        = 1.5\n",
    "        self.alpha_tot  = tf.Variable(tf.random.uniform([1], minval=2,maxval = 2.5, dtype=tf.float32), name='alpha_post')\n",
    "        self.prior        = self.make_mvn_prior(1,True)\n",
    "        self.dense1       = tfkl.Dense(5, activation ='relu',kernel_initializer =tfk.initializers.RandomUniform(minval=0.01, maxval=0.02))\n",
    "        self.dense2       = tfkl.Dense(5, activation ='relu',kernel_initializer =tfk.initializers.RandomUniform(minval=0.01, maxval=0.02))\n",
    "        self.dense3       = tfkl.Dense(1, activation='relu',bias_initializer = tfk.initializers.RandomUniform(minval=1, maxval=2))\n",
    "        self.lambda1      = tfkl.Lambda(lambda x: tf.abs(x)+0.001)\n",
    "        self.dist_lambda3 = tfpl.DistributionLambda(\n",
    "                            make_distribution_fn=lambda t: (tfd.Gamma(\n",
    "                                concentration=self.alpha_post, rate=t[...,0])),\n",
    "            activity_regularizer=tfpl.KLDivergenceRegularizer(self.prior, use_exact_kl=True)\n",
    "        )  \n",
    "        \n",
    "        #self.KL_Loss      = tfpl.KLDivergenceAddLoss()\n",
    "        \n",
    "    def make_mvn_prior(self,ndim, trainable):       \n",
    "        if trainable:\n",
    "            c = self.alpha_tot\n",
    "            print(c)\n",
    "            rate = 1\n",
    "        else:\n",
    "            c = self.alpha\n",
    "            rate = 1\n",
    "        prior = (tfd.Gamma(concentration=c, rate=rate))\n",
    "        return prior\n",
    "    \n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x = self.dense1(inputs)\n",
    "        x = self.dense2(x)\n",
    "        x = self.dense3(x)\n",
    "        x = self.dist_lambda3(x)\n",
    "        return(x)\n",
    "    \n",
    " \n",
    " \n",
    "class U_Decoder2(tfk.Model):\n",
    "    def __init__(self):\n",
    "        super(U_Decoder2,self).__init__()\n",
    "        \n",
    "        self.dense1  = tfkl.Dense(5, use_bias=True, activation='relu')\n",
    "        self.lambda1 = tfkl.Lambda(lambda x: 1/x)\n",
    "        self.dense2  = tfkl.Dense(5, use_bias=True, activation='relu')\n",
    "        self.dense31 = tfkl.Dense(1, use_bias=True)\n",
    "        self.dense32 = tfkl.Dense(1, use_bias=True)\n",
    "        self.lambda2 = tfkl.Lambda(lambda x: tf.abs(x)+0.001)\n",
    "        self.concat1 = tfkl.Concatenate()\n",
    "        self.dist_lambda1 = tfpl.DistributionLambda(\n",
    "            make_distribution_fn=lambda t: tfd.Gamma(\n",
    "                concentration=t[...,0], rate=t[...,1]))\n",
    "        \n",
    "        \n",
    "    def call(self, inputs):\n",
    "        y     = tfkl.Reshape(target_shape=[1])(inputs)\n",
    "        y     = self.lambda1(y)\n",
    "        x     = self.dense1(y)\n",
    "        x     = self.dense2(x)\n",
    "        alpha = self.dense31(x)\n",
    "        alpha = self.lambda1(alpha)\n",
    "        beta  = self.dense32(x)\n",
    "        beta  = self.lambda2(beta/y**2)\n",
    "        x     = self.concat1([alpha,beta])\n",
    "        x     = self.dist_lambda1(x)\n",
    "        return x\n",
    "\n",
    "    \n",
    "class U_Ext_VAE2(tfk.Model):\n",
    "    def __init__(self):      \n",
    "        super(U_Ext_VAE2,self).__init__()\n",
    "        self.encoder = U_Encoder2()\n",
    "        self.decoder = U_Decoder2()\n",
    "        #self.Block   = tfpl.DistributionLambda(\n",
    "         #   make_distribution_fn=lambda d: tfd.Blockwise(\n",
    "        #        d))\n",
    "    \n",
    "    def call(self,inputs):\n",
    "        res =  self.decoder(self.encoder(inputs))\n",
    "        return res\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'prior_c:0' shape=(1,) dtype=float32, numpy=array([2.3300772], dtype=float32)>\n",
      "<tf.Variable 'alpha_post:0' shape=(1,) dtype=float32, numpy=array([2.043808], dtype=float32)>\n",
      "Epoch 1/5\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (distribution_lambda_5), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'alpha_post:0' shape=(1,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "8/8 [==============================] - 1s 3ms/step - loss: 2.6306\n",
      "Epoch 2/5\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2.5832\n",
      "Epoch 3/5\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2.5361\n",
      "Epoch 4/5\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2.4876\n",
      "Epoch 5/5\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2.4392\n",
      "<tf.Variable 'alpha_post:0' shape=(1,) dtype=float32, numpy=array([2.0810235], dtype=float32)>\n"
     ]
    }
   ],
   "source": [
    "enco = U_Encoder2()\n",
    "print(enco.alpha_post)\n",
    "negative_log_likelihood = lambda x, rv_x: -rv_x.log_prob(x)\n",
    "enco.compile(optimizer=tf.optimizers.Adam(learning_rate=1e-3),\n",
    "            loss=negative_log_likelihood)\n",
    "\n",
    "enco.fit(x=train_dataset,\n",
    "\ty=train_dataset, \n",
    "        #validation_data=(eval_dataset,eval_dataset), \n",
    "        batch_size=32,\n",
    "        epochs=5)\n",
    "print(enco.alpha_post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'prior_c:0' shape=(1,) dtype=float32, numpy=array([2.0348763], dtype=float32)>\n",
      "<tf.Variable 'prior_c:0' shape=(1,) dtype=float32, numpy=array([2.0348763], dtype=float32)>\n",
      "<tf.Variable 'alpha_post:0' shape=(1,) dtype=float32, numpy=array([2.3697517], dtype=float32)>\n"
     ]
    }
   ],
   "source": [
    "vae = U_Ext_VAE2()\n",
    "print(vae.encoder.prior.concentration)\n",
    "print(vae.encoder.alpha_post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "8/8 [==============================] - 2s 107ms/step - loss: 1.0746 - val_loss: 1.1184\n",
      "Epoch 2/1000\n",
      "8/8 [==============================] - 0s 25ms/step - loss: 1.0820 - val_loss: 1.1290\n",
      "Epoch 3/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0814 - val_loss: 1.1142\n",
      "Epoch 4/1000\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 1.0767 - val_loss: 1.1217\n",
      "Epoch 5/1000\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 1.0858 - val_loss: 1.1273\n",
      "Epoch 6/1000\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 1.0885 - val_loss: 1.1206\n",
      "Epoch 7/1000\n",
      "8/8 [==============================] - 0s 24ms/step - loss: 1.0777 - val_loss: 1.1220\n",
      "Epoch 8/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0862 - val_loss: 1.1225\n",
      "Epoch 9/1000\n",
      "8/8 [==============================] - 0s 25ms/step - loss: 1.0765 - val_loss: 1.1159\n",
      "Epoch 10/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0831 - val_loss: 1.1356\n",
      "Epoch 11/1000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.0673 - val_loss: 1.1238\n",
      "Epoch 12/1000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.0698 - val_loss: 1.1220\n",
      "Epoch 13/1000\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 1.0930 - val_loss: 1.1187\n",
      "Epoch 14/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0675 - val_loss: 1.1160\n",
      "Epoch 15/1000\n",
      "8/8 [==============================] - 0s 25ms/step - loss: 1.0749 - val_loss: 1.1255\n",
      "Epoch 16/1000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 1.0742 - val_loss: 1.1227\n",
      "Epoch 17/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0928 - val_loss: 1.1234\n",
      "Epoch 18/1000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.0761 - val_loss: 1.1161\n",
      "Epoch 19/1000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 1.0734 - val_loss: 1.1151\n",
      "Epoch 20/1000\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 1.0821 - val_loss: 1.1304\n",
      "Epoch 21/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0713 - val_loss: 1.1207\n",
      "Epoch 22/1000\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 1.0817 - val_loss: 1.1193\n",
      "Epoch 23/1000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.0763 - val_loss: 1.1129\n",
      "Epoch 24/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0810 - val_loss: 1.1223\n",
      "Epoch 25/1000\n",
      "8/8 [==============================] - 0s 25ms/step - loss: 1.0818 - val_loss: 1.1207\n",
      "Epoch 26/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0857 - val_loss: 1.1238\n",
      "Epoch 27/1000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 1.0765 - val_loss: 1.1145\n",
      "Epoch 28/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0869 - val_loss: 1.1213\n",
      "Epoch 29/1000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.0842 - val_loss: 1.1201\n",
      "Epoch 30/1000\n",
      "8/8 [==============================] - 0s 36ms/step - loss: 1.0870 - val_loss: 1.1219\n",
      "Epoch 31/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0818 - val_loss: 1.1198\n",
      "Epoch 32/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0764 - val_loss: 1.1227\n",
      "Epoch 33/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0765 - val_loss: 1.1308\n",
      "Epoch 34/1000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.0766 - val_loss: 1.1305\n",
      "Epoch 35/1000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 1.0713 - val_loss: 1.1292\n",
      "Epoch 36/1000\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 1.0731 - val_loss: 1.1341\n",
      "Epoch 37/1000\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 1.0746 - val_loss: 1.1129\n",
      "Epoch 38/1000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 1.1021 - val_loss: 1.1276\n",
      "Epoch 39/1000\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 1.0834 - val_loss: 1.1186\n",
      "Epoch 40/1000\n",
      "8/8 [==============================] - 0s 25ms/step - loss: 1.0785 - val_loss: 1.1087\n",
      "Epoch 41/1000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 1.0767 - val_loss: 1.1310\n",
      "Epoch 42/1000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 1.0845 - val_loss: 1.1252\n",
      "Epoch 43/1000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.0761 - val_loss: 1.1191\n",
      "Epoch 44/1000\n",
      "8/8 [==============================] - 0s 21ms/step - loss: 1.0774 - val_loss: 1.1413\n",
      "Epoch 45/1000\n",
      "8/8 [==============================] - 0s 25ms/step - loss: 1.1042 - val_loss: 1.1188\n",
      "Epoch 46/1000\n",
      "8/8 [==============================] - 0s 25ms/step - loss: 1.0664 - val_loss: 1.1352\n",
      "Epoch 47/1000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.0807 - val_loss: 1.1156\n",
      "Epoch 48/1000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.0802 - val_loss: 1.1222\n",
      "Epoch 49/1000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.0777 - val_loss: 1.1307\n",
      "Epoch 50/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0822 - val_loss: 1.1265\n",
      "Epoch 51/1000\n",
      "8/8 [==============================] - 0s 25ms/step - loss: 1.0773 - val_loss: 1.1196\n",
      "Epoch 52/1000\n",
      "8/8 [==============================] - 0s 36ms/step - loss: 1.0923 - val_loss: 1.1247\n",
      "Epoch 53/1000\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 1.0751 - val_loss: 1.1207\n",
      "Epoch 54/1000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 1.0787 - val_loss: 1.1271\n",
      "Epoch 55/1000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.0704 - val_loss: 1.1201\n",
      "Epoch 56/1000\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 1.0723 - val_loss: 1.1249\n",
      "Epoch 57/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0735 - val_loss: 1.1280\n",
      "Epoch 58/1000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 1.0717 - val_loss: 1.1349\n",
      "Epoch 59/1000\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 1.0843 - val_loss: 1.1249\n",
      "Epoch 60/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0758 - val_loss: 1.1286\n",
      "Epoch 61/1000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.0822 - val_loss: 1.1250\n",
      "Epoch 62/1000\n",
      "8/8 [==============================] - 0s 37ms/step - loss: 1.0657 - val_loss: 1.1193\n",
      "Epoch 63/1000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.0782 - val_loss: 1.1285\n",
      "Epoch 64/1000\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 1.0716 - val_loss: 1.1226\n",
      "Epoch 65/1000\n",
      "8/8 [==============================] - 0s 24ms/step - loss: 1.0745 - val_loss: 1.1205\n",
      "Epoch 66/1000\n",
      "8/8 [==============================] - 0s 23ms/step - loss: 1.0858 - val_loss: 1.1301\n",
      "Epoch 67/1000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.0815 - val_loss: 1.1300\n",
      "Epoch 68/1000\n",
      "8/8 [==============================] - 0s 25ms/step - loss: 1.0674 - val_loss: 1.1294\n",
      "Epoch 69/1000\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 1.0829 - val_loss: 1.1195\n",
      "Epoch 70/1000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.0860 - val_loss: 1.1236\n",
      "Epoch 71/1000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 1.0769 - val_loss: 1.1245\n",
      "Epoch 72/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0852 - val_loss: 1.1234\n",
      "Epoch 73/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0761 - val_loss: 1.1279\n",
      "Epoch 74/1000\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 1.0863 - val_loss: 1.1257\n",
      "Epoch 75/1000\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 1.0777 - val_loss: 1.1313\n",
      "Epoch 76/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0729 - val_loss: 1.1237\n",
      "Epoch 77/1000\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 1.0618 - val_loss: 1.1231\n",
      "Epoch 78/1000\n",
      "8/8 [==============================] - 0s 25ms/step - loss: 1.0726 - val_loss: 1.1324\n",
      "Epoch 79/1000\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 1.0882 - val_loss: 1.1226\n",
      "Epoch 80/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0678 - val_loss: 1.1263\n",
      "Epoch 81/1000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 1.0866 - val_loss: 1.1243\n",
      "Epoch 82/1000\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 1.0977 - val_loss: 1.1215\n",
      "Epoch 83/1000\n",
      "8/8 [==============================] - 0s 24ms/step - loss: 1.0809 - val_loss: 1.1275\n",
      "Epoch 84/1000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 1.0721 - val_loss: 1.1308\n",
      "Epoch 85/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0767 - val_loss: 1.1194\n",
      "Epoch 86/1000\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 1.0758 - val_loss: 1.1270\n",
      "Epoch 87/1000\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 1.0671 - val_loss: 1.1248\n",
      "Epoch 88/1000\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 1.0813 - val_loss: 1.1168\n",
      "Epoch 89/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0797 - val_loss: 1.1310\n",
      "Epoch 90/1000\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 1.0787 - val_loss: 1.1289\n",
      "Epoch 91/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0751 - val_loss: 1.1214\n",
      "Epoch 92/1000\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 1.0850 - val_loss: 1.1202\n",
      "Epoch 93/1000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 1.0793 - val_loss: 1.1052\n",
      "Epoch 94/1000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 1.0921 - val_loss: 1.1205\n",
      "Epoch 95/1000\n",
      "8/8 [==============================] - 0s 59ms/step - loss: 1.0713 - val_loss: 1.1248\n",
      "Epoch 96/1000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 1.0754 - val_loss: 1.1210\n",
      "Epoch 97/1000\n",
      "8/8 [==============================] - 0s 24ms/step - loss: 1.0656 - val_loss: 1.1150\n",
      "Epoch 98/1000\n",
      "8/8 [==============================] - 0s 25ms/step - loss: 1.0828 - val_loss: 1.1254\n",
      "Epoch 99/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0801 - val_loss: 1.1101\n",
      "Epoch 100/1000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 1.0792 - val_loss: 1.1221\n",
      "Epoch 101/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0620 - val_loss: 1.1186\n",
      "Epoch 102/1000\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 1.0788 - val_loss: 1.1214\n",
      "Epoch 103/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0802 - val_loss: 1.1228\n",
      "Epoch 104/1000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 1.0820 - val_loss: 1.1218\n",
      "Epoch 105/1000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 1.0844 - val_loss: 1.1253\n",
      "Epoch 106/1000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 1.0849 - val_loss: 1.1214\n",
      "Epoch 107/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0700 - val_loss: 1.1251\n",
      "Epoch 108/1000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 1.0726 - val_loss: 1.1328\n",
      "Epoch 109/1000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 1.0778 - val_loss: 1.1213\n",
      "Epoch 110/1000\n",
      "8/8 [==============================] - 0s 37ms/step - loss: 1.0797 - val_loss: 1.1234\n",
      "Epoch 111/1000\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 1.0686 - val_loss: 1.1155\n",
      "Epoch 112/1000\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 1.0675 - val_loss: 1.1329\n",
      "Epoch 113/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0895 - val_loss: 1.1160\n",
      "Epoch 114/1000\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 1.0672 - val_loss: 1.1216\n",
      "Epoch 115/1000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.0731 - val_loss: 1.1071\n",
      "Epoch 116/1000\n",
      "8/8 [==============================] - 0s 38ms/step - loss: 1.0784 - val_loss: 1.1165\n",
      "Epoch 117/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0813 - val_loss: 1.1140\n",
      "Epoch 118/1000\n",
      "8/8 [==============================] - 0s 25ms/step - loss: 1.0796 - val_loss: 1.1254\n",
      "Epoch 119/1000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.0847 - val_loss: 1.1229\n",
      "Epoch 120/1000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 1.0837 - val_loss: 1.1192\n",
      "Epoch 121/1000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 1.0815 - val_loss: 1.1339\n",
      "Epoch 122/1000\n",
      "8/8 [==============================] - 0s 37ms/step - loss: 1.0805 - val_loss: 1.1266\n",
      "Epoch 123/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0765 - val_loss: 1.1175\n",
      "Epoch 124/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0789 - val_loss: 1.1248\n",
      "Epoch 125/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0922 - val_loss: 1.1210\n",
      "Epoch 126/1000\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 1.0711 - val_loss: 1.1240\n",
      "Epoch 127/1000\n",
      "8/8 [==============================] - 0s 37ms/step - loss: 1.0838 - val_loss: 1.1324\n",
      "Epoch 128/1000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 1.0670 - val_loss: 1.1249\n",
      "Epoch 129/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0749 - val_loss: 1.1175\n",
      "Epoch 130/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0859 - val_loss: 1.1209\n",
      "Epoch 131/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0764 - val_loss: 1.1227\n",
      "Epoch 132/1000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 1.0813 - val_loss: 1.1283\n",
      "Epoch 133/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0783 - val_loss: 1.1282\n",
      "Epoch 134/1000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.0823 - val_loss: 1.1133\n",
      "Epoch 135/1000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.0770 - val_loss: 1.1233\n",
      "Epoch 136/1000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.0744 - val_loss: 1.1169\n",
      "Epoch 137/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0736 - val_loss: 1.1183\n",
      "Epoch 138/1000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.0861 - val_loss: 1.1283\n",
      "Epoch 139/1000\n",
      "8/8 [==============================] - 0s 37ms/step - loss: 1.0793 - val_loss: 1.1279\n",
      "Epoch 140/1000\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 1.0843 - val_loss: 1.1156\n",
      "Epoch 141/1000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 1.0735 - val_loss: 1.1208\n",
      "Epoch 142/1000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.0733 - val_loss: 1.1181\n",
      "Epoch 143/1000\n",
      "8/8 [==============================] - 0s 39ms/step - loss: 1.0739 - val_loss: 1.1179\n",
      "Epoch 144/1000\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 1.0705 - val_loss: 1.1164\n",
      "Epoch 145/1000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.0702 - val_loss: 1.1221\n",
      "Epoch 146/1000\n",
      "8/8 [==============================] - 0s 37ms/step - loss: 1.0719 - val_loss: 1.1227\n",
      "Epoch 147/1000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.0815 - val_loss: 1.1150\n",
      "Epoch 148/1000\n",
      "8/8 [==============================] - 0s 36ms/step - loss: 1.0852 - val_loss: 1.1123\n",
      "Epoch 149/1000\n",
      "8/8 [==============================] - 0s 25ms/step - loss: 1.0669 - val_loss: 1.1246\n",
      "Epoch 150/1000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 1.0875 - val_loss: 1.1170\n",
      "Epoch 151/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0715 - val_loss: 1.1193\n",
      "Epoch 152/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0756 - val_loss: 1.1165\n",
      "Epoch 153/1000\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 1.0860 - val_loss: 1.1132\n",
      "Epoch 154/1000\n",
      "8/8 [==============================] - 0s 36ms/step - loss: 1.0903 - val_loss: 1.1144\n",
      "Epoch 155/1000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 1.0824 - val_loss: 1.1185\n",
      "Epoch 156/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0832 - val_loss: 1.1198\n",
      "Epoch 157/1000\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 1.0780 - val_loss: 1.1172\n",
      "Epoch 158/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0871 - val_loss: 1.1178\n",
      "Epoch 159/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0713 - val_loss: 1.1229\n",
      "Epoch 160/1000\n",
      "8/8 [==============================] - 0s 25ms/step - loss: 1.0843 - val_loss: 1.1169\n",
      "Epoch 161/1000\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 1.0786 - val_loss: 1.1220\n",
      "Epoch 162/1000\n",
      "8/8 [==============================] - 0s 24ms/step - loss: 1.0809 - val_loss: 1.1302\n",
      "Epoch 163/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0781 - val_loss: 1.1143\n",
      "Epoch 164/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0671 - val_loss: 1.1184\n",
      "Epoch 165/1000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 1.0679 - val_loss: 1.1183\n",
      "Epoch 166/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0708 - val_loss: 1.1188\n",
      "Epoch 167/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0873 - val_loss: 1.1179\n",
      "Epoch 168/1000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 1.0705 - val_loss: 1.1151\n",
      "Epoch 169/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0671 - val_loss: 1.1154\n",
      "Epoch 170/1000\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 1.0764 - val_loss: 1.1274\n",
      "Epoch 171/1000\n",
      "8/8 [==============================] - 0s 25ms/step - loss: 1.0723 - val_loss: 1.1254\n",
      "Epoch 172/1000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.0787 - val_loss: 1.1180\n",
      "Epoch 173/1000\n",
      "8/8 [==============================] - 0s 25ms/step - loss: 1.0830 - val_loss: 1.1278\n",
      "Epoch 174/1000\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 1.0746 - val_loss: 1.1236\n",
      "Epoch 175/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0782 - val_loss: 1.1162\n",
      "Epoch 176/1000\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 1.0721 - val_loss: 1.1220\n",
      "Epoch 177/1000\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 1.0915 - val_loss: 1.1260\n",
      "Epoch 178/1000\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 1.0752 - val_loss: 1.1227\n",
      "Epoch 179/1000\n",
      "8/8 [==============================] - 0s 25ms/step - loss: 1.0795 - val_loss: 1.1280\n",
      "Epoch 180/1000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 1.0752 - val_loss: 1.1285\n",
      "Epoch 181/1000\n",
      "8/8 [==============================] - 0s 25ms/step - loss: 1.0879 - val_loss: 1.1209\n",
      "Epoch 182/1000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 1.0730 - val_loss: 1.1272\n",
      "Epoch 183/1000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 1.0803 - val_loss: 1.1196\n",
      "Epoch 184/1000\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 1.0697 - val_loss: 1.1188\n",
      "Epoch 185/1000\n",
      "8/8 [==============================] - 0s 36ms/step - loss: 1.0743 - val_loss: 1.1280\n",
      "Epoch 186/1000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.0814 - val_loss: 1.1223\n",
      "Epoch 187/1000\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 1.0708 - val_loss: 1.1171\n",
      "Epoch 188/1000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 1.0678 - val_loss: 1.1167\n",
      "Epoch 189/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0743 - val_loss: 1.1163\n",
      "Epoch 190/1000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 1.0872 - val_loss: 1.1213\n",
      "Epoch 191/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0722 - val_loss: 1.1141\n",
      "Epoch 192/1000\n",
      "8/8 [==============================] - 0s 25ms/step - loss: 1.0779 - val_loss: 1.1198\n",
      "Epoch 193/1000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 1.0835 - val_loss: 1.1241\n",
      "Epoch 194/1000\n",
      "8/8 [==============================] - 0s 25ms/step - loss: 1.0723 - val_loss: 1.1269\n",
      "Epoch 195/1000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 1.0791 - val_loss: 1.1326\n",
      "Epoch 196/1000\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 1.0723 - val_loss: 1.1223\n",
      "Epoch 197/1000\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 1.0814 - val_loss: 1.1199\n",
      "Epoch 198/1000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.0740 - val_loss: 1.1167\n",
      "Epoch 199/1000\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 1.0741 - val_loss: 1.1214\n",
      "Epoch 200/1000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.0870 - val_loss: 1.1152\n",
      "Epoch 201/1000\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 1.0845 - val_loss: 1.1126\n",
      "Epoch 202/1000\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 1.0786 - val_loss: 1.1259\n",
      "Epoch 203/1000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 1.0742 - val_loss: 1.1179\n",
      "Epoch 204/1000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 1.0799 - val_loss: 1.1114\n",
      "Epoch 205/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0796 - val_loss: 1.1311\n",
      "Epoch 206/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0818 - val_loss: 1.1306\n",
      "Epoch 207/1000\n",
      "8/8 [==============================] - 0s 36ms/step - loss: 1.0772 - val_loss: 1.1126\n",
      "Epoch 208/1000\n",
      "8/8 [==============================] - 0s 23ms/step - loss: 1.0813 - val_loss: 1.1184\n",
      "Epoch 209/1000\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 1.0751 - val_loss: 1.1175\n",
      "Epoch 210/1000\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 1.0857 - val_loss: 1.1187\n",
      "Epoch 211/1000\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 1.0717 - val_loss: 1.1122\n",
      "Epoch 212/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0735 - val_loss: 1.1195\n",
      "Epoch 213/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0775 - val_loss: 1.1291\n",
      "Epoch 214/1000\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 1.0725 - val_loss: 1.1201\n",
      "Epoch 215/1000\n",
      "8/8 [==============================] - 0s 25ms/step - loss: 1.0750 - val_loss: 1.1224\n",
      "Epoch 216/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0714 - val_loss: 1.1177\n",
      "Epoch 217/1000\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 1.0713 - val_loss: 1.1195\n",
      "Epoch 218/1000\n",
      "8/8 [==============================] - 0s 36ms/step - loss: 1.0718 - val_loss: 1.1243\n",
      "Epoch 219/1000\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 1.0812 - val_loss: 1.1265\n",
      "Epoch 220/1000\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 1.0780 - val_loss: 1.1234\n",
      "Epoch 221/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0719 - val_loss: 1.1211\n",
      "Epoch 222/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0852 - val_loss: 1.1158\n",
      "Epoch 223/1000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.0842 - val_loss: 1.1167\n",
      "Epoch 224/1000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 1.0760 - val_loss: 1.1083\n",
      "Epoch 225/1000\n",
      "8/8 [==============================] - 0s 25ms/step - loss: 1.0757 - val_loss: 1.1222\n",
      "Epoch 226/1000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.0836 - val_loss: 1.1161\n",
      "Epoch 227/1000\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 1.0742 - val_loss: 1.1094\n",
      "Epoch 228/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0778 - val_loss: 1.1161\n",
      "Epoch 229/1000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.0668 - val_loss: 1.1194\n",
      "Epoch 230/1000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 1.0727 - val_loss: 1.1221\n",
      "Epoch 231/1000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 1.0929 - val_loss: 1.1241\n",
      "Epoch 232/1000\n",
      "8/8 [==============================] - 0s 52ms/step - loss: 1.0774 - val_loss: 1.1170\n",
      "Epoch 233/1000\n",
      "8/8 [==============================] - 0s 43ms/step - loss: 1.0704 - val_loss: 1.1136\n",
      "Epoch 234/1000\n",
      "8/8 [==============================] - 0s 37ms/step - loss: 1.0735 - val_loss: 1.1183\n",
      "Epoch 235/1000\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 1.0791 - val_loss: 1.1242\n",
      "Epoch 236/1000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 1.0752 - val_loss: 1.1157\n",
      "Epoch 237/1000\n",
      "8/8 [==============================] - 0s 38ms/step - loss: 1.0693 - val_loss: 1.1255\n",
      "Epoch 238/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0934 - val_loss: 1.1256\n",
      "Epoch 239/1000\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 1.0776 - val_loss: 1.1267\n",
      "Epoch 240/1000\n",
      "8/8 [==============================] - 0s 24ms/step - loss: 1.0775 - val_loss: 1.1093\n",
      "Epoch 241/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0718 - val_loss: 1.1149\n",
      "Epoch 242/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0831 - val_loss: 1.1167\n",
      "Epoch 243/1000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 1.0645 - val_loss: 1.1249\n",
      "Epoch 244/1000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.0887 - val_loss: 1.1149\n",
      "Epoch 245/1000\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 1.0715 - val_loss: 1.1301\n",
      "Epoch 246/1000\n",
      "8/8 [==============================] - 0s 36ms/step - loss: 1.0743 - val_loss: 1.1204\n",
      "Epoch 247/1000\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 1.0687 - val_loss: 1.1279\n",
      "Epoch 248/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0732 - val_loss: 1.1285\n",
      "Epoch 249/1000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 1.0741 - val_loss: 1.1185\n",
      "Epoch 250/1000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 1.0757 - val_loss: 1.1273\n",
      "Epoch 251/1000\n",
      "8/8 [==============================] - 0s 36ms/step - loss: 1.0682 - val_loss: 1.1230\n",
      "Epoch 252/1000\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 1.0777 - val_loss: 1.1264\n",
      "Epoch 253/1000\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 1.0863 - val_loss: 1.1175\n",
      "Epoch 254/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0669 - val_loss: 1.1177\n",
      "Epoch 255/1000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.0688 - val_loss: 1.1170\n",
      "Epoch 256/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0783 - val_loss: 1.1229\n",
      "Epoch 257/1000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 1.0786 - val_loss: 1.1114\n",
      "Epoch 258/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0787 - val_loss: 1.1229\n",
      "Epoch 259/1000\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 1.0616 - val_loss: 1.1141\n",
      "Epoch 260/1000\n",
      "8/8 [==============================] - 0s 25ms/step - loss: 1.0832 - val_loss: 1.1213\n",
      "Epoch 261/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0808 - val_loss: 1.1178\n",
      "Epoch 262/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0791 - val_loss: 1.1130\n",
      "Epoch 263/1000\n",
      "8/8 [==============================] - 0s 37ms/step - loss: 1.0855 - val_loss: 1.1397\n",
      "Epoch 264/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0860 - val_loss: 1.1162\n",
      "Epoch 265/1000\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 1.0811 - val_loss: 1.1197\n",
      "Epoch 266/1000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.0752 - val_loss: 1.1259\n",
      "Epoch 267/1000\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 1.0775 - val_loss: 1.1204\n",
      "Epoch 268/1000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 1.0770 - val_loss: 1.1289\n",
      "Epoch 269/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0760 - val_loss: 1.1296\n",
      "Epoch 270/1000\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 1.0696 - val_loss: 1.1198\n",
      "Epoch 271/1000\n",
      "8/8 [==============================] - 0s 36ms/step - loss: 1.0875 - val_loss: 1.1196\n",
      "Epoch 272/1000\n",
      "8/8 [==============================] - 0s 25ms/step - loss: 1.0823 - val_loss: 1.1162\n",
      "Epoch 273/1000\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 1.0768 - val_loss: 1.1205\n",
      "Epoch 274/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0757 - val_loss: 1.1228\n",
      "Epoch 275/1000\n",
      "8/8 [==============================] - 0s 25ms/step - loss: 1.0684 - val_loss: 1.1155\n",
      "Epoch 276/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0744 - val_loss: 1.1252\n",
      "Epoch 277/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0784 - val_loss: 1.1164\n",
      "Epoch 278/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0830 - val_loss: 1.1242\n",
      "Epoch 279/1000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 1.0686 - val_loss: 1.1187\n",
      "Epoch 280/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0771 - val_loss: 1.1182\n",
      "Epoch 281/1000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.0727 - val_loss: 1.1191\n",
      "Epoch 282/1000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 1.0856 - val_loss: 1.1200\n",
      "Epoch 283/1000\n",
      "8/8 [==============================] - 0s 38ms/step - loss: 1.0947 - val_loss: 1.1137\n",
      "Epoch 284/1000\n",
      "8/8 [==============================] - 0s 20ms/step - loss: 1.0847 - val_loss: 1.1131\n",
      "Epoch 285/1000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.0791 - val_loss: 1.1236\n",
      "Epoch 286/1000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.0665 - val_loss: 1.1270\n",
      "Epoch 287/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0861 - val_loss: 1.1155\n",
      "Epoch 288/1000\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 1.0799 - val_loss: 1.1100\n",
      "Epoch 289/1000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 1.0697 - val_loss: 1.1250\n",
      "Epoch 290/1000\n",
      "8/8 [==============================] - 0s 37ms/step - loss: 1.0769 - val_loss: 1.1283\n",
      "Epoch 291/1000\n",
      "8/8 [==============================] - 0s 36ms/step - loss: 1.0771 - val_loss: 1.1193\n",
      "Epoch 292/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0699 - val_loss: 1.1183\n",
      "Epoch 293/1000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 1.0722 - val_loss: 1.1223\n",
      "Epoch 294/1000\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 1.0818 - val_loss: 1.1192\n",
      "Epoch 295/1000\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 1.0899 - val_loss: 1.1157\n",
      "Epoch 296/1000\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 1.0752 - val_loss: 1.1179\n",
      "Epoch 297/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0760 - val_loss: 1.1197\n",
      "Epoch 298/1000\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 1.0721 - val_loss: 1.1200\n",
      "Epoch 299/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0713 - val_loss: 1.1184\n",
      "Epoch 300/1000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 1.0706 - val_loss: 1.1253\n",
      "Epoch 301/1000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.0773 - val_loss: 1.1236\n",
      "Epoch 302/1000\n",
      "8/8 [==============================] - 0s 39ms/step - loss: 1.0674 - val_loss: 1.1050\n",
      "Epoch 303/1000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.0781 - val_loss: 1.1215\n",
      "Epoch 304/1000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.0754 - val_loss: 1.1240\n",
      "Epoch 305/1000\n",
      "8/8 [==============================] - 0s 36ms/step - loss: 1.0815 - val_loss: 1.1231\n",
      "Epoch 306/1000\n",
      "8/8 [==============================] - 0s 36ms/step - loss: 1.0777 - val_loss: 1.1213\n",
      "Epoch 307/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0767 - val_loss: 1.1181\n",
      "Epoch 308/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0704 - val_loss: 1.1192\n",
      "Epoch 309/1000\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 1.0769 - val_loss: 1.1291\n",
      "Epoch 310/1000\n",
      "8/8 [==============================] - 0s 36ms/step - loss: 1.0668 - val_loss: 1.1221\n",
      "Epoch 311/1000\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 1.0741 - val_loss: 1.1161\n",
      "Epoch 312/1000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 1.0795 - val_loss: 1.1266\n",
      "Epoch 313/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0660 - val_loss: 1.1182\n",
      "Epoch 314/1000\n",
      "8/8 [==============================] - 0s 25ms/step - loss: 1.0790 - val_loss: 1.1229\n",
      "Epoch 315/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0692 - val_loss: 1.1145\n",
      "Epoch 316/1000\n",
      "8/8 [==============================] - 0s 25ms/step - loss: 1.0914 - val_loss: 1.1214\n",
      "Epoch 317/1000\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 1.0802 - val_loss: 1.1165\n",
      "Epoch 318/1000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.0776 - val_loss: 1.1243\n",
      "Epoch 319/1000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 1.0744 - val_loss: 1.1197\n",
      "Epoch 320/1000\n",
      "8/8 [==============================] - 0s 25ms/step - loss: 1.0743 - val_loss: 1.1080\n",
      "Epoch 321/1000\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 1.0710 - val_loss: 1.1225\n",
      "Epoch 322/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0764 - val_loss: 1.1106\n",
      "Epoch 323/1000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 1.0724 - val_loss: 1.1101\n",
      "Epoch 324/1000\n",
      "8/8 [==============================] - 0s 25ms/step - loss: 1.0717 - val_loss: 1.1291\n",
      "Epoch 325/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0773 - val_loss: 1.1233\n",
      "Epoch 326/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0752 - val_loss: 1.1215\n",
      "Epoch 327/1000\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 1.0812 - val_loss: 1.1186\n",
      "Epoch 328/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0738 - val_loss: 1.1179\n",
      "Epoch 329/1000\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 1.0731 - val_loss: 1.1154\n",
      "Epoch 330/1000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 1.0939 - val_loss: 1.1194\n",
      "Epoch 331/1000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 1.0781 - val_loss: 1.1070\n",
      "Epoch 332/1000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 1.0721 - val_loss: 1.1231\n",
      "Epoch 333/1000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 1.0664 - val_loss: 1.1118\n",
      "Epoch 334/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0702 - val_loss: 1.1220\n",
      "Epoch 335/1000\n",
      "8/8 [==============================] - 0s 24ms/step - loss: 1.0730 - val_loss: 1.1186\n",
      "Epoch 336/1000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.0784 - val_loss: 1.1279\n",
      "Epoch 337/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0788 - val_loss: 1.1181\n",
      "Epoch 338/1000\n",
      "8/8 [==============================] - 0s 51ms/step - loss: 1.0687 - val_loss: 1.1201\n",
      "Epoch 339/1000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 1.0881 - val_loss: 1.1196\n",
      "Epoch 340/1000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 1.0744 - val_loss: 1.1142\n",
      "Epoch 341/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0823 - val_loss: 1.1185\n",
      "Epoch 342/1000\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 1.0767 - val_loss: 1.1189\n",
      "Epoch 343/1000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 1.0708 - val_loss: 1.1133\n",
      "Epoch 344/1000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.0867 - val_loss: 1.1289\n",
      "Epoch 345/1000\n",
      "8/8 [==============================] - 0s 25ms/step - loss: 1.0689 - val_loss: 1.1147\n",
      "Epoch 346/1000\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 1.0752 - val_loss: 1.1206\n",
      "Epoch 347/1000\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 1.0802 - val_loss: 1.1191\n",
      "Epoch 348/1000\n",
      "8/8 [==============================] - 0s 25ms/step - loss: 1.0671 - val_loss: 1.1185\n",
      "Epoch 349/1000\n",
      "8/8 [==============================] - 0s 25ms/step - loss: 1.0810 - val_loss: 1.1134\n",
      "Epoch 350/1000\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 1.0894 - val_loss: 1.1201\n",
      "Epoch 351/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0700 - val_loss: 1.1209\n",
      "Epoch 352/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0684 - val_loss: 1.1159\n",
      "Epoch 353/1000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 1.0729 - val_loss: 1.1269\n",
      "Epoch 354/1000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.0758 - val_loss: 1.1139\n",
      "Epoch 355/1000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 1.0847 - val_loss: 1.1130\n",
      "Epoch 356/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0715 - val_loss: 1.1221\n",
      "Epoch 357/1000\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 1.0670 - val_loss: 1.1284\n",
      "Epoch 358/1000\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 1.0796 - val_loss: 1.1170\n",
      "Epoch 359/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0725 - val_loss: 1.1213\n",
      "Epoch 360/1000\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 1.0836 - val_loss: 1.1273\n",
      "Epoch 361/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0760 - val_loss: 1.1144\n",
      "Epoch 362/1000\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 1.0694 - val_loss: 1.1113\n",
      "Epoch 363/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0723 - val_loss: 1.1178\n",
      "Epoch 364/1000\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 1.0675 - val_loss: 1.1132\n",
      "Epoch 365/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0753 - val_loss: 1.1153\n",
      "Epoch 366/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0746 - val_loss: 1.1260\n",
      "Epoch 367/1000\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 1.0732 - val_loss: 1.1164\n",
      "Epoch 368/1000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.0804 - val_loss: 1.1178\n",
      "Epoch 369/1000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 1.0748 - val_loss: 1.1135\n",
      "Epoch 370/1000\n",
      "8/8 [==============================] - 0s 38ms/step - loss: 1.0743 - val_loss: 1.1113\n",
      "Epoch 371/1000\n",
      "8/8 [==============================] - 0s 42ms/step - loss: 1.0717 - val_loss: 1.1249\n",
      "Epoch 372/1000\n",
      "8/8 [==============================] - 0s 38ms/step - loss: 1.0706 - val_loss: 1.1136\n",
      "Epoch 373/1000\n",
      "8/8 [==============================] - 0s 38ms/step - loss: 1.0747 - val_loss: 1.1145\n",
      "Epoch 374/1000\n",
      "8/8 [==============================] - 0s 48ms/step - loss: 1.0744 - val_loss: 1.1259\n",
      "Epoch 375/1000\n",
      "8/8 [==============================] - 0s 65ms/step - loss: 1.0682 - val_loss: 1.1111\n",
      "Epoch 376/1000\n",
      "8/8 [==============================] - 0s 54ms/step - loss: 1.0812 - val_loss: 1.1154\n",
      "Epoch 377/1000\n",
      "8/8 [==============================] - 0s 66ms/step - loss: 1.0793 - val_loss: 1.1211\n",
      "Epoch 378/1000\n",
      "8/8 [==============================] - 0s 59ms/step - loss: 1.0751 - val_loss: 1.1199\n",
      "Epoch 379/1000\n",
      "8/8 [==============================] - 0s 54ms/step - loss: 1.0767 - val_loss: 1.1239\n",
      "Epoch 380/1000\n",
      "8/8 [==============================] - 0s 55ms/step - loss: 1.0717 - val_loss: 1.1221\n",
      "Epoch 381/1000\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 1.0689 - val_loss: 1.1143\n",
      "Epoch 382/1000\n",
      "8/8 [==============================] - 0s 56ms/step - loss: 1.0879 - val_loss: 1.1256\n",
      "Epoch 383/1000\n",
      "8/8 [==============================] - 0s 59ms/step - loss: 1.0723 - val_loss: 1.1157\n",
      "Epoch 384/1000\n",
      "8/8 [==============================] - 0s 57ms/step - loss: 1.0715 - val_loss: 1.1244\n",
      "Epoch 385/1000\n",
      "8/8 [==============================] - 0s 49ms/step - loss: 1.0643 - val_loss: 1.1245\n",
      "Epoch 386/1000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 1.0647 - val_loss: 1.1187\n",
      "Epoch 387/1000\n",
      "8/8 [==============================] - 0s 38ms/step - loss: 1.0794 - val_loss: 1.1174\n",
      "Epoch 388/1000\n",
      "8/8 [==============================] - 0s 36ms/step - loss: 1.0777 - val_loss: 1.1039\n",
      "Epoch 389/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0709 - val_loss: 1.1141\n",
      "Epoch 390/1000\n",
      "8/8 [==============================] - 0s 24ms/step - loss: 1.0747 - val_loss: 1.1170\n",
      "Epoch 391/1000\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 1.0720 - val_loss: 1.1257\n",
      "Epoch 392/1000\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 1.0782 - val_loss: 1.1178\n",
      "Epoch 393/1000\n",
      "8/8 [==============================] - 0s 25ms/step - loss: 1.0675 - val_loss: 1.1089\n",
      "Epoch 394/1000\n",
      "8/8 [==============================] - 0s 25ms/step - loss: 1.0781 - val_loss: 1.1228\n",
      "Epoch 395/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0664 - val_loss: 1.1249\n",
      "Epoch 396/1000\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 1.0773 - val_loss: 1.1187\n",
      "Epoch 397/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0707 - val_loss: 1.1025\n",
      "Epoch 398/1000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 1.0759 - val_loss: 1.1300\n",
      "Epoch 399/1000\n",
      "8/8 [==============================] - 0s 51ms/step - loss: 1.0758 - val_loss: 1.1131\n",
      "Epoch 400/1000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.0757 - val_loss: 1.1200\n",
      "Epoch 401/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0817 - val_loss: 1.1072\n",
      "Epoch 402/1000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.0737 - val_loss: 1.1149\n",
      "Epoch 403/1000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.0746 - val_loss: 1.1093\n",
      "Epoch 404/1000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 1.0758 - val_loss: 1.1172\n",
      "Epoch 405/1000\n",
      "8/8 [==============================] - 0s 25ms/step - loss: 1.0783 - val_loss: 1.1125\n",
      "Epoch 406/1000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.0752 - val_loss: 1.1128\n",
      "Epoch 407/1000\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 1.0779 - val_loss: 1.1273\n",
      "Epoch 408/1000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 1.0751 - val_loss: 1.1212\n",
      "Epoch 409/1000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.0790 - val_loss: 1.1268\n",
      "Epoch 410/1000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.0775 - val_loss: 1.1158\n",
      "Epoch 411/1000\n",
      "8/8 [==============================] - 0s 37ms/step - loss: 1.0776 - val_loss: 1.1238\n",
      "Epoch 412/1000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.0621 - val_loss: 1.1183\n",
      "Epoch 413/1000\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 1.0696 - val_loss: 1.1139\n",
      "Epoch 414/1000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.0939 - val_loss: 1.1262\n",
      "Epoch 415/1000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.0738 - val_loss: 1.1127\n",
      "Epoch 416/1000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.0767 - val_loss: 1.1285\n",
      "Epoch 417/1000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 1.0705 - val_loss: 1.1108\n",
      "Epoch 418/1000\n",
      "8/8 [==============================] - 0s 25ms/step - loss: 1.0842 - val_loss: 1.1170\n",
      "Epoch 419/1000\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 1.0757 - val_loss: 1.1184\n",
      "Epoch 420/1000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 1.0757 - val_loss: 1.1210\n",
      "Epoch 421/1000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.0823 - val_loss: 1.1122\n",
      "Epoch 422/1000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 1.0773 - val_loss: 1.1225\n",
      "Epoch 423/1000\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 1.0745 - val_loss: 1.1244\n",
      "Epoch 424/1000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 1.0694 - val_loss: 1.1105\n",
      "Epoch 425/1000\n",
      "8/8 [==============================] - 0s 25ms/step - loss: 1.0697 - val_loss: 1.1193\n",
      "Epoch 426/1000\n",
      "8/8 [==============================] - 0s 25ms/step - loss: 1.0672 - val_loss: 1.1178\n",
      "Epoch 427/1000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.0701 - val_loss: 1.1044\n",
      "Epoch 428/1000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.0644 - val_loss: 1.1269\n",
      "Epoch 429/1000\n",
      "8/8 [==============================] - 0s 37ms/step - loss: 1.0716 - val_loss: 1.1096\n",
      "Epoch 430/1000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 1.0786 - val_loss: 1.1298\n",
      "Epoch 431/1000\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 1.0768 - val_loss: 1.1122\n",
      "Epoch 432/1000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.0847 - val_loss: 1.1222\n",
      "Epoch 433/1000\n",
      "8/8 [==============================] - 0s 25ms/step - loss: 1.0700 - val_loss: 1.1156\n",
      "Epoch 434/1000\n",
      "8/8 [==============================] - 0s 24ms/step - loss: 1.0716 - val_loss: 1.1259\n",
      "Epoch 435/1000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.0717 - val_loss: 1.1263\n",
      "Epoch 436/1000\n",
      "8/8 [==============================] - 0s 43ms/step - loss: 1.0787 - val_loss: 1.1203\n",
      "Epoch 437/1000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.0663 - val_loss: 1.1128\n",
      "Epoch 438/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0743 - val_loss: 1.1232\n",
      "Epoch 439/1000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 1.0722 - val_loss: 1.1262\n",
      "Epoch 440/1000\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 1.0758 - val_loss: 1.1189\n",
      "Epoch 441/1000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 1.0752 - val_loss: 1.1184\n",
      "Epoch 442/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0842 - val_loss: 1.1233\n",
      "Epoch 443/1000\n",
      "8/8 [==============================] - 0s 36ms/step - loss: 1.0752 - val_loss: 1.1202\n",
      "Epoch 444/1000\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 1.0637 - val_loss: 1.1194\n",
      "Epoch 445/1000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 1.0743 - val_loss: 1.1147\n",
      "Epoch 446/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0674 - val_loss: 1.1195\n",
      "Epoch 447/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0757 - val_loss: 1.1194\n",
      "Epoch 448/1000\n",
      "8/8 [==============================] - 0s 36ms/step - loss: 1.0806 - val_loss: 1.1118\n",
      "Epoch 449/1000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 1.0676 - val_loss: 1.1186\n",
      "Epoch 450/1000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.0691 - val_loss: 1.1191\n",
      "Epoch 451/1000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.0680 - val_loss: 1.1156\n",
      "Epoch 452/1000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 1.0738 - val_loss: 1.1235\n",
      "Epoch 453/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0704 - val_loss: 1.1138\n",
      "Epoch 454/1000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.0762 - val_loss: 1.1145\n",
      "Epoch 455/1000\n",
      "8/8 [==============================] - 0s 25ms/step - loss: 1.0829 - val_loss: 1.1211\n",
      "Epoch 456/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0841 - val_loss: 1.1171\n",
      "Epoch 457/1000\n",
      "8/8 [==============================] - 0s 25ms/step - loss: 1.0680 - val_loss: 1.1208\n",
      "Epoch 458/1000\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 1.0896 - val_loss: 1.1112\n",
      "Epoch 459/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0709 - val_loss: 1.1138\n",
      "Epoch 460/1000\n",
      "8/8 [==============================] - 0s 25ms/step - loss: 1.0710 - val_loss: 1.1066\n",
      "Epoch 461/1000\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 1.0715 - val_loss: 1.1226\n",
      "Epoch 462/1000\n",
      "8/8 [==============================] - 0s 25ms/step - loss: 1.0762 - val_loss: 1.1195\n",
      "Epoch 463/1000\n",
      "8/8 [==============================] - 0s 20ms/step - loss: 1.0813 - val_loss: 1.1126\n",
      "Epoch 464/1000\n",
      "8/8 [==============================] - 0s 25ms/step - loss: 1.0827 - val_loss: 1.1189\n",
      "Epoch 465/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0747 - val_loss: 1.1108\n",
      "Epoch 466/1000\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 1.0692 - val_loss: 1.1278\n",
      "Epoch 467/1000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.0757 - val_loss: 1.1227\n",
      "Epoch 468/1000\n",
      "8/8 [==============================] - 0s 39ms/step - loss: 1.0690 - val_loss: 1.1114\n",
      "Epoch 469/1000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 1.0711 - val_loss: 1.1123\n",
      "Epoch 470/1000\n",
      "8/8 [==============================] - 0s 36ms/step - loss: 1.0687 - val_loss: 1.1194\n",
      "Epoch 471/1000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 1.0691 - val_loss: 1.1142\n",
      "Epoch 472/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0782 - val_loss: 1.1136\n",
      "Epoch 473/1000\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 1.0758 - val_loss: 1.1133\n",
      "Epoch 474/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0672 - val_loss: 1.1188\n",
      "Epoch 475/1000\n",
      "8/8 [==============================] - 0s 25ms/step - loss: 1.0739 - val_loss: 1.1232\n",
      "Epoch 476/1000\n",
      "8/8 [==============================] - 0s 36ms/step - loss: 1.0679 - val_loss: 1.1177\n",
      "Epoch 477/1000\n",
      "8/8 [==============================] - 0s 23ms/step - loss: 1.0716 - val_loss: 1.1294\n",
      "Epoch 478/1000\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 1.0702 - val_loss: 1.1210\n",
      "Epoch 479/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0837 - val_loss: 1.1189\n",
      "Epoch 480/1000\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 1.0777 - val_loss: 1.1195\n",
      "Epoch 481/1000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.0785 - val_loss: 1.1170\n",
      "Epoch 482/1000\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 1.0733 - val_loss: 1.1126\n",
      "Epoch 483/1000\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 1.0751 - val_loss: 1.1232\n",
      "Epoch 484/1000\n",
      "8/8 [==============================] - 0s 25ms/step - loss: 1.0779 - val_loss: 1.1160\n",
      "Epoch 485/1000\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 1.0724 - val_loss: 1.1074\n",
      "Epoch 486/1000\n",
      "8/8 [==============================] - 0s 25ms/step - loss: 1.0829 - val_loss: 1.1230\n",
      "Epoch 487/1000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 1.0713 - val_loss: 1.1241\n",
      "Epoch 488/1000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.0612 - val_loss: 1.1150\n",
      "Epoch 489/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0641 - val_loss: 1.1273\n",
      "Epoch 490/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0737 - val_loss: 1.1252\n",
      "Epoch 491/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0792 - val_loss: 1.1168\n",
      "Epoch 492/1000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 1.0632 - val_loss: 1.1104\n",
      "Epoch 493/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0658 - val_loss: 1.1249\n",
      "Epoch 494/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0686 - val_loss: 1.1140\n",
      "Epoch 495/1000\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 1.0832 - val_loss: 1.1212\n",
      "Epoch 496/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0718 - val_loss: 1.1246\n",
      "Epoch 497/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0808 - val_loss: 1.1117\n",
      "Epoch 498/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0784 - val_loss: 1.1191\n",
      "Epoch 499/1000\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 1.0926 - val_loss: 1.1189\n",
      "Epoch 500/1000\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 1.0742 - val_loss: 1.1233\n",
      "Epoch 501/1000\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 1.0727 - val_loss: 1.1095\n",
      "Epoch 502/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0703 - val_loss: 1.1230\n",
      "Epoch 503/1000\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 1.0759 - val_loss: 1.1193\n",
      "Epoch 504/1000\n",
      "8/8 [==============================] - 0s 44ms/step - loss: 1.0715 - val_loss: 1.1159\n",
      "Epoch 505/1000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 1.0818 - val_loss: 1.1146\n",
      "Epoch 506/1000\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 1.0669 - val_loss: 1.1222\n",
      "Epoch 507/1000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 1.0745 - val_loss: 1.1259\n",
      "Epoch 508/1000\n",
      "8/8 [==============================] - 0s 36ms/step - loss: 1.0751 - val_loss: 1.1133\n",
      "Epoch 509/1000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 1.0748 - val_loss: 1.1150\n",
      "Epoch 510/1000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.0716 - val_loss: 1.0986\n",
      "Epoch 511/1000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 1.0652 - val_loss: 1.1213\n",
      "Epoch 512/1000\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 1.0783 - val_loss: 1.1215\n",
      "Epoch 513/1000\n",
      "8/8 [==============================] - 0s 36ms/step - loss: 1.0669 - val_loss: 1.1117\n",
      "Epoch 514/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0703 - val_loss: 1.1199\n",
      "Epoch 515/1000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 1.0766 - val_loss: 1.1250\n",
      "Epoch 516/1000\n",
      "8/8 [==============================] - 0s 22ms/step - loss: 1.0608 - val_loss: 1.1081\n",
      "Epoch 517/1000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 1.0733 - val_loss: 1.1040\n",
      "Epoch 518/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0757 - val_loss: 1.1071\n",
      "Epoch 519/1000\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 1.0826 - val_loss: 1.1291\n",
      "Epoch 520/1000\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 1.0802 - val_loss: 1.1239\n",
      "Epoch 521/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0665 - val_loss: 1.1226\n",
      "Epoch 522/1000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 1.0842 - val_loss: 1.1168\n",
      "Epoch 523/1000\n",
      "8/8 [==============================] - 0s 38ms/step - loss: 1.0805 - val_loss: 1.1223\n",
      "Epoch 524/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0672 - val_loss: 1.1135\n",
      "Epoch 525/1000\n",
      "8/8 [==============================] - 0s 25ms/step - loss: 1.0744 - val_loss: 1.1080\n",
      "Epoch 526/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0802 - val_loss: 1.1155\n",
      "Epoch 527/1000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 1.0743 - val_loss: 1.1188\n",
      "Epoch 528/1000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 1.0723 - val_loss: 1.1143\n",
      "Epoch 529/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0688 - val_loss: 1.1109\n",
      "Epoch 530/1000\n",
      "8/8 [==============================] - 0s 25ms/step - loss: 1.0824 - val_loss: 1.1113\n",
      "Epoch 531/1000\n",
      "8/8 [==============================] - 0s 37ms/step - loss: 1.0706 - val_loss: 1.1204\n",
      "Epoch 532/1000\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 1.0658 - val_loss: 1.1148\n",
      "Epoch 533/1000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 1.0824 - val_loss: 1.1104\n",
      "Epoch 534/1000\n",
      "8/8 [==============================] - 0s 25ms/step - loss: 1.0665 - val_loss: 1.1219\n",
      "Epoch 535/1000\n",
      "8/8 [==============================] - 0s 38ms/step - loss: 1.0633 - val_loss: 1.1169\n",
      "Epoch 536/1000\n",
      "8/8 [==============================] - 0s 25ms/step - loss: 1.0722 - val_loss: 1.1154\n",
      "Epoch 537/1000\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 1.0725 - val_loss: 1.1054\n",
      "Epoch 538/1000\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 1.0576 - val_loss: 1.1187\n",
      "Epoch 539/1000\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 1.0752 - val_loss: 1.1155\n",
      "Epoch 540/1000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.0763 - val_loss: 1.1222\n",
      "Epoch 541/1000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.0801 - val_loss: 1.1166\n",
      "Epoch 542/1000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.0768 - val_loss: 1.1153\n",
      "Epoch 543/1000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.0833 - val_loss: 1.1152\n",
      "Epoch 544/1000\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 1.0725 - val_loss: 1.1125\n",
      "Epoch 545/1000\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 1.0735 - val_loss: 1.1187\n",
      "Epoch 546/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0738 - val_loss: 1.1082\n",
      "Epoch 547/1000\n",
      "8/8 [==============================] - 0s 37ms/step - loss: 1.0796 - val_loss: 1.1186\n",
      "Epoch 548/1000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.0713 - val_loss: 1.1260\n",
      "Epoch 549/1000\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 1.0826 - val_loss: 1.1208\n",
      "Epoch 550/1000\n",
      "8/8 [==============================] - 0s 37ms/step - loss: 1.0670 - val_loss: 1.1148\n",
      "Epoch 551/1000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 1.0786 - val_loss: 1.1131\n",
      "Epoch 552/1000\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 1.0732 - val_loss: 1.1257\n",
      "Epoch 553/1000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.0678 - val_loss: 1.1132\n",
      "Epoch 554/1000\n",
      "8/8 [==============================] - 0s 25ms/step - loss: 1.0732 - val_loss: 1.1146\n",
      "Epoch 555/1000\n",
      "8/8 [==============================] - 0s 25ms/step - loss: 1.0667 - val_loss: 1.1267\n",
      "Epoch 556/1000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 1.0749 - val_loss: 1.1110\n",
      "Epoch 557/1000\n",
      "8/8 [==============================] - 0s 37ms/step - loss: 1.0784 - val_loss: 1.1223\n",
      "Epoch 558/1000\n",
      "8/8 [==============================] - 0s 36ms/step - loss: 1.0772 - val_loss: 1.1250\n",
      "Epoch 559/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0740 - val_loss: 1.1175\n",
      "Epoch 560/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0715 - val_loss: 1.1144\n",
      "Epoch 561/1000\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 1.0658 - val_loss: 1.1108\n",
      "Epoch 562/1000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 1.0682 - val_loss: 1.1092\n",
      "Epoch 563/1000\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 1.0731 - val_loss: 1.1247\n",
      "Epoch 564/1000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 1.0699 - val_loss: 1.1178\n",
      "Epoch 565/1000\n",
      "8/8 [==============================] - 0s 25ms/step - loss: 1.0766 - val_loss: 1.1119\n",
      "Epoch 566/1000\n",
      "8/8 [==============================] - 0s 25ms/step - loss: 1.0820 - val_loss: 1.1282\n",
      "Epoch 567/1000\n",
      "8/8 [==============================] - 0s 25ms/step - loss: 1.0627 - val_loss: 1.1088\n",
      "Epoch 568/1000\n",
      "8/8 [==============================] - 0s 25ms/step - loss: 1.0742 - val_loss: 1.1131\n",
      "Epoch 569/1000\n",
      "8/8 [==============================] - 0s 50ms/step - loss: 1.0765 - val_loss: 1.1210\n",
      "Epoch 570/1000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 1.0749 - val_loss: 1.1216\n",
      "Epoch 571/1000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 1.0741 - val_loss: 1.1087\n",
      "Epoch 572/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0801 - val_loss: 1.1143\n",
      "Epoch 573/1000\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 1.0765 - val_loss: 1.1107\n",
      "Epoch 574/1000\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 1.0734 - val_loss: 1.1032\n",
      "Epoch 575/1000\n",
      "8/8 [==============================] - 0s 37ms/step - loss: 1.0757 - val_loss: 1.1149\n",
      "Epoch 576/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0704 - val_loss: 1.1274\n",
      "Epoch 577/1000\n",
      "8/8 [==============================] - 0s 25ms/step - loss: 1.0769 - val_loss: 1.1131\n",
      "Epoch 578/1000\n",
      "8/8 [==============================] - 0s 24ms/step - loss: 1.0707 - val_loss: 1.1146\n",
      "Epoch 579/1000\n",
      "8/8 [==============================] - 0s 25ms/step - loss: 1.0806 - val_loss: 1.1076\n",
      "Epoch 580/1000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 1.0873 - val_loss: 1.1226\n",
      "Epoch 581/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0596 - val_loss: 1.1150\n",
      "Epoch 582/1000\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 1.0709 - val_loss: 1.1202\n",
      "Epoch 583/1000\n",
      "8/8 [==============================] - 0s 25ms/step - loss: 1.0706 - val_loss: 1.1204\n",
      "Epoch 584/1000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 1.0664 - val_loss: 1.1155\n",
      "Epoch 585/1000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.0718 - val_loss: 1.1107\n",
      "Epoch 586/1000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 1.0693 - val_loss: 1.1061\n",
      "Epoch 587/1000\n",
      "8/8 [==============================] - 0s 25ms/step - loss: 1.0676 - val_loss: 1.1080\n",
      "Epoch 588/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0712 - val_loss: 1.1108\n",
      "Epoch 589/1000\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 1.0709 - val_loss: 1.1180\n",
      "Epoch 590/1000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 1.0860 - val_loss: 1.1091\n",
      "Epoch 591/1000\n",
      "8/8 [==============================] - 0s 25ms/step - loss: 1.0729 - val_loss: 1.1121\n",
      "Epoch 592/1000\n",
      "8/8 [==============================] - 0s 20ms/step - loss: 1.0620 - val_loss: 1.1091\n",
      "Epoch 593/1000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 1.0739 - val_loss: 1.1161\n",
      "Epoch 594/1000\n",
      "8/8 [==============================] - 0s 38ms/step - loss: 1.0800 - val_loss: 1.1083\n",
      "Epoch 595/1000\n",
      "8/8 [==============================] - 0s 44ms/step - loss: 1.0754 - val_loss: 1.1222\n",
      "Epoch 596/1000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 1.0742 - val_loss: 1.1228\n",
      "Epoch 597/1000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 1.0742 - val_loss: 1.1175\n",
      "Epoch 598/1000\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 1.0729 - val_loss: 1.1332\n",
      "Epoch 599/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0761 - val_loss: 1.1167\n",
      "Epoch 600/1000\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 1.0752 - val_loss: 1.1163\n",
      "Epoch 601/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0790 - val_loss: 1.1136\n",
      "Epoch 602/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0692 - val_loss: 1.1099\n",
      "Epoch 603/1000\n",
      "8/8 [==============================] - 0s 36ms/step - loss: 1.0709 - val_loss: 1.1190\n",
      "Epoch 604/1000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.0707 - val_loss: 1.1089\n",
      "Epoch 605/1000\n",
      "8/8 [==============================] - 0s 25ms/step - loss: 1.0678 - val_loss: 1.1214\n",
      "Epoch 606/1000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 1.0811 - val_loss: 1.1024\n",
      "Epoch 607/1000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.0760 - val_loss: 1.1195\n",
      "Epoch 608/1000\n",
      "8/8 [==============================] - 0s 36ms/step - loss: 1.0828 - val_loss: 1.1174\n",
      "Epoch 609/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0637 - val_loss: 1.1206\n",
      "Epoch 610/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0837 - val_loss: 1.1162\n",
      "Epoch 611/1000\n",
      "8/8 [==============================] - 0s 25ms/step - loss: 1.0735 - val_loss: 1.1153\n",
      "Epoch 612/1000\n",
      "8/8 [==============================] - 0s 23ms/step - loss: 1.0687 - val_loss: 1.1186\n",
      "Epoch 613/1000\n",
      "8/8 [==============================] - 0s 25ms/step - loss: 1.0699 - val_loss: 1.1122\n",
      "Epoch 614/1000\n",
      "8/8 [==============================] - 0s 25ms/step - loss: 1.0694 - val_loss: 1.1122\n",
      "Epoch 615/1000\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 1.0764 - val_loss: 1.1150\n",
      "Epoch 616/1000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 1.0713 - val_loss: 1.1134\n",
      "Epoch 617/1000\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 1.0690 - val_loss: 1.1205\n",
      "Epoch 618/1000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.0693 - val_loss: 1.1168\n",
      "Epoch 619/1000\n",
      "8/8 [==============================] - 0s 37ms/step - loss: 1.0824 - val_loss: 1.1072\n",
      "Epoch 620/1000\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 1.0638 - val_loss: 1.1162\n",
      "Epoch 621/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0808 - val_loss: 1.1180\n",
      "Epoch 622/1000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.0757 - val_loss: 1.1090\n",
      "Epoch 623/1000\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 1.0733 - val_loss: 1.1089\n",
      "Epoch 624/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0747 - val_loss: 1.1167\n",
      "Epoch 625/1000\n",
      "8/8 [==============================] - 0s 37ms/step - loss: 1.0701 - val_loss: 1.1200\n",
      "Epoch 626/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0832 - val_loss: 1.1174\n",
      "Epoch 627/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0757 - val_loss: 1.1125\n",
      "Epoch 628/1000\n",
      "8/8 [==============================] - 0s 36ms/step - loss: 1.0696 - val_loss: 1.1193\n",
      "Epoch 629/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0751 - val_loss: 1.1166\n",
      "Epoch 630/1000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 1.0702 - val_loss: 1.1166\n",
      "Epoch 631/1000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 1.0739 - val_loss: 1.1133\n",
      "Epoch 632/1000\n",
      "8/8 [==============================] - 0s 25ms/step - loss: 1.0762 - val_loss: 1.1109\n",
      "Epoch 633/1000\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 1.0629 - val_loss: 1.1190\n",
      "Epoch 634/1000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 1.0719 - val_loss: 1.1234\n",
      "Epoch 635/1000\n",
      "8/8 [==============================] - 0s 36ms/step - loss: 1.0732 - val_loss: 1.1136\n",
      "Epoch 636/1000\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 1.0704 - val_loss: 1.1100\n",
      "Epoch 637/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0760 - val_loss: 1.1228\n",
      "Epoch 638/1000\n",
      "8/8 [==============================] - 0s 25ms/step - loss: 1.0728 - val_loss: 1.1153\n",
      "Epoch 639/1000\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 1.0734 - val_loss: 1.1133\n",
      "Epoch 640/1000\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 1.0854 - val_loss: 1.1111\n",
      "Epoch 641/1000\n",
      "8/8 [==============================] - 0s 25ms/step - loss: 1.0659 - val_loss: 1.1113\n",
      "Epoch 642/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0718 - val_loss: 1.1224\n",
      "Epoch 643/1000\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 1.0605 - val_loss: 1.1117\n",
      "Epoch 644/1000\n",
      "8/8 [==============================] - 0s 44ms/step - loss: 1.0694 - val_loss: 1.1252\n",
      "Epoch 645/1000\n",
      "8/8 [==============================] - 0s 47ms/step - loss: 1.0636 - val_loss: 1.1066\n",
      "Epoch 646/1000\n",
      "8/8 [==============================] - 0s 38ms/step - loss: 1.0724 - val_loss: 1.1127\n",
      "Epoch 647/1000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.0764 - val_loss: 1.1138\n",
      "Epoch 648/1000\n",
      "8/8 [==============================] - 0s 48ms/step - loss: 1.0726 - val_loss: 1.1192\n",
      "Epoch 649/1000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 1.0784 - val_loss: 1.1147\n",
      "Epoch 650/1000\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 1.0800 - val_loss: 1.1253\n",
      "Epoch 651/1000\n",
      "8/8 [==============================] - 0s 38ms/step - loss: 1.0721 - val_loss: 1.1160\n",
      "Epoch 652/1000\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 1.0727 - val_loss: 1.1075\n",
      "Epoch 653/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0783 - val_loss: 1.1080\n",
      "Epoch 654/1000\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 1.0687 - val_loss: 1.1267\n",
      "Epoch 655/1000\n",
      "8/8 [==============================] - 0s 53ms/step - loss: 1.0705 - val_loss: 1.1170\n",
      "Epoch 656/1000\n",
      "8/8 [==============================] - 0s 54ms/step - loss: 1.0775 - val_loss: 1.1163\n",
      "Epoch 657/1000\n",
      "8/8 [==============================] - 0s 48ms/step - loss: 1.0698 - val_loss: 1.1182\n",
      "Epoch 658/1000\n",
      "8/8 [==============================] - 0s 49ms/step - loss: 1.0720 - val_loss: 1.1091\n",
      "Epoch 659/1000\n",
      "8/8 [==============================] - 0s 49ms/step - loss: 1.0695 - val_loss: 1.1244\n",
      "Epoch 660/1000\n",
      "8/8 [==============================] - 0s 42ms/step - loss: 1.0723 - val_loss: 1.1172\n",
      "Epoch 661/1000\n",
      "8/8 [==============================] - 0s 48ms/step - loss: 1.0753 - val_loss: 1.1060\n",
      "Epoch 662/1000\n",
      "8/8 [==============================] - 0s 55ms/step - loss: 1.0765 - val_loss: 1.1043\n",
      "Epoch 663/1000\n",
      "8/8 [==============================] - 0s 42ms/step - loss: 1.0767 - val_loss: 1.1150\n",
      "Epoch 664/1000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.0695 - val_loss: 1.1228\n",
      "Epoch 665/1000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.0831 - val_loss: 1.1012\n",
      "Epoch 666/1000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 1.0749 - val_loss: 1.1196\n",
      "Epoch 667/1000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 1.0845 - val_loss: 1.1177\n",
      "Epoch 668/1000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 1.0783 - val_loss: 1.1168\n",
      "Epoch 669/1000\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 1.0651 - val_loss: 1.1094\n",
      "Epoch 670/1000\n",
      "8/8 [==============================] - 0s 20ms/step - loss: 1.0693 - val_loss: 1.1223\n",
      "Epoch 671/1000\n",
      "8/8 [==============================] - 0s 20ms/step - loss: 1.0746 - val_loss: 1.1105\n",
      "Epoch 672/1000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.0726 - val_loss: 1.1163\n",
      "Epoch 673/1000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 1.0825 - val_loss: 1.1101\n",
      "Epoch 674/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0846 - val_loss: 1.1165\n",
      "Epoch 675/1000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.0741 - val_loss: 1.1177\n",
      "Epoch 676/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0672 - val_loss: 1.1213\n",
      "Epoch 677/1000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.0664 - val_loss: 1.1090\n",
      "Epoch 678/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0676 - val_loss: 1.1201\n",
      "Epoch 679/1000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.0726 - val_loss: 1.1183\n",
      "Epoch 680/1000\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 1.0653 - val_loss: 1.1110\n",
      "Epoch 681/1000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 1.0670 - val_loss: 1.1182\n",
      "Epoch 682/1000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.0711 - val_loss: 1.1015\n",
      "Epoch 683/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0786 - val_loss: 1.1089\n",
      "Epoch 684/1000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 1.0779 - val_loss: 1.1118\n",
      "Epoch 685/1000\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 1.0649 - val_loss: 1.1085\n",
      "Epoch 686/1000\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 1.0679 - val_loss: 1.1206\n",
      "Epoch 687/1000\n",
      "8/8 [==============================] - 0s 57ms/step - loss: 1.0800 - val_loss: 1.1193\n",
      "Epoch 688/1000\n",
      "8/8 [==============================] - 0s 48ms/step - loss: 1.0657 - val_loss: 1.1108\n",
      "Epoch 689/1000\n",
      "8/8 [==============================] - 0s 51ms/step - loss: 1.0705 - val_loss: 1.1193\n",
      "Epoch 690/1000\n",
      "8/8 [==============================] - 0s 48ms/step - loss: 1.0616 - val_loss: 1.1212\n",
      "Epoch 691/1000\n",
      "8/8 [==============================] - 0s 58ms/step - loss: 1.0688 - val_loss: 1.1203\n",
      "Epoch 692/1000\n",
      "8/8 [==============================] - 0s 55ms/step - loss: 1.0730 - val_loss: 1.1175\n",
      "Epoch 693/1000\n",
      "8/8 [==============================] - 0s 53ms/step - loss: 1.0702 - val_loss: 1.1147\n",
      "Epoch 694/1000\n",
      "8/8 [==============================] - 0s 49ms/step - loss: 1.0694 - val_loss: 1.1226\n",
      "Epoch 695/1000\n",
      "8/8 [==============================] - 0s 67ms/step - loss: 1.0719 - val_loss: 1.1189\n",
      "Epoch 696/1000\n",
      "8/8 [==============================] - 0s 41ms/step - loss: 1.0773 - val_loss: 1.1172\n",
      "Epoch 697/1000\n",
      "8/8 [==============================] - 0s 48ms/step - loss: 1.0843 - val_loss: 1.1180\n",
      "Epoch 698/1000\n",
      "8/8 [==============================] - 0s 44ms/step - loss: 1.0720 - val_loss: 1.1065\n",
      "Epoch 699/1000\n",
      "8/8 [==============================] - 0s 53ms/step - loss: 1.0763 - val_loss: 1.1171\n",
      "Epoch 700/1000\n",
      "8/8 [==============================] - 0s 50ms/step - loss: 1.0796 - val_loss: 1.1188\n",
      "Epoch 701/1000\n",
      "8/8 [==============================] - 0s 25ms/step - loss: 1.0783 - val_loss: 1.1156\n",
      "Epoch 702/1000\n",
      "8/8 [==============================] - 0s 36ms/step - loss: 1.0644 - val_loss: 1.1097\n",
      "Epoch 703/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0689 - val_loss: 1.1226\n",
      "Epoch 704/1000\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 1.0837 - val_loss: 1.1177\n",
      "Epoch 705/1000\n",
      "8/8 [==============================] - 0s 45ms/step - loss: 1.0574 - val_loss: 1.1218\n",
      "Epoch 706/1000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 1.0706 - val_loss: 1.1079\n",
      "Epoch 707/1000\n",
      "8/8 [==============================] - 0s 21ms/step - loss: 1.0672 - val_loss: 1.1044\n",
      "Epoch 708/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0688 - val_loss: 1.1204\n",
      "Epoch 709/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0649 - val_loss: 1.1023\n",
      "Epoch 710/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0590 - val_loss: 1.1044\n",
      "Epoch 711/1000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 1.0696 - val_loss: 1.1188\n",
      "Epoch 712/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0900 - val_loss: 1.1283\n",
      "Epoch 713/1000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 1.0716 - val_loss: 1.1136\n",
      "Epoch 714/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0804 - val_loss: 1.1205\n",
      "Epoch 715/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0714 - val_loss: 1.1113\n",
      "Epoch 716/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0787 - val_loss: 1.1125\n",
      "Epoch 717/1000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 1.0661 - val_loss: 1.1119\n",
      "Epoch 718/1000\n",
      "8/8 [==============================] - 0s 25ms/step - loss: 1.0662 - val_loss: 1.1168\n",
      "Epoch 719/1000\n",
      "8/8 [==============================] - 0s 25ms/step - loss: 1.0682 - val_loss: 1.1172\n",
      "Epoch 720/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0645 - val_loss: 1.1108\n",
      "Epoch 721/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0724 - val_loss: 1.1153\n",
      "Epoch 722/1000\n",
      "8/8 [==============================] - 0s 25ms/step - loss: 1.0747 - val_loss: 1.1228\n",
      "Epoch 723/1000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.0714 - val_loss: 1.1222\n",
      "Epoch 724/1000\n",
      "8/8 [==============================] - 0s 24ms/step - loss: 1.0633 - val_loss: 1.1185\n",
      "Epoch 725/1000\n",
      "8/8 [==============================] - 0s 36ms/step - loss: 1.0641 - val_loss: 1.1093\n",
      "Epoch 726/1000\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 1.0635 - val_loss: 1.1144\n",
      "Epoch 727/1000\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 1.0664 - val_loss: 1.1108\n",
      "Epoch 728/1000\n",
      "8/8 [==============================] - 0s 37ms/step - loss: 1.0763 - val_loss: 1.1188\n",
      "Epoch 729/1000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.0812 - val_loss: 1.1047\n",
      "Epoch 730/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0761 - val_loss: 1.1084\n",
      "Epoch 731/1000\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 1.0686 - val_loss: 1.1055\n",
      "Epoch 732/1000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.0709 - val_loss: 1.1172\n",
      "Epoch 733/1000\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 1.0683 - val_loss: 1.1213\n",
      "Epoch 734/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0682 - val_loss: 1.1123\n",
      "Epoch 735/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0790 - val_loss: 1.1084\n",
      "Epoch 736/1000\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 1.0776 - val_loss: 1.1104\n",
      "Epoch 737/1000\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 1.0743 - val_loss: 1.1099\n",
      "Epoch 738/1000\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 1.0608 - val_loss: 1.1126\n",
      "Epoch 739/1000\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 1.0698 - val_loss: 1.1200\n",
      "Epoch 740/1000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 1.0763 - val_loss: 1.1159\n",
      "Epoch 741/1000\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 1.0741 - val_loss: 1.1126\n",
      "Epoch 742/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0688 - val_loss: 1.1078\n",
      "Epoch 743/1000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 1.0768 - val_loss: 1.1135\n",
      "Epoch 744/1000\n",
      "8/8 [==============================] - 0s 36ms/step - loss: 1.0752 - val_loss: 1.1213\n",
      "Epoch 745/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0728 - val_loss: 1.1118\n",
      "Epoch 746/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0718 - val_loss: 1.1060\n",
      "Epoch 747/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0736 - val_loss: 1.1055\n",
      "Epoch 748/1000\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 1.0698 - val_loss: 1.1052\n",
      "Epoch 749/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0739 - val_loss: 1.1106\n",
      "Epoch 750/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0731 - val_loss: 1.1128\n",
      "Epoch 751/1000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 1.0820 - val_loss: 1.1093\n",
      "Epoch 752/1000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.0667 - val_loss: 1.1155\n",
      "Epoch 753/1000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 1.0817 - val_loss: 1.1116\n",
      "Epoch 754/1000\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 1.0666 - val_loss: 1.1114\n",
      "Epoch 755/1000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 1.0727 - val_loss: 1.1121\n",
      "Epoch 756/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0738 - val_loss: 1.1164\n",
      "Epoch 757/1000\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 1.0770 - val_loss: 1.1113\n",
      "Epoch 758/1000\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 1.0665 - val_loss: 1.1247\n",
      "Epoch 759/1000\n",
      "8/8 [==============================] - 0s 22ms/step - loss: 1.0682 - val_loss: 1.1217\n",
      "Epoch 760/1000\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 1.0670 - val_loss: 1.1152\n",
      "Epoch 761/1000\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 1.0817 - val_loss: 1.1166\n",
      "Epoch 762/1000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.0664 - val_loss: 1.1136\n",
      "Epoch 763/1000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.0695 - val_loss: 1.1076\n",
      "Epoch 764/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0579 - val_loss: 1.1141\n",
      "Epoch 765/1000\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 1.0766 - val_loss: 1.1179\n",
      "Epoch 766/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0652 - val_loss: 1.1167\n",
      "Epoch 767/1000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 1.0678 - val_loss: 1.1148\n",
      "Epoch 768/1000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 1.0696 - val_loss: 1.1196\n",
      "Epoch 769/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0583 - val_loss: 1.1237\n",
      "Epoch 770/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0744 - val_loss: 1.1065\n",
      "Epoch 771/1000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 1.0723 - val_loss: 1.1225\n",
      "Epoch 772/1000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.0694 - val_loss: 1.1141\n",
      "Epoch 773/1000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.0714 - val_loss: 1.1058\n",
      "Epoch 774/1000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.0729 - val_loss: 1.1155\n",
      "Epoch 775/1000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.0638 - val_loss: 1.1014\n",
      "Epoch 776/1000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.0748 - val_loss: 1.1179\n",
      "Epoch 777/1000\n",
      "8/8 [==============================] - 0s 22ms/step - loss: 1.0658 - val_loss: 1.1076\n",
      "Epoch 778/1000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 1.0752 - val_loss: 1.1161\n",
      "Epoch 779/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0723 - val_loss: 1.1134\n",
      "Epoch 780/1000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 1.0646 - val_loss: 1.1098\n",
      "Epoch 781/1000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 1.0678 - val_loss: 1.1101\n",
      "Epoch 782/1000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.0634 - val_loss: 1.0994\n",
      "Epoch 783/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0798 - val_loss: 1.1112\n",
      "Epoch 784/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0647 - val_loss: 1.1115\n",
      "Epoch 785/1000\n",
      "8/8 [==============================] - 0s 25ms/step - loss: 1.0688 - val_loss: 1.1147\n",
      "Epoch 786/1000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 1.0667 - val_loss: 1.1248\n",
      "Epoch 787/1000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 1.0744 - val_loss: 1.1145\n",
      "Epoch 788/1000\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 1.0743 - val_loss: 1.0999\n",
      "Epoch 789/1000\n",
      "8/8 [==============================] - 0s 36ms/step - loss: 1.0695 - val_loss: 1.1090\n",
      "Epoch 790/1000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 1.0695 - val_loss: 1.1147\n",
      "Epoch 791/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0579 - val_loss: 1.1077\n",
      "Epoch 792/1000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.0686 - val_loss: 1.1184\n",
      "Epoch 793/1000\n",
      "8/8 [==============================] - 0s 38ms/step - loss: 1.0621 - val_loss: 1.1164\n",
      "Epoch 794/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0663 - val_loss: 1.1179\n",
      "Epoch 795/1000\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 1.0762 - val_loss: 1.1041\n",
      "Epoch 796/1000\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 1.0814 - val_loss: 1.1173\n",
      "Epoch 797/1000\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 1.0772 - val_loss: 1.1053\n",
      "Epoch 798/1000\n",
      "8/8 [==============================] - 0s 24ms/step - loss: 1.0696 - val_loss: 1.1099\n",
      "Epoch 799/1000\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 1.0712 - val_loss: 1.1209\n",
      "Epoch 800/1000\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 1.0735 - val_loss: 1.1159\n",
      "Epoch 801/1000\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 1.0716 - val_loss: 1.1175\n",
      "Epoch 802/1000\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 1.0703 - val_loss: 1.1067\n",
      "Epoch 803/1000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 1.0819 - val_loss: 1.1120\n",
      "Epoch 804/1000\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 1.0633 - val_loss: 1.1178\n",
      "Epoch 805/1000\n",
      "8/8 [==============================] - 0s 25ms/step - loss: 1.0622 - val_loss: 1.1118\n",
      "Epoch 806/1000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.0643 - val_loss: 1.1146\n",
      "Epoch 807/1000\n",
      "8/8 [==============================] - 0s 40ms/step - loss: 1.0741 - val_loss: 1.1167\n",
      "Epoch 808/1000\n",
      "8/8 [==============================] - 0s 20ms/step - loss: 1.0728 - val_loss: 1.1188\n",
      "Epoch 809/1000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.0769 - val_loss: 1.1128\n",
      "Epoch 810/1000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.0702 - val_loss: 1.1115\n",
      "Epoch 811/1000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 1.0764 - val_loss: 1.1183\n",
      "Epoch 812/1000\n",
      "8/8 [==============================] - 0s 37ms/step - loss: 1.0672 - val_loss: 1.1143\n",
      "Epoch 813/1000\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 1.0751 - val_loss: 1.1150\n",
      "Epoch 814/1000\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 1.0832 - val_loss: 1.1076\n",
      "Epoch 815/1000\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 1.0719 - val_loss: 1.1164\n",
      "Epoch 816/1000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 1.0685 - val_loss: 1.1224\n",
      "Epoch 817/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0641 - val_loss: 1.1133\n",
      "Epoch 818/1000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 1.0811 - val_loss: 1.1015\n",
      "Epoch 819/1000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 1.0742 - val_loss: 1.1173\n",
      "Epoch 820/1000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 1.0644 - val_loss: 1.1112\n",
      "Epoch 821/1000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 1.0758 - val_loss: 1.1187\n",
      "Epoch 822/1000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 1.0686 - val_loss: 1.1104\n",
      "Epoch 823/1000\n",
      "8/8 [==============================] - 0s 25ms/step - loss: 1.0664 - val_loss: 1.1103\n",
      "Epoch 824/1000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.0738 - val_loss: 1.1134\n",
      "Epoch 825/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0648 - val_loss: 1.1006\n",
      "Epoch 826/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0685 - val_loss: 1.1053\n",
      "Epoch 827/1000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 1.0636 - val_loss: 1.1226\n",
      "Epoch 828/1000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.0595 - val_loss: 1.1075\n",
      "Epoch 829/1000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.0735 - val_loss: 1.1050\n",
      "Epoch 830/1000\n",
      "8/8 [==============================] - 0s 36ms/step - loss: 1.0714 - val_loss: 1.1108\n",
      "Epoch 831/1000\n",
      "8/8 [==============================] - 0s 38ms/step - loss: 1.0783 - val_loss: 1.1070\n",
      "Epoch 832/1000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 1.0658 - val_loss: 1.1139\n",
      "Epoch 833/1000\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 1.0744 - val_loss: 1.1237\n",
      "Epoch 834/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0657 - val_loss: 1.1090\n",
      "Epoch 835/1000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 1.0718 - val_loss: 1.1095\n",
      "Epoch 836/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0844 - val_loss: 1.1152\n",
      "Epoch 837/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0630 - val_loss: 1.1230\n",
      "Epoch 838/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0710 - val_loss: 1.1164\n",
      "Epoch 839/1000\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 1.0715 - val_loss: 1.1134\n",
      "Epoch 840/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0781 - val_loss: 1.1099\n",
      "Epoch 841/1000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 1.0742 - val_loss: 1.1203\n",
      "Epoch 842/1000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 1.0680 - val_loss: 1.1043\n",
      "Epoch 843/1000\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 1.0677 - val_loss: 1.1167\n",
      "Epoch 844/1000\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 1.0641 - val_loss: 1.1098\n",
      "Epoch 845/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0807 - val_loss: 1.1105\n",
      "Epoch 846/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0716 - val_loss: 1.1101\n",
      "Epoch 847/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0637 - val_loss: 1.1163\n",
      "Epoch 848/1000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.0660 - val_loss: 1.1022\n",
      "Epoch 849/1000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 1.0705 - val_loss: 1.1160\n",
      "Epoch 850/1000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 1.0667 - val_loss: 1.1146\n",
      "Epoch 851/1000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.0655 - val_loss: 1.1103\n",
      "Epoch 852/1000\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 1.0809 - val_loss: 1.1177\n",
      "Epoch 853/1000\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 1.0623 - val_loss: 1.1078\n",
      "Epoch 854/1000\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 1.0736 - val_loss: 1.1141\n",
      "Epoch 855/1000\n",
      "8/8 [==============================] - 0s 21ms/step - loss: 1.0676 - val_loss: 1.1082\n",
      "Epoch 856/1000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 1.0745 - val_loss: 1.1072\n",
      "Epoch 857/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0624 - val_loss: 1.1105\n",
      "Epoch 858/1000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.0733 - val_loss: 1.1205\n",
      "Epoch 859/1000\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 1.0665 - val_loss: 1.1088\n",
      "Epoch 860/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0654 - val_loss: 1.1210\n",
      "Epoch 861/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0746 - val_loss: 1.1178\n",
      "Epoch 862/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0566 - val_loss: 1.1191\n",
      "Epoch 863/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0526 - val_loss: 1.1183\n",
      "Epoch 864/1000\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 1.0795 - val_loss: 1.1173\n",
      "Epoch 865/1000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 1.0619 - val_loss: 1.1132\n",
      "Epoch 866/1000\n",
      "8/8 [==============================] - 0s 39ms/step - loss: 1.0601 - val_loss: 1.1220\n",
      "Epoch 867/1000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.0678 - val_loss: 1.1209\n",
      "Epoch 868/1000\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 1.0757 - val_loss: 1.1010\n",
      "Epoch 869/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0825 - val_loss: 1.1031\n",
      "Epoch 870/1000\n",
      "8/8 [==============================] - 0s 37ms/step - loss: 1.0770 - val_loss: 1.1106\n",
      "Epoch 871/1000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.0706 - val_loss: 1.1095\n",
      "Epoch 872/1000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 1.0657 - val_loss: 1.1156\n",
      "Epoch 873/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0709 - val_loss: 1.1106\n",
      "Epoch 874/1000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 1.0682 - val_loss: 1.1089\n",
      "Epoch 875/1000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 1.0744 - val_loss: 1.1127\n",
      "Epoch 876/1000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.0688 - val_loss: 1.1162\n",
      "Epoch 877/1000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.0628 - val_loss: 1.1026\n",
      "Epoch 878/1000\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 1.0645 - val_loss: 1.1137\n",
      "Epoch 879/1000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.0636 - val_loss: 1.0971\n",
      "Epoch 880/1000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.0664 - val_loss: 1.1155\n",
      "Epoch 881/1000\n",
      "8/8 [==============================] - 0s 44ms/step - loss: 1.0581 - val_loss: 1.1084\n",
      "Epoch 882/1000\n",
      "8/8 [==============================] - 0s 59ms/step - loss: 1.0680 - val_loss: 1.1157\n",
      "Epoch 883/1000\n",
      "8/8 [==============================] - 0s 43ms/step - loss: 1.0648 - val_loss: 1.1166\n",
      "Epoch 884/1000\n",
      "8/8 [==============================] - 0s 54ms/step - loss: 1.0639 - val_loss: 1.1061\n",
      "Epoch 885/1000\n",
      "8/8 [==============================] - 0s 41ms/step - loss: 1.0624 - val_loss: 1.1076\n",
      "Epoch 886/1000\n",
      "8/8 [==============================] - 0s 41ms/step - loss: 1.0686 - val_loss: 1.1197\n",
      "Epoch 887/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0658 - val_loss: 1.1240\n",
      "Epoch 888/1000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 1.0644 - val_loss: 1.1185\n",
      "Epoch 889/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0701 - val_loss: 1.1202\n",
      "Epoch 890/1000\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 1.0650 - val_loss: 1.1044\n",
      "Epoch 891/1000\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 1.0713 - val_loss: 1.1180\n",
      "Epoch 892/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0703 - val_loss: 1.1160\n",
      "Epoch 893/1000\n",
      "8/8 [==============================] - 0s 25ms/step - loss: 1.0631 - val_loss: 1.1097\n",
      "Epoch 894/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0561 - val_loss: 1.1177\n",
      "Epoch 895/1000\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 1.0616 - val_loss: 1.1097\n",
      "Epoch 896/1000\n",
      "8/8 [==============================] - 0s 36ms/step - loss: 1.0647 - val_loss: 1.1146\n",
      "Epoch 897/1000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 1.0814 - val_loss: 1.1209\n",
      "Epoch 898/1000\n",
      "8/8 [==============================] - 0s 36ms/step - loss: 1.0695 - val_loss: 1.1170\n",
      "Epoch 899/1000\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 1.0711 - val_loss: 1.1062\n",
      "Epoch 900/1000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 1.0659 - val_loss: 1.1144\n",
      "Epoch 901/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0618 - val_loss: 1.1137\n",
      "Epoch 902/1000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 1.0725 - val_loss: 1.1133\n",
      "Epoch 903/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0608 - val_loss: 1.1087\n",
      "Epoch 904/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0692 - val_loss: 1.1129\n",
      "Epoch 905/1000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.0778 - val_loss: 1.1156\n",
      "Epoch 906/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0626 - val_loss: 1.1106\n",
      "Epoch 907/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0669 - val_loss: 1.1167\n",
      "Epoch 908/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0620 - val_loss: 1.1179\n",
      "Epoch 909/1000\n",
      "8/8 [==============================] - 0s 20ms/step - loss: 1.0640 - val_loss: 1.1241\n",
      "Epoch 910/1000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 1.0681 - val_loss: 1.1195\n",
      "Epoch 911/1000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 1.0686 - val_loss: 1.1249\n",
      "Epoch 912/1000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.0659 - val_loss: 1.1098\n",
      "Epoch 913/1000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.0658 - val_loss: 1.1028\n",
      "Epoch 914/1000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.0713 - val_loss: 1.1068\n",
      "Epoch 915/1000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.0646 - val_loss: 1.1129\n",
      "Epoch 916/1000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.0617 - val_loss: 1.1164\n",
      "Epoch 917/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0665 - val_loss: 1.1115\n",
      "Epoch 918/1000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 1.0712 - val_loss: 1.1092\n",
      "Epoch 919/1000\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 1.0669 - val_loss: 1.1141\n",
      "Epoch 920/1000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 1.0635 - val_loss: 1.1153\n",
      "Epoch 921/1000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 1.0625 - val_loss: 1.1113\n",
      "Epoch 922/1000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 1.0600 - val_loss: 1.1163\n",
      "Epoch 923/1000\n",
      "8/8 [==============================] - 0s 37ms/step - loss: 1.0731 - val_loss: 1.1142\n",
      "Epoch 924/1000\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 1.0673 - val_loss: 1.1147\n",
      "Epoch 925/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0580 - val_loss: 1.1143\n",
      "Epoch 926/1000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.0660 - val_loss: 1.1219\n",
      "Epoch 927/1000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 1.0689 - val_loss: 1.1079\n",
      "Epoch 928/1000\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 1.0672 - val_loss: 1.1023\n",
      "Epoch 929/1000\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 1.0790 - val_loss: 1.1160\n",
      "Epoch 930/1000\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 1.0691 - val_loss: 1.1116\n",
      "Epoch 931/1000\n",
      "8/8 [==============================] - 0s 37ms/step - loss: 1.0895 - val_loss: 1.1034\n",
      "Epoch 932/1000\n",
      "8/8 [==============================] - 0s 25ms/step - loss: 1.0783 - val_loss: 1.1099\n",
      "Epoch 933/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0709 - val_loss: 1.1186\n",
      "Epoch 934/1000\n",
      "8/8 [==============================] - 0s 25ms/step - loss: 1.0652 - val_loss: 1.1169\n",
      "Epoch 935/1000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 1.0743 - val_loss: 1.1161\n",
      "Epoch 936/1000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.0726 - val_loss: 1.1211\n",
      "Epoch 937/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0676 - val_loss: 1.1126\n",
      "Epoch 938/1000\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 1.0746 - val_loss: 1.1119\n",
      "Epoch 939/1000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.0766 - val_loss: 1.1130\n",
      "Epoch 940/1000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.0598 - val_loss: 1.1090\n",
      "Epoch 941/1000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 1.0604 - val_loss: 1.1187\n",
      "Epoch 942/1000\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 1.0635 - val_loss: 1.1181\n",
      "Epoch 943/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0621 - val_loss: 1.1025\n",
      "Epoch 944/1000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 1.0678 - val_loss: 1.1196\n",
      "Epoch 945/1000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 1.0585 - val_loss: 1.1037\n",
      "Epoch 946/1000\n",
      "8/8 [==============================] - 0s 25ms/step - loss: 1.0655 - val_loss: 1.1114\n",
      "Epoch 947/1000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.0589 - val_loss: 1.1149\n",
      "Epoch 948/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0660 - val_loss: 1.1169\n",
      "Epoch 949/1000\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 1.0737 - val_loss: 1.1159\n",
      "Epoch 950/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0681 - val_loss: 1.1116\n",
      "Epoch 951/1000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.0616 - val_loss: 1.1016\n",
      "Epoch 952/1000\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 1.0606 - val_loss: 1.1089\n",
      "Epoch 953/1000\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 1.0901 - val_loss: 1.1132\n",
      "Epoch 954/1000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.0690 - val_loss: 1.1174\n",
      "Epoch 955/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0666 - val_loss: 1.1092\n",
      "Epoch 956/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0838 - val_loss: 1.1183\n",
      "Epoch 957/1000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 1.0726 - val_loss: 1.1153\n",
      "Epoch 958/1000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 1.0683 - val_loss: 1.1034\n",
      "Epoch 959/1000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.0630 - val_loss: 1.1130\n",
      "Epoch 960/1000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 1.0780 - val_loss: 1.1038\n",
      "Epoch 961/1000\n",
      "8/8 [==============================] - 0s 37ms/step - loss: 1.0622 - val_loss: 1.1168\n",
      "Epoch 962/1000\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 1.0834 - val_loss: 1.1111\n",
      "Epoch 963/1000\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 1.0783 - val_loss: 1.1104\n",
      "Epoch 964/1000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 1.0660 - val_loss: 1.1131\n",
      "Epoch 965/1000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.0702 - val_loss: 1.1039\n",
      "Epoch 966/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0619 - val_loss: 1.1137\n",
      "Epoch 967/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0632 - val_loss: 1.1078\n",
      "Epoch 968/1000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 1.0730 - val_loss: 1.1179\n",
      "Epoch 969/1000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 1.0784 - val_loss: 1.1036\n",
      "Epoch 970/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0689 - val_loss: 1.1106\n",
      "Epoch 971/1000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 1.0744 - val_loss: 1.1155\n",
      "Epoch 972/1000\n",
      "8/8 [==============================] - 0s 37ms/step - loss: 1.0735 - val_loss: 1.1140\n",
      "Epoch 973/1000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 1.0734 - val_loss: 1.1158\n",
      "Epoch 974/1000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.0662 - val_loss: 1.1210\n",
      "Epoch 975/1000\n",
      "8/8 [==============================] - 0s 36ms/step - loss: 1.0707 - val_loss: 1.1081\n",
      "Epoch 976/1000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 1.0669 - val_loss: 1.1125\n",
      "Epoch 977/1000\n",
      "8/8 [==============================] - 0s 40ms/step - loss: 1.0605 - val_loss: 1.1047\n",
      "Epoch 978/1000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.0731 - val_loss: 1.1146\n",
      "Epoch 979/1000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.0604 - val_loss: 1.1076\n",
      "Epoch 980/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0696 - val_loss: 1.1242\n",
      "Epoch 981/1000\n",
      "8/8 [==============================] - 0s 24ms/step - loss: 1.0654 - val_loss: 1.1244\n",
      "Epoch 982/1000\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 1.0674 - val_loss: 1.1148\n",
      "Epoch 983/1000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.0685 - val_loss: 1.1144\n",
      "Epoch 984/1000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.0716 - val_loss: 1.1176\n",
      "Epoch 985/1000\n",
      "8/8 [==============================] - 0s 36ms/step - loss: 1.0727 - val_loss: 1.1091\n",
      "Epoch 986/1000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 1.0654 - val_loss: 1.1151\n",
      "Epoch 987/1000\n",
      "8/8 [==============================] - 0s 38ms/step - loss: 1.0777 - val_loss: 1.1088\n",
      "Epoch 988/1000\n",
      "8/8 [==============================] - 0s 37ms/step - loss: 1.0759 - val_loss: 1.1090\n",
      "Epoch 989/1000\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 1.0617 - val_loss: 1.1188\n",
      "Epoch 990/1000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.0770 - val_loss: 1.1111\n",
      "Epoch 991/1000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 1.0629 - val_loss: 1.1086\n",
      "Epoch 992/1000\n",
      "8/8 [==============================] - 0s 25ms/step - loss: 1.0743 - val_loss: 1.1100\n",
      "Epoch 993/1000\n",
      "8/8 [==============================] - 0s 25ms/step - loss: 1.0699 - val_loss: 1.1055\n",
      "Epoch 994/1000\n",
      "8/8 [==============================] - 0s 25ms/step - loss: 1.0712 - val_loss: 1.1139\n",
      "Epoch 995/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0666 - val_loss: 1.1039\n",
      "Epoch 996/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0561 - val_loss: 1.1018\n",
      "Epoch 997/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0751 - val_loss: 1.1252\n",
      "Epoch 998/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0689 - val_loss: 1.1189\n",
      "Epoch 999/1000\n",
      "8/8 [==============================] - 0s 44ms/step - loss: 1.0648 - val_loss: 1.1145\n",
      "Epoch 1000/1000\n",
      "8/8 [==============================] - 0s 37ms/step - loss: 1.0731 - val_loss: 1.1080\n",
      "<tf.Variable 'prior_c:0' shape=(1,) dtype=float32, numpy=array([2.1295652], dtype=float32)>\n",
      "<tf.Variable 'alpha_post:0' shape=(1,) dtype=float32, numpy=array([2.4993858], dtype=float32)>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "vae.compile(optimizer=tf.optimizers.Adam(learning_rate=1e-5),\n",
    "            loss=negative_log_likelihood)\n",
    "\n",
    "vae.fit(x=train_dataset,\n",
    "\ty=train_dataset, \n",
    "        validation_data=(eval_dataset,eval_dataset), \n",
    "        batch_size=32,\n",
    "        epochs=1000)\n",
    "print(vae.encoder.prior.concentration)\n",
    "print(vae.encoder.alpha_post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_components =2\n",
    "ev_shape = [1]\n",
    "tfpl.MixtureSameFamily.params_size(num_components,\n",
    "                                                 component_params_size=tfpl.IndependentNormal.params_size(ev_shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfpl.MixtureSameFamily.params_size(num_components,\n",
    "                                                 component_params_size=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixt_dist1     = tfpl.MixtureSameFamily(2, tfpl.DistributionLambda(\n",
    "            make_distribution_fn=lambda t: tfd.Gamma(\n",
    "                concentration=t[...,0], rate=t[...,1])))\n",
    "\n",
    "\n",
    "\n",
    "samples = mixt_dist1([[0.,0.,0.01,2.,10.,10.]]).sample(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_91612/1549178342.py:2: UserWarning: \n",
      "\n",
      "`distplot` is a deprecated function and will be removed in seaborn v0.14.0.\n",
      "\n",
      "Please adapt your code to use either `displot` (a figure-level function with\n",
      "similar flexibility) or `histplot` (an axes-level function for histograms).\n",
      "\n",
      "For a guide to updating your code to use the new functions, please see\n",
      "https://gist.github.com/mwaskom/de44147ed2974457ad6372750bbe5751\n",
      "\n",
      "  sns.distplot(samples)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [49]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m sns\u001b[38;5;241m.\u001b[39mdistplot(samples)\n\u001b[1;32m      3\u001b[0m g \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m t: tfd\u001b[38;5;241m.\u001b[39mGamma(\n\u001b[1;32m      4\u001b[0m                 concentration\u001b[38;5;241m=\u001b[39mt[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m,\u001b[38;5;241m0\u001b[39m], rate\u001b[38;5;241m=\u001b[39mt[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m,\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m----> 6\u001b[0m \u001b[43mg\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m2.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m10.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m10.\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [49]\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m\n\u001b[1;32m      2\u001b[0m sns\u001b[38;5;241m.\u001b[39mdistplot(samples)\n\u001b[1;32m      3\u001b[0m g \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m t: tfd\u001b[38;5;241m.\u001b[39mGamma(\n\u001b[0;32m----> 4\u001b[0m                 concentration\u001b[38;5;241m=\u001b[39m\u001b[43mt\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m, rate\u001b[38;5;241m=\u001b[39mt[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m,\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m      6\u001b[0m g([[\u001b[38;5;241m0.01\u001b[39m,\u001b[38;5;241m2.\u001b[39m,\u001b[38;5;241m10.\u001b[39m,\u001b[38;5;241m10.\u001b[39m]])\n",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not tuple"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjEAAAGdCAYAAADjWSL8AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAA9hAAAPYQGoP6dpAAA24UlEQVR4nO3deXRb9Z3//5ck27IdW4r3JbYTZ99DQiAJJFC2QOhQlvY709JCoN/2AAU6TMq3A8w5bafz7QR+benybclA4YTJj1KYAmHgW7YAWdgSSAhZyB4vcRw7ju3Y8irb0v3+YUvZnMSWJV1d6/k4x4fo6kp653Jjv/z5vD/32gzDMAQAAGAxdrMLAAAACAUhBgAAWBIhBgAAWBIhBgAAWBIhBgAAWBIhBgAAWBIhBgAAWBIhBgAAWFKC2QUMhd/v15EjR5Seni6bzWZ2OQAAYAAMw1BLS4sKCwtlt4c+nmLpEHPkyBEVFxebXQYAAAhBVVWVioqKQn69pUNMenq6pN6D4HK5TK4GAAAMhMfjUXFxcfDneKgsHWICU0gul4sQAwCAxQy1FYTGXgAAYEmEGAAAYEmEGAAAYEmEGAAAYEmEGAAAYEmEGAAAYEmEGAAAYEmEGAAAYEmEGAAAYEmEGAAAYEmEGAAAYEmEGAAAYEmEGAAAYEmEGAAAYEkJZheAwXl+06FBv+bWeSURqAQAAHMxEgMAACyJEAMAACyJEAMAACyJEAMAACyJEAMAACyJEAMAACyJEAMAACyJEAMAACyJEAMAACyJEAMAACyJEAMAACyJEAMAACyJEAMAACyJEAMAACyJEAMAACyJEAMAACyJEAMAACyJEAMAACyJEAMAACyJEAMAACyJEAMAACyJEAMAACyJEAMAACzJ1BDzs5/9TDab7ZSv/Px8M0sCAAAWkWB2AdOmTdO7774bfOxwOEysBgAAWIXpISYhIYHRFwAAMGim98Ts379fhYWFKi0t1Te/+U2VlZWddV+v1yuPx3PKFwAAiE+mhph58+Zp1apVevvtt/WnP/1JtbW1uuSSS9TQ0NDv/suXL5fb7Q5+FRcXR7liAAAQK2yGYRhmFxHQ1tamcePG6cc//rGWLVt2xvNer1derzf42OPxqLi4WM3NzXK5XNEs1TTPbzo06NfcOq8kApUAABAaj8cjt9s95J/fpvfEnGzEiBGaMWOG9u/f3+/zTqdTTqczylUBAIBYZHpPzMm8Xq92796tgoICs0sBAAAxztQQ8+CDD2r9+vUqLy/Xpk2b9I1vfEMej0dLly41sywAAGABpk4nHT58WN/61rdUX1+vnJwczZ8/Xxs3btTo0aPNLAsAAFiAqSHmhRdeMPPjAQCAhcVUTwwAAMBAEWIAAIAlEWIAAIAlEWIAAIAlEWIAAIAlEWIAAIAlEWIAAIAlEWIAAIAlEWIAAIAlEWIAAIAlEWIAAIAlEWIAAIAlEWIAAIAlEWIAAIAlEWIAAIAlEWIAAIAlEWIAAIAlEWIAAIAlEWIAAIAlEWIAAIAlEWIAAIAlEWIAAIAlEWIAAIAlEWIAAIAlEWIAAIAlEWIAAIAlEWIAAIAlEWIAAIAlEWIAAIAlEWIAAIAlEWIAAIAlEWIAAIAlEWIAAIAlEWIAAIAlEWIAAIAlEWIAAIAlEWIAAIAlEWIAAIAlEWIAAIAlEWIAAIAlEWIAAIAlEWIAAIAlEWIAAIAlEWIAAIAlEWIAAIAlEWIAAIAlEWIAAIAlEWIAAIAlEWIAAIAlEWIAAIAlEWIAAIAlEWIAAIAlEWIAAIAlxUyIWb58uWw2mx544AGzSwEAABYQEyHms88+01NPPaWZM2eaXQoAALAI00NMa2urvv3tb+tPf/qTMjIyzC4HAABYhOkh5t5779VXv/pVXX311WaXAgAALCTBzA9/4YUX9Pnnn+uzzz4b0P5er1derzf42OPxRKo0AAAQ40wbiamqqtI//uM/6rnnnlNycvKAXrN8+XK53e7gV3FxcYSrBAAAscpmGIZhxge/+uqruvnmm+VwOILbfD6fbDab7Ha7vF7vKc9J/Y/EFBcXq7m5WS6XK2q1m+n5TYcG/Zpb55VEoBIAAELj8XjkdruH/PPbtOmkq666Sjt27Dhl25133qnJkyfrn//5n88IMJLkdDrldDqjVSIAAIhhpoWY9PR0TZ8+/ZRtI0aMUFZW1hnbAQAATmf66iQAAIBQmLo66XTr1q0zuwQAAGARjMQAAABLIsQAAABLIsQAAABLIsQAAABLIsQAAABLIsQAAABLIsQAAABLIsQAAABLIsQAAABLIsQAAABLIsQAAABLIsQAAABLIsQAAABLIsQAAABLIsQAAABLIsQAAABLIsQAAABLIsQAAABLIsQAAABLIsQAAABLIsQAAABLIsQAAABLIsQAAABLIsQAAABLIsQAAABLIsQAAABLIsQAAABLIsQAAABLIsQAAABLIsQAAABLIsQAAABLIsQAAABLIsQAAABLIsQAAABLIsQAAABLIsQAAABLIsQAAABLIsQAAABLIsQAAABLIsQAAABLIsQAAABLIsQAAABLIsQAAABLIsQAAABLIsQAAABLCinElJeXh7sOAACAQQkpxIwfP15XXHGFnnvuOXV2doa7JgAAgPMKKcRs27ZNs2fP1o9+9CPl5+frrrvu0qeffhru2gAAAM4qpBAzffp0Pf7446qurtbKlStVW1urhQsXatq0aXr88cd17NixcNcJAABwiiE19iYkJOjmm2/Wf/3Xf+mxxx7TwYMH9eCDD6qoqEi33367ampqwlUnAADAKYYUYjZv3qwf/OAHKigo0OOPP64HH3xQBw8e1Pvvv6/q6mrdeOON4aoTAADgFAmhvOjxxx/XypUrtXfvXl1//fVatWqVrr/+etntvZmotLRUTz75pCZPnhzWYgEAAAJCCjErVqzQd7/7Xd15553Kz8/vd5+SkhI988wzQyoOAADgbEIKMWvWrFFJSUlw5CXAMAxVVVWppKRESUlJWrp0aViKBAAAOF1IPTHjxo1TfX39GdsbGxtVWlo64PdZsWKFZs6cKZfLJZfLpQULFujNN98MpSQAABBnQgoxhmH0u721tVXJyckDfp+ioiI9+uij2rx5szZv3qwrr7xSN954o7788stQygIAAHFkUNNJy5YtkyTZbDb95Cc/UWpqavA5n8+nTZs26YILLhjw+91www2nPP7FL36hFStWaOPGjZo2bdpgSgMAAHFmUCFm69atknpHYnbs2KGkpKTgc0lJSZo1a5YefPDBkArx+Xz661//qra2Ni1YsCCk9wAAAPFjUCFm7dq1kqQ777xTv/vd7+RyuYZcwI4dO7RgwQJ1dnYqLS1Nq1ev1tSpU/vd1+v1yuv1Bh97PJ4hfz4AALCmkHpiVq5cGZYAI0mTJk3SF198oY0bN+qee+7R0qVLtWvXrn73Xb58udxud/CruLg4LDUAAADrsRln69I9zS233KJnn31WLpdLt9xyyzn3feWVV0Iu6Oqrr9a4ceP05JNPnvFcfyMxxcXFam5uDluoinXPbzo06NfcOq8kApUAABAaj8cjt9s95J/fA55OcrvdstlswT9HimEYpwSVkzmdTjmdzoh9NgAAsI4Bh5iVK1f2++eheOSRR7RkyRIVFxerpaVFL7zwgtatW6e33norLO8PAACGr5Cu2NvR0SHDMIJLrCsrK4MNuYsXLx7w+xw9elS33Xabampq5Ha7NXPmTL311lu65pprQikLAADEkZBCzI033qhbbrlFd999t5qamnTxxRcrKSlJ9fX1evzxx3XPPfcM6H24txIAAAhVSKuTPv/8cy1atEiS9NJLLyk/P1+VlZVatWqVfv/734e1QAAAgP6EFGLa29uVnp4uSXrnnXd0yy23yG63a/78+aqsrAxrgQAAAP0JKcSMHz9er776qqqqqvT2228H+2Dq6uriZqkzAAAwV0gh5ic/+YkefPBBjRkzRvPmzQveJuCdd97R7Nmzw1ogAABAf0Jq7P3GN76hhQsXqqamRrNmzQpuv+qqq3TzzTeHrTgAAICzCSnESFJ+fr7y8/NP2XbxxRcPuSAAAICBCCnEtLW16dFHH9V7772nuro6+f3+U54vKysLS3EAAABnE1KI+d73vqf169frtttuU0FBQfB2BAAAANESUoh588039be//U2XXnppuOsBAAAYkJBWJ2VkZCgzMzPctQAAAAxYSCHm3/7t3/STn/xE7e3t4a4HAABgQEKaTvr1r3+tgwcPKi8vT2PGjFFiYuIpz3/++edhKQ4AAOBsQgoxN910U5jLAAAAGJyQQsxPf/rTcNcBAAAwKCH1xEhSU1OTnn76aT388MNqbGyU1DuNVF1dHbbiAAAAziakkZjt27fr6quvltvtVkVFhb7//e8rMzNTq1evVmVlpVatWhXuOgEAAE4R0kjMsmXLdMcdd2j//v1KTk4Obl+yZIk2bNgQtuIAAADOJqQQ89lnn+muu+46Y/uoUaNUW1s75KIAAADOJ6QQk5ycLI/Hc8b2vXv3KicnZ8hFAQAAnE9IIebGG2/Uz3/+c3V3d0uSbDabDh06pIceekhf//rXw1ogAABAf0IKMb/61a907Ngx5ebmqqOjQ5dffrnGjx+v9PR0/eIXvwh3jQAAAGcIaXWSy+XShx9+qLVr12rLli3y+/2aM2eOrr766nDXBwAA0K9Bhxi/369nn31Wr7zyiioqKmSz2VRaWqr8/HwZhiGbzRaJOgEAAE4xqOkkwzD0ta99Td/73vdUXV2tGTNmaNq0aaqsrNQdd9yhm2++OVJ1AgAAnGJQIzHPPvusNmzYoPfee09XXHHFKc+9//77uummm7Rq1SrdfvvtYS0SAADgdIMaifnLX/6iRx555IwAI0lXXnmlHnroIf35z38OW3EAAABnM6gQs337dl133XVnfX7JkiXatm3bkIsCAAA4n0GFmMbGRuXl5Z31+by8PB0/fnzIRQEAAJzPoEKMz+dTQsLZ22gcDod6enqGXBQAAMD5DKqx1zAM3XHHHXI6nf0+7/V6w1IUAADA+QwqxCxduvS8+7AyCQAARMOgQszKlSsjVQcAAMCghHTvJAAAALMRYgAAgCURYgAAgCURYgAAgCURYgAAgCURYgAAgCURYgAAgCURYgAAgCURYgAAgCUN6oq9MM+7u45qc+Vxba5oVEqiQ9dMzVOCgwwKAIhfhBgL2FPr0fdWbT5lmyslUZeOzzapIgAAzMev8hbw1PoySdKsIrdmF4+UJG3Yd0zdPr+JVQEAYC5CTIyrburQa9uOSJL+900zdPOcURqZmqgWb48+LW80uToAAMxDiIlxT39Qph6/oUvHZ2lGkVsJdruumJgridEYAEB8I8TEsONtXXrh0ypJ0l2XjQtunz16ZHA0ZkvlcbPKAwDAVISYGPbmzlp1dPs0pcClRRNONPEm2O2aX5olSdpb22JWeQAAmIoQE8M2ljVIkq6dliebzXbKc+Nz0yRJ5Q1t8vmNqNcGAIDZCDExyjCMYIiZ1zfqcrJ8d7JSEh3q6vGruqkj2uUBAGA6QkyMqmhoV12LV0kOu2aXjDzjebvNprE5IyRJZcdao1wdAADmI8TEqMAozAUlI5Wc6Oh3n7E5vVNKBwkxAIA4RIiJUZv6Qsz80syz7jM2u3ckprKhXT0stQYAxBlCTAzq7YfpvZDd/LFn9sME5KY7leZMUI/f0KHj7dEqDwCAmGBqiFm+fLkuuugipaenKzc3VzfddJP27t1rZkkx4VBju2o9nUp02DS7JOOs+9lO6Ytpi1Z5AADEBFNDzPr163Xvvfdq48aNWrNmjXp6erR48WK1tcX3D+RNfaMws4pGKiWp/36YgHF9fTE09wIA4o2pd7F+6623Tnm8cuVK5ebmasuWLbrssstMqsp8n1b0hph5Y8/eDxNQnJEqSapp7pTfMGQ/7XoyAAAMV6aGmNM1NzdLkjIz+//h7fV65fV6g489Hk9U6oq23TW9f68Zo0aed9+cdKccdpu8PX41tXcrc0RShKsDACA2xExjr2EYWrZsmRYuXKjp06f3u8/y5cvldruDX8XFxVGuMvJ6fH7tP9o7NTS1wHXe/R12m/LSnZKkI1z0DgAQR2ImxNx3333avn27/vKXv5x1n4cffljNzc3Br6qqqihWGB3l9W3q8vk1IsmhooyUAb0m3927X62nM5KlAQAQU2JiOun+++/Xa6+9pg0bNqioqOis+zmdTjmdzihWFn27+27oOCk/XXb7wPpbCtzJknr7YgAAiBemhhjDMHT//fdr9erVWrdunUpLS80sJyYE+mEmD2AqKSAQYmqbmU4CAMQPU0PMvffeq+eff17//d//rfT0dNXW1kqS3G63UlIGNpUy3OzpCzFTBhVieo/V8fZudXT5zrssGwCA4cDUnpgVK1aoublZX/nKV1RQUBD8evHFF80sy1R7+qaTpuSnD/g1KUkOjUxJlERfDAAgfpg+nYQTmtq7gn0tEwcRYiQp352spo5u1TR3qLTvnkoAAAxnMbM6CdLumt5RmKKMFLmSEwf12hN9MYzEAADiAyEmhuypHXw/TEBgmTUrlAAA8YIQE0P21Ay+HyagsG8k5qinUz4/03QAgOGPEBNDAiMxg1leHZAxIkmJDpt6/IaOt3eFuzQAAGIOISZGGIah/XW9txuYmDf4kRi7zabstN4LAR5r8Z5nbwAArI8QEyNqPZ1q7/IpwW7T6KzUkN4jEGLqWwkxAIDhjxATI8qOtUmSSjJTlegI7X9LTjojMQCA+EGIiRFlx3qnksbmhH6NlxymkwAAcYQQEyMO9o3EjM1JC/k9giMxTCcBAOIAISZGlNX3hZghXG030BPT3uVTm7cnLHUBABCrCDEx4sR0UugjMUkJdrn77qFEcy8AYLgjxMSAzm6fqps6JEnjhtATI9HcCwCIH4SYGFBe3ybDkNwpicockTSk9wo29zISAwAY5ggxMaAs2NQ7QjabbUjvlc1IDAAgThBiYkCwHyY79H6YgBwueAcAiBOEmBgQXJk0xH4Y6URPTGNbl3r8/iG/HwAAsYoQEwMCIzFDbeqVJFdygpIS7PIbUmMrN4IEAAxfhBiTGYZxUk/M0KeTbDYbzb0AgLhAiDHZsVavWrw9stsU8o0fT5eV1rvCqYGRGADAMEaIMVlFfbskaVRGipwJjrC8Z9aI3pGYhjZGYgAAwxchxmQVDb1TSWOyht4PE5DdNxJTz0gMAGAYI8SYrKI+/CEmq++CeY1thBgAwPBFiDFZZUPvdNKYIdz48XRZfY29zR3d6uphmTUAYHgixJjsxHRSeJp6JSk1yaHkxN7/tYzGAACGK0KMiQzDCE4njQ7jdJLNZqO5FwAw7BFiTFTf2qW2Lp/sNqk4MyWs780yawDAcEeIMVFl31RS4cjwLa8OyOYeSgCAYY4QY6LyCKxMCgisUGqgJwYAMEwRYkx0YmVS+Jp6AwIrlBoYiQEADFOEGBNF4kJ3Adl9IzGezh51dPnC/v4AAJiNEGOiQIgJ58qkgJSTlllXNraF/f0BADAbIcYkhmGosu++SaURmE6y2WzB5t7AMm4AAIYTQoxJGtu61OLtkc0mFWWEP8RIJ5p7y/vCEgAAwwkhxiSBqaRCd4qSE8O7vDog0NwbWMoNAMBwQogxSUV95FYmBZwYiSHEAACGH0KMSSoj2NQbEOyJYSQGADAMEWJMUh64RkwYb/x4usBIzFGPV+1dPRH7HAAAzECIMUllBK8RE5DqTFBKX79N4MJ6AAAMF4QYExiGceKWA9mRCzHSiRtBsswaADDcEGJMcLy9Wy2dvdM7JZmRm06STkwpVTASAwAYZggxJjixvDo5YsurA7K44B0AYJgixJggGiuTArL7ppPKWaEEABhmCDEmKI/CNWICskZwwTsAwPBEiDFBNFYmBQQae1lmDQAYbggxJgg02UZjOik1KUEjUxN7P5d7KAEAhhFCjAkqgsurIz+dJJ0IS0wpAQCGE0JMlDW1d6m5o1uSNDoz8iMxklTad1VgmnsBAMMJISbKAlNJ+a5kpSRFdnl1QOCCeiyzBgAMJ4SYKAsEidERvGfS6QINxFzwDgAwnBBioixwobvSCN9u4GSBkZhyRmIAAMMIISbKTozERC/EjM3p/axjLV61dHZH7XMBAIgkQkyUBUZDSqO0MkmSXMmJyu67/UDZMUZjAADDAyEmigzDUFlfiBmbkxbVzw6MxjClBAAYLggxUVTf2qWWzh7ZbNFt7JWkcX0hpuxYa1Q/FwCASDE1xGzYsEE33HCDCgsLZbPZ9Oqrr5pZTsQFAkRRRoqcCdFZXh0QaCQ+yEgMAGCYMDXEtLW1adasWfrDH/5gZhlRE5xKyo7uVNLJn0lPDABguEgw88OXLFmiJUuWmFlCVJUH+2GitzIp4ERPTKv8fkN2uy3qNQAAEE6mhpjB8nq98nq9wccej8fEagYvMJ00NorXiAkozkxVgt2mzm6/ajydGjUyJeo1AAAQTpZq7F2+fLncbnfwq7i42OySBiUwlRPtlUmSlOiwq6SvmZjmXgDAcGCpEPPwww+rubk5+FVVVWV2SQPW7fPrUGPvZf+jebXekwX6YlhmDQAYDiw1neR0OuV0Os0uIyRVje3q8RtKSXQo35VsSg3jckbo3d009wIAhgdLjcRYWSA4lGaPMK2pNrjMmukkAMAwYOpITGtrqw4cOBB8XF5eri+++EKZmZkqKSkxsbLwC95uwISVSQGBXhxGYgAAw4GpIWbz5s264oorgo+XLVsmSVq6dKmeffZZk6qKjLL63tGPcSb1w0gnllkfae5QZ7dPyYnRveAeAADhZGqI+cpXviLDMMwsIWoOmrgyKSBrRJLcKYlq7ujWwWOtmlboNq0WAACGip6YKDm5J8YsNptNE/N6Q9T+o/TFAACsjRATBU3tXapv7b1InxlX6z3ZhLx0SdK+oy2m1gEAwFARYqJgX9+ox6iRKUpPTjS1lkmEGADAMEGIiYJAYAhM5ZhpQl8N+5hOAgBYHCEmCk6EmHSTKzkxEnOosV3tXT0mVwMAQOgIMVEQCDETYiDEZKU5lTUiSZJ0oI7RGACAdRFioiAwdTMpBkKMdGJEaG8tfTEAAOsixERYfatXjW1dstmk8bnm98RIJ3pz9jMSAwCwMEJMhAWmkoozUpWSFBtXyGWZNQBgOCDERNi+2thp6g2YlN8XYphOAgBYGCEmwvb1TdnEwvLqgIm5vSHmSHOnWjq7Ta4GAIDQEGIibH8MLa8OcKcmKs/llMT1YgAA1kWIiSDDMIIrgGIpxEisUAIAWB8hJoLqWrzydPbIbjP/nkmnm1rokiTtPNJsciUAAISGEBNBu2o8kqQx2SOUnBgbK5MCZoxyS5J2VhNiAADWlGB2AcPZzsO9ASEQGGJJoKY9NS3q6vErKYE8a5bnNx0a8L4+v6GObp+WTM9XosOupIS+L4ddiQ6bbDZbBCsFgNhCiImgHdWxG2JKMlPlSk6Qp7NH+462aHoM1hivWr09qmpsV3VTh+pbvWpo7VKbt0cd3T55e/ySpH9/Y/cZr0tzJqhwZLIKR6Zo1MgUFWWkamqhSzNGuZXZd6sJABhOCDERFJiqicWAYLPZNH2UWx8fbNDO6uaYrDFeeDq6tbvWo/L6NlU1tut4+/mXvSc6bPL5DfmNE9tavT3ad7S13xVnGamJGjUyRSWZqRqfl668dOeARm1unVcyqL8LAEQTISZCGlq9OtLcKUma1tdEG2tm9IWY7dXN+qbZxcSZA3UtevvLo1qz66i+qGo64/mcdKeKM1KVm+5UVlqSXMmJSklyKCXRoeREhxz23gDiNwz5/IZ6fIZavT1q6uhSU3u3mtq71dDm1ZGmDtW3dul4e7eOt3dr5xGPtLNWruQEjc9N14S8NE3KS4+5ni0AGAhCTIQEppLGZo9QenKiydX0bzrNvVHV0tmt17Yd0QufVgXPj4DijBRNzE9XSWaqikYO/BYVdptNdodNiQ4pJcmhnHTnGft0dvtU3dSh6uMdOnisVeX1bfJ09ujzQ8f1+aHjcthtmpCbphmj3JpS4CLQALAMQkyExPJUUgDNveHXX5PuocZ2fVreqB3VTer29c7/OGw2jcsdoakFbk0uSJcrgkE3OdGhcTlpGpeTpssm5qjb51dFQ5v2H23VntoW1bd6tae2RXtqW+Sw2zQxL11zSkYGb08BALGKEBMhsdzUGzA6K1XpyQlqobk37PyGod01Hn2wv16HGtuD23PSnbpoTKZmF4/UCKc5//wSHXZNyE3XhNx0LZmer6MtXu2sbtaO6mYda/Fqd41Hu2s8Sk1yqOxYm75xYZGmFbpY+QQg5hBiImTH4dgfibHZbJpe6NYnZTT3hku3z6/PDx3Xh/vr1dDWJUly2G2aVeTWRWMyVZKZGlNhwGazKd+VrHxXsq6ekqdaT6e+OHRcW6ua1NLZo2c/rtCzH1docn66vnFhkW6aPUrZaWdOWQGAGQgxEXBKU++o2GzqDZhZ1BtiaO4dms5un1749JB+vWafWjp7JEnJiXbNL83S/HFZEZ0uCqd8V7Kum16ga6bm60BdqxravHpn11HtqW3R//7bbj321h5dN71A35lXootLM2MqkAGIP4SYCDi5qTfWf3hdUDxSkrSl4ri5hViUt8enFz+r0hNrD6rW0xtc3SmJWjg+W3PHZMiZYM0mWYfdpkn56bp13jQ1t3fr9e1H9NfNVdp2uFmvbzui17cd0YTcNH17XoluubAo5s9zAMMTISYCtltgKingotJMSdLeoy063talDC6KNiA+v6FXt1br1+/sDY66FbiTddGYTM0dnaEEx/BpknanJuo780frO/NHa2d1s/68qVKvbj2i/XWt+tnru/TYW3v1tVmFWnrJmOA9uQAgGggxEfBpeaMk6cLRGSZXcn7ZaU6Nz03TgbpWfVrRqGun5ZtdUszbsO+Ylr+5R7v77o2V70rWD64Yp3+4qFgvb6k2ubrImj7KreW3zNTD10/Rq1ur9dzGSu072qoXN1fpxc1VunR8lr63cKwun5gju52pJgCRRYgJs26fX1sqe6dm5o3NNLmagZlXmqkDda3aVEaIOdnpy6Vrmzv1xs4aHajrvSJucqJdX5mYqwXjspRgtw/7AHMyV3Kibl8wRrfNH63Nlcf1nx9X6M2dtfroQIM+OtCgcTkj9L1FY3XLnFGWnVIDEPsIMWG2/XCzOrp9GpmaqIm51rjOxryxWfrzpkP6tKLB7FJiUkeXT2t2H9WmsgYZ6r3Gy/yxmbpiUq5STVomHS0DvTnlJeOyNaXApU8ONuizikYdPNamh1/ZoUff3KNFE7J10ZhMJZ5jio3bGwAIxfD+DmyCTeW9QeDiMZmWGU6f19cXs+uIR57Obpo0+/gNQ1sqj+vtL2vV3uWTJE0vdOm66QXcULEfGalJun5Gga6cnKvNFY368EC9mju69X+312j93mNaNCFbF5dmcVFFAGFDiAmzTWW9/TDzxmaZXMnA5bmSNSYrVRUN7dpc0agrJ+eZXZLpdlY36z/WH9Th4x2SpNx0p/5uZqHG56aZXFnsS050aOGEHM0bm6Utlce1Yd8xNXV0642dtVq/75gWTsjR/NJMObm9AYAhIsSEUY/Pr80VfSGm1Br9MAHzSrNU0dCuTeXxHWI6unz67bv79PSH5fL5DTkT7LpqSp4WjM0K3nQRA5PosGv+2CzNHZOhrYeatH7fMTW2dentL2u1YV/vyMyCsVmEGQAhI8SE0ZdHPGrr8ik9OUFTCqy11HTe2Ey9uLkqOJIUjz7Yf0yPrN6hqsbe0ZcZo9z66swCpteGKMFu10VjMjWnJEPbDjdp3d461bd26Z1dR/XB/notmpCtr11QqLRh3l8EIPz4rhFGJ/fDWO239gXjeqe/th9uUkOrV1lxdGl5T2e3fv76Lr205bCk3uu9/NuN01XX4jW5suHFYbdpTkmGLigeqe2Hm/T+nhNhZuFj7+v7i8bq9gWjY/au7wBiDx12YfTJwd4QY5Wl1ScrcKdo+iiX/Ib03u46s8uJmg/31+u632zQS1sOy2aT7rhkjNYsu1xXT43fKbVIs9tsuqA4Qw9cPVF/P7dI2WlJamrv1i/f3qtF/99a/XHtAbV0dptdJgALYCQmTNq8PfqoL8QsmpBjcjWhuXZqvnZWe/T2l7X6+4uKzS4notq7evTom3u06pNKSb139P71/5iluWOsF0CtKhBmZhaNVJozQb9/b7/K6tv0y7f36qkNZfqfC0t12/zRXEUawFkRYsJk/b5j6urxqyQzVZPzrXF9mNMtnpavX6/Zpw8O1KvV2zOsehROvt5JZUOb/rrlsBr77jI9rzRTS6YXaN/RVu072mpWiXHLbrPpptmjdMOsQr2+7UgwzDy+Zp+eWHdAX59TpP+5sFRjc1gZBuBUw+enlMne/rJWknTttDzL3tl3Yl5acKn1hn3HdP2MArNLCqtun1/v7j6qD/fXy1DvjRpvmTNKEyxyUcLhzmE/EWb+7/YjempDmb484tGfNx3Snzcd0tVTcvWd+aO1aEKO5XrOAEQGISYMunr8en9Pbx+JlS/bb7PZtHhavp7aUKa3v6wdViGm+niH/rqlKtisO6dkpL46o1ApSSzvjQX9XRn41otLVF7fpg8P1GtPbYve3V2nd3fXaWRKoi4cnaGf3zRdo0ammFAtgFhBiAmDjWUNaunsUXaaU3NKYv+mj+dy7bQ8PbWhTO/vqVNXj9/yV1ft9vn1x7UHtGL9AfkNaYQzQTdfMIq7LVuAzWbT2Jw0jc1JU32LV5+UN+iLQ01q6ujWe3vq9P5j72vh+Gx9bVahFk/LlzuFVU1AvCHEhEFgKumaqXmWudXA2cwuzlBOulPHWrx6d/dRS4/G7Kn16Ef/tU1fHum92/T0QpduvGCURgyjXp94kZ3u1A0zC3XdtHx9ecSjzRWNKqtv0wf76/XB/nr9y+qdumxijm6Y1XvbA5ZpA/GB7+ZD1OPz651dRyVJ10237lRSgN1u0zcvKtb/ef+AVn1SYckQ0+Pz68kNZfrtu/vU7TPkTknUddPyNbPIbdl+JfRKdNh1QfFIXVA8UpeOz9JrXxzR69uPaN/RVr27+6je3X1UiQ6b5o/N0lWTc3XVlDwVZ6aaXTaACCHEDNG7u+t0rMWrjNRELbDQ/ZLO5dZ5JXpi3UFtLGvU3toWTbLQaqsDdS360V+3a1tVkyTp6im5+vebZ+jdOLr2Tbz46ECDstKcuuOSUtV6OrXjcJN2VDervrUrOELzs9d3Kd+VrMn56Zpc4NL/unYSTcHAMEKIGaL//LhCkvTNi0ss3z8SUOBO0bXT8vTGjlqt+qRCv7h5htklnZfPb+iZD8v0q3f2qavHr/TkBP30hmn6+pxRjL7EgXxXsvKn5uuaqfmqb/Fqd61Hu2taVNnQplpPp2o9nVq375he+OyQFk3I0eUTc3TZhGzlupLNLh3AEBBihmBvbYs+KWuQ3SZ9Z/5os8sJq9sXjNEbO2r1yufV+vF1k2O6aXLH4Wb9y6s7tP1wsyTpsok5euzrM1TgZuVKPMpOd2pReo4WTchRe1eP9h1t0e6aFu072qKm9m69vu2IXt92RJI0pcClyyZm6/KJOZo7OnPY/CICxAtCzBCs+qRCkrR4av6wW+o5rzRTk/LStfdoi1Z9XKH7r5pgdkln8HR269dv79X/v7FSfkNKdyboka9O0TcvKmb0BZKk1KQEXVCcoQuKM+TzG5pckK4N+45p/b5j2lHdrN01Hu2u8ejJ9WVKTXLoknFZvaM0E3M0OmuE2eUDOA9CTIiaO7r1yufVkqSll4wxt5gIsNls+sEV4/SPL3yhJ9Yd1NcvLFJhjAS15zZWavvhZr25o0Yt3h5J0swit66fUSDDkP7yaZXJFSIWOew27T/aqgJ3ir55UYn+bmaPDtS1av/RFu2va1Wrtyd4LRpJyhqRpAl5afr+orGaPzaLVW1ADOJfZYj+uPaAOrp9mpSXrvkWvOHjQHxtVqGe21ipzyqO6xdv7NYfb51jdkn66EC9Vqw7qOqmDkm9P2huvGCUxudySXoMTpozIbjSyW8Yqm3u1P6jLdpX16rKhjY1tHWpoaxRG8saleiw6aIxmbpsYm8/zeT8dEb7gBhgMwzDMLuIUHk8HrndbjU3N8vlit7Fyw7Utei6336gHr+hlXdepCsm5Ubts/u7sun53DqvJOTP23XEo7/7Px/Ib0jPf3+eLhmXHfJ7DcXO6mY99tYefbC/XpKUlGDXZRNytGhCthId9DEgvDq7fSo71qZ9dS060tShw8c7Tnk+O82pS8Zl6dLxWbpkXDbLuIFBCtfPb0ZiBskwDP3stV3q8Ru6ekpuVAOMGaYWuvSd+aO16pNKLXtxm16991Llu6OzosMwDG0qb9TTH5Tr3d291+JJdNg0d0ymrpiUO6xuUInYkpzo0NRCl6YWuvSti4tVXt8W7KX5pKxB9a1evbbtiF7raxAenZWqS8ZlB0NNJnfeBqKCkZhBem3bEf3wL1uVlGDXmn+6LOrNf9EeiZF6G2hv/uNHOnisTVMKXPrr3QsiGiC6evz6244jeubDcu2s9gS333RBoZZdM0kfHqiP2GcD59Pj8+vQ8XYdrGvTwWOtOny8Xf7TvotOLXD1Bprx2bp4TCb9NMBpGIkxwfbDTfrxS9skSXdfPi5uVi+4khP17J0X6+YnPtLuGo++/5+b9cS35ygjjL9tGoahL6qa9Nq2I3p9W43qW3tv1JicaNfX5xTpuwtLNS6nr+/lQNg+Fhi0BIddY7PTNDY7TdcoT53dPlXUt+nAsVYdPNaqox6vdtV4tKvGoz99UC6H3aYpBemaOzpTc8dkaO7ozKiNZgLDHSMxA3T4eLtufuJjHWvx6vKJOXpm6VwlmNCLYcZITMC2qiZ986mN6uj2Kd+VrN9/a7YuLg29qbnH59eO6ma9t7tOr207okON7cHnctKdWrpgtG6dN/qMoflQjgEQLS2d3cp3J+ujA/X66EBDsAn9ZEUZKZo7OkMzi0ZqWqFLUwpdcnG/J8SRcP38JsQMwPbDTbrnuc9V3dShyfnp+uvdC0y7wZyZIUbqbfS97/nPVVbfJkn6yqQc3XlpqS4Zl3XeBtuOLp/21Hr0aXmjPilr0GfljWrr8gWfT0l06JqpefrarEJdNjHnrBceI8TASprau1TZ2K7KhvbeKwg3d6q/b7olmamaVujS1ILeXpxxOWkqykgx5ZclINKGTYh54okn9Mtf/lI1NTWaNm2afvvb32rRokUDem2kQ0y3z68/b6zUv7+xR10+v0Znpeov359v6vVSzA4xktTm7dHPXvtSL31+WIGzx5lg18wit0oyRygjNVGJCXZ1dvvU0tmjmube1R2HGtt1+tmWkuhQafYIzRjl1pQCF1dMxbDX2e1T1fF2HWpo15GmDtU0d6qpo7vffRPsNhVnpqo0e4SKM1KU507uvcWCK1m5rmTlu5NpcIclDYsQ8+KLL+q2227TE088oUsvvVRPPvmknn76ae3atUslJef/wRupENPt8+uVzw/rj2sPBqc4rp6Sp1///SzTL78fCyEmoLKhTSs/qtCrX1Srqb3/b8Kny05L0qyikVowLkvzx2bpi6om2bneBuJcu7dHUwtd+vJIby/N7hqPKhra1NntP+9r05wJyhyRpJGpiXKnJGpkapJGpiQGH6c5E5TqTNCIJIdGOBM0IilBqU5H7/Ykh1KTErgpJqJuWISYefPmac6cOVqxYkVw25QpU3TTTTdp+fLl5319pELMb9bs0+/e2y+p92Jq9185XrcvGCN7DPxDj6UQE2AYhsrq27Ri3UG1dPaovatHfr+hBIddzgS73Cm930xzXfzWCAyU3zDU0tmj+lav6lu9amrvlqejW57Obnk6euTp7Ja35/whZyCSE+19oSYQbBxKTnTImWA/63+d/WxPdNjksNvlsEsOu10JdpvsNpsSHDY57DY5bL3/TXCc+LPDbpNNNgV+l7HbbbJJstkU3G6TpNMe22y9+9n7Npy+/eTX66THdttp+/BLlCksvzqpq6tLW7Zs0UMPPXTK9sWLF+vjjz/u9zVer1derzf4uLm594Z/Ho+n3/1D9bUpI/Xix359e16J/sfcYqUmJai1tSWsnxGq9rbB1/H0+19GoJIzTc5K0NlPKUPq6VB7T1RKAYaFREkFqVJBapKkM1cDdnX75OnsUUe3Tx1dvV+d3T61d/f9t8unbp9f3h6/unr86u7xq8vnl9fX++fA0vB2r9TeGtW/WszpLwTplEB05j4KPO4LS/aTX3tK4Dr9cW9wstv7D2bqp5bgfn2vtZ322TadWu8Zf79+/879B7jTty4Yl6UfXDH+/AdxEAI/t4c6jmJaiKmvr5fP51NeXt4p2/Py8lRbW9vva5YvX65//dd/PWN7cXFxRGrcJOmHEXlnAACs4TVJD0fovVtaWuR2u0N+velj+6cnQcMwzpoOH374YS1btiz42O/3q7GxUVlZWXExJOjxeFRcXKyqqqqo3mbBKjg+58bxOTeOz7lxfM6N43N+Jx+j9PR0tbS0qLCwcEjvaVqIyc7OlsPhOGPUpa6u7ozRmQCn0ymn03nKtpEjR0aqxJjlcrn4R3IOHJ9z4/icG8fn3Dg+58bxOb/AMRrKCEyAaetZk5KSdOGFF2rNmjWnbF+zZo0uueQSk6oCAABWYep00rJly3Tbbbdp7ty5WrBggZ566ikdOnRId999t5llAQAACzA1xPzDP/yDGhoa9POf/1w1NTWaPn263njjDY0ePdrMsmKW0+nUT3/60zOm1NCL43NuHJ9z4/icG8fn3Dg+5xeJY2T6FXsBAABCwTXeAQCAJRFiAACAJRFiAACAJRFiAACAJRFiYswTTzyh0tJSJScn68ILL9QHH3xwzv3Xr1+vCy+8UMnJyRo7dqz+4z/+I0qVmmMwx2fdunW99x457WvPnj1RrDh6NmzYoBtuuEGFhYWy2Wx69dVXz/uaeDp/Bnt84un8Wb58uS666CKlp6crNzdXN910k/bu3Xve18XL+RPK8Ymn80eSVqxYoZkzZwYvZLdgwQK9+eab53xNOM4fQkwMefHFF/XAAw/oX/7lX7R161YtWrRIS5Ys0aFD/d+5ury8XNdff70WLVqkrVu36pFHHtEPf/hDvfzyy1GuPDoGe3wC9u7dq5qamuDXhAkTolRxdLW1tWnWrFn6wx/+MKD94+38GezxCYiH82f9+vW69957tXHjRq1Zs0Y9PT1avHix2trazvqaeDp/Qjk+AfFw/khSUVGRHn30UW3evFmbN2/WlVdeqRtvvFFfftn/DYjDdv4YiBkXX3yxcffdd5+ybfLkycZDDz3U7/4//vGPjcmTJ5+y7a677jLmz58fsRrNNNjjs3btWkOScfz48ShUF1skGatXrz7nPvF2/pxsIMcnns+furo6Q5Kxfv36s+4Tz+fPQI5PPJ8/ARkZGcbTTz/d73PhOn8YiYkRXV1d2rJlixYvXnzK9sWLF+vjjz/u9zWffPLJGftfe+212rx5s7q7uyNWqxlCOT4Bs2fPVkFBga666iqtXbs2kmVaSjydP0MRj+dPc3OzJCkzM/Os+8Tz+TOQ4xMQj+ePz+fTCy+8oLa2Ni1YsKDffcJ1/hBiYkR9fb18Pt8ZN7/My8s74yaZAbW1tf3u39PTo/r6+ojVaoZQjk9BQYGeeuopvfzyy3rllVc0adIkXXXVVdqwYUM0So558XT+hCJezx/DMLRs2TItXLhQ06dPP+t+8Xr+DPT4xOP5s2PHDqWlpcnpdOruu+/W6tWrNXXq1H73Ddf5Y+ptB3Amm812ymPDMM7Ydr79+9s+XAzm+EyaNEmTJk0KPl6wYIGqqqr0q1/9SpdddllE67SKeDt/BiNez5/77rtP27dv14cffnjefePx/Bno8YnH82fSpEn64osv1NTUpJdffllLly7V+vXrzxpkwnH+MBITI7Kzs+VwOM4YVairqzsjrQbk5+f3u39CQoKysrIiVqsZQjk+/Zk/f772798f7vIsKZ7On3AZ7ufP/fffr9dee01r165VUVHROfeNx/NnMMenP8P9/ElKStL48eM1d+5cLV++XLNmzdLvfve7fvcN1/lDiIkRSUlJuvDCC7VmzZpTtq9Zs0aXXHJJv69ZsGDBGfu/8847mjt3rhITEyNWqxlCOT792bp1qwoKCsJdniXF0/kTLsP1/DEMQ/fdd59eeeUVvf/++yotLT3va+Lp/Anl+PRnuJ4/Z2MYhrxeb7/Phe38GVyvMSLphRdeMBITE41nnnnG2LVrl/HAAw8YI0aMMCoqKgzDMIyHHnrIuO2224L7l5WVGampqcY//dM/Gbt27TKeeeYZIzEx0XjppZfM+itE1GCPz29+8xtj9erVxr59+4ydO3caDz30kCHJePnll836K0RUS0uLsXXrVmPr1q2GJOPxxx83tm7dalRWVhqGwfkz2OMTT+fPPffcY7jdbmPdunVGTU1N8Ku9vT24TzyfP6Ecn3g6fwzDMB5++GFjw4YNRnl5ubF9+3bjkUceMex2u/HOO+8YhhG584cQE2P++Mc/GqNHjzaSkpKMOXPmnLKEb+nSpcbll19+yv7r1q0zZs+ebSQlJRljxowxVqxYEeWKo2swx+exxx4zxo0bZyQnJxsZGRnGwoULjb/97W8mVB0dgSWdp38tXbrUMAzOn8Een3g6f/o7LpKMlStXBveJ5/MnlOMTT+ePYRjGd7/73eD35pycHOOqq64KBhjDiNz5YzOMvk4aAAAAC6EnBgAAWBIhBgAAWBIhBgAAWBIhBgAAWBIhBgAAWBIhBgAAWBIhBgAAWBIhBgAAWBIhBgAAWBIhBgAAWBIhBgAAWBIhBgAAWNL/A8Lr6WHn/088AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "sns.distplot(samples)\n",
    "g = lambda t: tfd.Gamma(\n",
    "                concentration=t[...,0], rate=t[...,1])\n",
    "\n",
    "g([[0.01,2.,10.,10.]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-15 14:33:11.081407: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-15 14:33:11.083836: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    }
   ],
   "source": [
    "A  = 2*tfd.Uniform().sample(1000)\n",
    "X  = tfd.InverseGamma(concentration =1.5, scale = 0.6 ).sample(1000)\n",
    "R1 = A*X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0.47068053, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(tf.sort(R1)[500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ber = tfd.Bernoulli(probs=0.3, dtype ='float').sample(1000) \n",
    "IG1 = tfd.InverseGamma(concentration =1.8, scale = 1 ).sample(1000)\n",
    "IG2 = tfd.InverseGamma(concentration =1.8, scale = 1 ).sample(1000)\n",
    "R2 = IG1*Ber +(6+IG2)*(1-Ber)\n",
    "\n",
    "train_dataset = R2[:250]\n",
    "train_dataset = tf.reshape(train_dataset,[250,1])\n",
    "eval_dataset = R2[250:]\n",
    "eval_dataset=tf.reshape(eval_dataset,[750,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_14732/4270754263.py:2: UserWarning: \n",
      "\n",
      "`distplot` is a deprecated function and will be removed in seaborn v0.14.0.\n",
      "\n",
      "Please adapt your code to use either `displot` (a figure-level function with\n",
      "similar flexibility) or `histplot` (an axes-level function for histograms).\n",
      "\n",
      "For a guide to updating your code to use the new functions, please see\n",
      "https://gist.github.com/mwaskom/de44147ed2974457ad6372750bbe5751\n",
      "\n",
      "  sns.distplot(Y_sort)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Y_sort=tf.sort(Y)\n",
    "sns.distplot(Y_sort)\n",
    "plt.title(\"Samples from Y\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Model with known prior\n",
    "class Encoder(tfk.Model):\n",
    "    \n",
    "    def __init__(self):      \n",
    "        super(Encoder,self).__init__()      \n",
    "        self.prior        = tfd.Gamma(concentration=1.5,rate = 1)\n",
    "        self.dense1       = tfkl.Dense(5, activation ='relu',kernel_initializer =tfk.initializers.Zeros())\n",
    "        self.dense2       = tfkl.Dense(5, activation ='relu',kernel_initializer =tfk.initializers.Zeros())\n",
    "        self.dense3       = tfkl.Dense(2, activation='relu',bias_initializer = tfk.initializers.RandomUniform(minval=1, maxval=2))\n",
    "        self.lambda1      = tfkl.Lambda(lambda x: tf.abs(x)+0.001, name='posterior_params')\n",
    "        self.dist_lambda1 = tfpl.DistributionLambda(\n",
    "                            make_distribution_fn=lambda t: tfd.Gamma(\n",
    "                                concentration=t[...,0], rate = t[...,1]),\n",
    "                                    activity_regularizer=tfpl.KLDivergenceRegularizer(self.prior,use_exact_kl =True))  \n",
    "        \n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x = self.dense1(inputs)\n",
    "        x = self.dense2(x)\n",
    "        x = self.dense3(x)\n",
    "        x = self.lambda1(x)\n",
    "        x = self.dist_lambda1(x)\n",
    "        return x\n",
    "    \n",
    "class Decoder(tfk.Model):\n",
    "    def __init__(self):\n",
    "        super(Decoder,self).__init__()\n",
    "        self.dense1       = tfkl.Dense(5, use_bias=True, activation='relu')\n",
    "        self.lambda1      = tfkl.Lambda(lambda x: 1/x)\n",
    "        self.dense2       = tfkl.Dense(5, use_bias=True, activation='relu')\n",
    "        self.dense31      = tfkl.Dense(1, use_bias=True,bias_initializer = tfk.initializers.RandomUniform(minval=1, maxval=2))\n",
    "        self.dense32      = tfkl.Dense(1, use_bias=True,bias_initializer = tfk.initializers.RandomUniform(minval=1, maxval=2), kernel_initializer = tfk.initializers.RandomUniform(minval=0.1, maxval=2))\n",
    "        self.dense3       = tfkl.Dense(2, activation='relu',bias_initializer = tfk.initializers.RandomUniform(minval=1, maxval=2))\n",
    "        self.lambda2      = tfkl.Lambda(lambda x: tf.abs(x)+0.00001)\n",
    "        self.concat1      = tfkl.Concatenate()\n",
    "        self.dist_lambda1 = tfpl.DistributionLambda(\n",
    "            make_distribution_fn=lambda t: tfd.Gamma(\n",
    "                concentration=t[...,0], rate=t[...,1]))\n",
    "        self.dist_n       = tfpl.IndependentNormal(1)\n",
    "        self.ind_norm1 = tfpl.DistributionLambda(\n",
    "                            make_distribution_fn=lambda v: tfd.TruncatedNormal(\n",
    "                                loc=v[...,0], scale=v[...,1],low = 0,high = 1000))\n",
    "        \n",
    "        \n",
    "    def call(self, inputs):\n",
    "        y     = tfkl.Reshape(target_shape=[1])(inputs)\n",
    "        y1    = self.lambda1(y)\n",
    "        x     = self.dense1(y1)\n",
    "        x     = self.dense2(x)\n",
    "        \n",
    "        #for gamma output :       \n",
    "        alpha = self.dense31(x*y)\n",
    "        alpha = self.lambda1(alpha)\n",
    "        beta  = self.dense32(x)\n",
    "        beta  = self.lambda2(beta*y**2)\n",
    "        x     = self.concat1([alpha,beta])                \n",
    "        x     = self.dist_lambda1(x)\n",
    "        \n",
    "        #normal output or truncated normal:\n",
    "        #mu    = self.dense31(x)\n",
    "        #sig   = self.dense32(x)\n",
    "        #sig   = self.lambda1(sig/(1+y1))\n",
    "        #x     = self.concat1([mu,sig])\n",
    "        #x     = self.dist_n(x)\n",
    "        #x     = self.ind_norm1(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "class Ext_VAE(tfk.Model):\n",
    "    def __init__(self):      \n",
    "        super(Ext_VAE,self).__init__()\n",
    "        self.encoder = Encoder()\n",
    "        self.decoder = Decoder()\n",
    "    \n",
    "    def call(self,inputs):\n",
    "        return self.decoder(self.encoder(inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vae = Ext_VAE()\n",
    "vae.decoder.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "165.50166\n",
      "16.883898\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "seed = 100\n",
    "random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)\n",
    "A = 2*tfd.Uniform().sample(1000)\n",
    "X = tfd.InverseGamma(concentration =1.5, scale = 0.6 ).sample(1000)\n",
    "R1 = A*X\n",
    "\n",
    "random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)\n",
    "train_dataset = R1[:250]\n",
    "train_dataset = tf.reshape(train_dataset,[250,1])\n",
    "eval_dataset = R1[250:]\n",
    "eval_dataset=tf.reshape(eval_dataset,[750,1])\n",
    "print(np.max(R1))\n",
    "print(np.max(R1[:250]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_log_likelihood = lambda x, rv_x: -rv_x.log_prob(x)\n",
    "\n",
    "vae.compile(optimizer=tf.optimizers.Adam(learning_rate=1e-4),\n",
    "            loss=negative_log_likelihood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "8/8 [==============================] - 2s 84ms/step - loss: 0.8884 - val_loss: 1.0874\n",
      "Epoch 2/1000\n",
      "8/8 [==============================] - 0s 63ms/step - loss: 0.9164 - val_loss: 1.0951\n",
      "Epoch 3/1000\n",
      "8/8 [==============================] - 0s 70ms/step - loss: 0.9062 - val_loss: 1.0967\n",
      "Epoch 4/1000\n",
      "8/8 [==============================] - 0s 66ms/step - loss: 0.9051 - val_loss: 1.0989\n",
      "Epoch 5/1000\n",
      "8/8 [==============================] - 0s 56ms/step - loss: 0.8829 - val_loss: 1.1041\n",
      "Epoch 6/1000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 0.8798 - val_loss: 1.0900\n",
      "Epoch 7/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 0.9049 - val_loss: 1.0834\n",
      "Epoch 8/1000\n",
      "8/8 [==============================] - 0s 42ms/step - loss: 0.9002 - val_loss: 1.1079\n",
      "Epoch 9/1000\n",
      "8/8 [==============================] - 0s 43ms/step - loss: 0.8855 - val_loss: 1.0933\n",
      "Epoch 10/1000\n",
      "8/8 [==============================] - 0s 37ms/step - loss: 0.8660 - val_loss: 1.0966\n",
      "Epoch 11/1000\n",
      "8/8 [==============================] - 0s 37ms/step - loss: 0.8756 - val_loss: 1.0878\n",
      "Epoch 12/1000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 0.8877 - val_loss: 1.0645\n",
      "Epoch 13/1000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 0.9071 - val_loss: 1.0829\n",
      "Epoch 14/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 0.8936 - val_loss: 1.0902\n",
      "Epoch 15/1000\n",
      "8/8 [==============================] - 0s 37ms/step - loss: 0.8905 - val_loss: 1.0870\n",
      "Epoch 16/1000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 0.9042 - val_loss: 1.0972\n",
      "Epoch 17/1000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 0.8714 - val_loss: 1.0992\n",
      "Epoch 18/1000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 0.8958 - val_loss: 1.0954\n",
      "Epoch 19/1000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 0.8761 - val_loss: 1.0821\n",
      "Epoch 20/1000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 0.8976 - val_loss: 1.0807\n",
      "Epoch 21/1000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 0.8801 - val_loss: 1.1072\n",
      "Epoch 22/1000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 0.9148 - val_loss: 1.0977\n",
      "Epoch 23/1000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 0.8850 - val_loss: 1.1088\n",
      "Epoch 24/1000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 0.9121 - val_loss: 1.0922\n",
      "Epoch 25/1000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 0.8863 - val_loss: 1.1109\n",
      "Epoch 26/1000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 0.8831 - val_loss: 1.0864\n",
      "Epoch 27/1000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 0.8882 - val_loss: 1.0781\n",
      "Epoch 28/1000\n",
      "8/8 [==============================] - 0s 39ms/step - loss: 0.8878 - val_loss: 1.1182\n",
      "Epoch 29/1000\n",
      "8/8 [==============================] - 1s 71ms/step - loss: 0.9171 - val_loss: 1.0907\n",
      "Epoch 30/1000\n",
      "8/8 [==============================] - 0s 66ms/step - loss: 0.8892 - val_loss: 1.0964\n",
      "Epoch 31/1000\n",
      "8/8 [==============================] - 0s 62ms/step - loss: 0.8785 - val_loss: 1.1271\n",
      "Epoch 32/1000\n",
      "8/8 [==============================] - 0s 66ms/step - loss: 0.8817 - val_loss: 1.1261\n",
      "Epoch 33/1000\n",
      "8/8 [==============================] - 0s 64ms/step - loss: 0.8937 - val_loss: 1.0976\n",
      "Epoch 34/1000\n",
      "8/8 [==============================] - 1s 73ms/step - loss: 0.8813 - val_loss: 1.1156\n",
      "Epoch 35/1000\n",
      "8/8 [==============================] - 0s 66ms/step - loss: 0.8661 - val_loss: 1.1232\n",
      "Epoch 36/1000\n",
      "8/8 [==============================] - 0s 63ms/step - loss: 0.8939 - val_loss: 1.1017\n",
      "Epoch 37/1000\n",
      "8/8 [==============================] - 1s 75ms/step - loss: 0.8799 - val_loss: 1.0740\n",
      "Epoch 38/1000\n",
      "8/8 [==============================] - 0s 38ms/step - loss: 0.8705 - val_loss: 1.1066\n",
      "Epoch 39/1000\n",
      "8/8 [==============================] - 0s 36ms/step - loss: 0.8982 - val_loss: 1.1086\n",
      "Epoch 40/1000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 0.8720 - val_loss: 1.1127\n",
      "Epoch 41/1000\n",
      "8/8 [==============================] - 0s 25ms/step - loss: 0.8816 - val_loss: 1.0738\n",
      "Epoch 42/1000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 0.8907 - val_loss: 1.1102\n",
      "Epoch 43/1000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 0.8932 - val_loss: 1.1078\n",
      "Epoch 44/1000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 0.8662 - val_loss: 1.0877\n",
      "Epoch 45/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 0.8917 - val_loss: 1.1059\n",
      "Epoch 46/1000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 0.8744 - val_loss: 1.1144\n",
      "Epoch 47/1000\n",
      "8/8 [==============================] - 0s 21ms/step - loss: 0.9032 - val_loss: 1.1417\n",
      "Epoch 48/1000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 0.8930 - val_loss: 1.1313\n",
      "Epoch 49/1000\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 0.8796 - val_loss: 1.1257\n",
      "Epoch 50/1000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 0.9039 - val_loss: 1.0721\n",
      "Epoch 51/1000\n",
      "8/8 [==============================] - 0s 51ms/step - loss: 0.8933 - val_loss: 1.0896\n",
      "Epoch 52/1000\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.6595"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [66]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mvae\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\n\u001b[1;32m      5\u001b[0m \u001b[43m       \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/tf/lib/python3.9/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.conda/envs/tf/lib/python3.9/site-packages/keras/engine/training.py:1606\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1591\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_eval_data_handler\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1592\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_eval_data_handler \u001b[38;5;241m=\u001b[39m data_adapter\u001b[38;5;241m.\u001b[39mget_data_handler(\n\u001b[1;32m   1593\u001b[0m         x\u001b[38;5;241m=\u001b[39mval_x,\n\u001b[1;32m   1594\u001b[0m         y\u001b[38;5;241m=\u001b[39mval_y,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1604\u001b[0m         steps_per_execution\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_steps_per_execution,\n\u001b[1;32m   1605\u001b[0m     )\n\u001b[0;32m-> 1606\u001b[0m val_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1607\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_x\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1608\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_y\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1609\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_sample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1610\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_batch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1611\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1612\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1613\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_queue_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_queue_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1614\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1615\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_multiprocessing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_multiprocessing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1616\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1617\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_use_cached_eval_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1618\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1619\u001b[0m val_logs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m   1620\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m name: val \u001b[38;5;28;01mfor\u001b[39;00m name, val \u001b[38;5;129;01min\u001b[39;00m val_logs\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m   1621\u001b[0m }\n\u001b[1;32m   1622\u001b[0m epoch_logs\u001b[38;5;241m.\u001b[39mupdate(val_logs)\n",
      "File \u001b[0;32m~/.conda/envs/tf/lib/python3.9/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.conda/envs/tf/lib/python3.9/site-packages/keras/engine/training.py:1939\u001b[0m, in \u001b[0;36mModel.evaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m   1937\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_test_counter\u001b[38;5;241m.\u001b[39massign(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m   1938\u001b[0m callbacks\u001b[38;5;241m.\u001b[39mon_test_begin()\n\u001b[0;32m-> 1939\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, iterator \u001b[38;5;129;01min\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39menumerate_epochs():  \u001b[38;5;66;03m# Single epoch.\u001b[39;00m\n\u001b[1;32m   1940\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreset_metrics()\n\u001b[1;32m   1941\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mcatch_stop_iteration():\n",
      "File \u001b[0;32m~/.conda/envs/tf/lib/python3.9/site-packages/keras/engine/data_adapter.py:1307\u001b[0m, in \u001b[0;36mDataHandler.enumerate_epochs\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1305\u001b[0m \u001b[38;5;124;03m\"\"\"Yields `(epoch, tf.data.Iterator)`.\"\"\"\u001b[39;00m\n\u001b[1;32m   1306\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_truncate_execution_to_epoch():\n\u001b[0;32m-> 1307\u001b[0m     data_iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1308\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initial_epoch, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_epochs):\n\u001b[1;32m   1309\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_insufficient_data:  \u001b[38;5;66;03m# Set by `catch_stop_iteration`.\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/tf/lib/python3.9/site-packages/tensorflow/python/data/ops/dataset_ops.py:499\u001b[0m, in \u001b[0;36mDatasetV2.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    497\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m context\u001b[38;5;241m.\u001b[39mexecuting_eagerly() \u001b[38;5;129;01mor\u001b[39;00m ops\u001b[38;5;241m.\u001b[39minside_function():\n\u001b[1;32m    498\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mcolocate_with(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variant_tensor):\n\u001b[0;32m--> 499\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43miterator_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mOwnedIterator\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    501\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`tf.data.Dataset` only supports Python-style \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    502\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miteration in eager mode or within tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.conda/envs/tf/lib/python3.9/site-packages/tensorflow/python/data/ops/iterator_ops.py:696\u001b[0m, in \u001b[0;36mOwnedIterator.__init__\u001b[0;34m(self, dataset, components, element_spec)\u001b[0m\n\u001b[1;32m    692\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m (components \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m element_spec \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    693\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    694\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhen `dataset` is provided, `element_spec` and `components` must \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    695\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnot be specified.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 696\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_next_call_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m~/.conda/envs/tf/lib/python3.9/site-packages/tensorflow/python/data/ops/iterator_ops.py:721\u001b[0m, in \u001b[0;36mOwnedIterator._create_iterator\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    716\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mcolocate_with(ds_variant):\n\u001b[1;32m    717\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator_resource \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    718\u001b[0m       gen_dataset_ops\u001b[38;5;241m.\u001b[39manonymous_iterator_v3(\n\u001b[1;32m    719\u001b[0m           output_types\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_output_types,\n\u001b[1;32m    720\u001b[0m           output_shapes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_output_shapes))\n\u001b[0;32m--> 721\u001b[0m   \u001b[43mgen_dataset_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mds_variant\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_iterator_resource\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/tf/lib/python3.9/site-packages/tensorflow/python/ops/gen_dataset_ops.py:3409\u001b[0m, in \u001b[0;36mmake_iterator\u001b[0;34m(dataset, iterator, name)\u001b[0m\n\u001b[1;32m   3407\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tld\u001b[38;5;241m.\u001b[39mis_eager:\n\u001b[1;32m   3408\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3409\u001b[0m     _result \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_FastPathExecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3410\u001b[0m \u001b[43m      \u001b[49m\u001b[43m_ctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMakeIterator\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3411\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[1;32m   3412\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "vae.fit(train_dataset,train_dataset, \n",
    "        validation_data=(eval_dataset,eval_dataset), \n",
    "        batch_size=32,\n",
    "        epochs=1000\n",
    "       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(10.197591, shape=(), dtype=float32)\n",
      "tf.Tensor(0.002986619, shape=(), dtype=float32)\n",
      "tf.Tensor([2.2816095  0.6936851  0.16705951 ... 0.7811353  1.9303243  4.2003145 ], shape=(10000,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "N_samples = 10000\n",
    "prior_samples = vae.encoder.prior.sample(N_samples)\n",
    "print(tf.reduce_max(prior_samples))\n",
    "print(tf.reduce_min(prior_samples))\n",
    "samples_vae = vae.decoder(prior_samples).sample()\n",
    "print(samples_vae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Samples from R1\n",
    "N_samples = 10000\n",
    "A = 2*tfd.Uniform().sample(N_samples)\n",
    "X = tfd.InverseGamma(concentration =1.5, scale = 0.6 ).sample(N_samples)\n",
    "test_set = A*X\n",
    "test_set =test_set.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(1.6301888, shape=(), dtype=float32)\n",
      "tf.Tensor(1.6301888, shape=(), dtype=float32)\n",
      "tf.Tensor(\n",
      "[2.9866190e-03 3.1623163e-03 7.8775650e-03 ... 9.4497108e+00 9.7904654e+00\n",
      " 1.0197591e+01], shape=(10000,), dtype=float32)\n",
      "tf.Tensor(0.5525508, shape=(), dtype=float32)\n",
      "tf.Tensor(0.00013467361, shape=(), dtype=float32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f47d9175a90>]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "U = vae.encoder(tf.expand_dims(np.arange(1,1000,1),-1)).concentration\n",
    "print(U[898])\n",
    "print(U[998])\n",
    "print(tf.sort(prior_samples))\n",
    "c_vae = vae.decoder((np.arange(0.01,10,0.01,dtype = float))).concentration\n",
    "print(c_vae[0])\n",
    "plt.plot(c_vae)\n",
    "plt.figure()\n",
    "r_vae = vae.decoder((np.arange(0.0001,0.1,0.001,dtype = float))).rate\n",
    "print(r_vae[0])\n",
    "plt.plot(r_vae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f47f7daa4c0>]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(samples_vae.shape)\n",
    "plt.figure()\n",
    "plt.title('log-QQplot')\n",
    "plt.scatter(tf.math.log(tf.sort(test_set[:])),tf.math.log(tf.sort(samples_vae)[:]))\n",
    "plt.plot(tf.math.log(tf.sort(test_set)[:]),tf.math.log(tf.sort(test_set)[:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 4ms/step - loss: 1.1204\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.1204229593276978"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test = tf.reshape(test_set,(N_samples,1))\n",
    "negative_log_likelihood = lambda x, rv_x: -rv_x.log_prob(x)\n",
    "vae.compile(optimizer=tf.optimizers.Adam(learning_rate=1e-4),\n",
    "            loss=negative_log_likelihood)\n",
    "vae.evaluate(data_test,data_test,batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f47f7cb7400>]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from statsmodels.distributions.empirical_distribution import ECDF\n",
    "def tail_mod(f,n,x):\n",
    "    return((n/n+1)*(1-f(x))+1/(n+1))\n",
    "\n",
    "\n",
    "\n",
    "def L(X,Y,u):\n",
    "    Nn = np.sum(X>u)\n",
    "    m=Y.shape[0]\n",
    "    fY=ECDF(Y)\n",
    "    Xmax = np.maximum(X,u)\n",
    "    Xu = np.minimum(Xmax,u)\n",
    "    res = 1 + 1/Nn*(np.sum(np.log(tail_mod(fY,m,Xmax)/tail_mod(fY,m,Xu))))\n",
    "    return(res)\n",
    "    \n",
    "def K(X,Y,u):\n",
    "    res = -L(X,Y,u)-L(Y,X,u)\n",
    "    return res\n",
    "X = np.arange(0,1,0.01)\n",
    "Ind = X*10000\n",
    "Ind = Ind.astype('int64')\n",
    "\n",
    "Q = np.sort(samples_vae)[Ind]\n",
    "#Q = np.sort(test_set)[Ind]\n",
    "\n",
    "Y = [K(test_set,samples_vae,qi) for qi in Q]\n",
    "plt.figure()\n",
    "plt.title(\"KL divergence upon threshold u\")\n",
    "plt.xlabel(\"u\")\n",
    "plt.ylabel(\"KL upon u\")\n",
    "plt.plot(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae.save_weights(\"Ext_VAE_custom_outputG_fig3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Model for bivariate distribution :\n",
    "class Multi_Encoder(tfk.Model):\n",
    "    \n",
    "    def __init__(self):      \n",
    "        super(Multi_Encoder,self).__init__()      \n",
    "        self.prior        = tfd.Gamma(concentration=1.8,rate = 1)\n",
    "        self.dense1       = tfkl.Dense(5, activation ='relu',kernel_initializer =tfk.initializers.Zeros())\n",
    "        self.dense2       = tfkl.Dense(5, activation ='relu',kernel_initializer =tfk.initializers.Zeros())\n",
    "        self.dense3       = tfkl.Dense(2, activation='relu',bias_initializer = tfk.initializers.RandomUniform(minval=1, maxval=2))\n",
    "        self.lambda1      = tfkl.Lambda(lambda x: tf.abs(x)+0.001, name='posterior_params')\n",
    "        self.dist_lambda1 = tfpl.DistributionLambda(\n",
    "                            make_distribution_fn=lambda t: tfd.Gamma(\n",
    "                                concentration=t[...,0], rate = t[...,1]),\n",
    "                                    activity_regularizer=tfpl.KLDivergenceRegularizer(self.prior,use_exact_kl =True))  \n",
    "        \n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x = self.dense1(inputs)\n",
    "        x = self.dense2(x)\n",
    "        x = self.dense3(x)\n",
    "        x = self.lambda1(x)\n",
    "        x = self.dist_lambda1(x)\n",
    "        return x\n",
    "    \n",
    "class Multi_Decoder(tfk.Model):\n",
    "    def __init__(self):\n",
    "        super(Multi_Decoder,self).__init__()\n",
    "        self.num_components = 2\n",
    "        self.ev_shape       = [1]\n",
    "        self.params_size    = tfpl.MixtureSameFamily.params_size(self.num_components,\n",
    "                                                 component_params_size=tfpl.IndependentNormal.params_size(self.ev_shape))\n",
    "        self.lambda1        = tfkl.Lambda(lambda x: 1/x)\n",
    "        self.dense1         = tfkl.Dense(8, use_bias=True, activation='relu')\n",
    "        self.dense2         = tfkl.Dense(8, use_bias=True, activation='relu')\n",
    "        self.dense3         = tfkl.Dense(self.params_size,activation = 'relu')\n",
    "        self.dense41        = tfkl.Dense(2, \n",
    "                                         bias_initializer = tfk.initializers.RandomUniform(minval=-0.1, maxval=0.1))\n",
    "        self.dense421       = tfkl.Dense(1, \n",
    "                                         bias_initializer = tfk.initializers.RandomUniform(minval=1, maxval=2))\n",
    "        self.dense422       = tfkl.Dense(1, \n",
    "                                         bias_initializer = tfk.initializers.RandomUniform(minval=0.5, maxval=2))\n",
    "        self.dense431       = tfkl.Dense(1, \n",
    "                                         bias_initializer = tfk.initializers.RandomUniform(minval=5, maxval=10))\n",
    "        self.dense432       = tfkl.Dense(1, \n",
    "                                         bias_initializer = tfk.initializers.RandomUniform(minval=0.5, maxval=2))\n",
    "        self.lambda2        = tfkl.Lambda(lambda x: tf.abs(x)+0.001)\n",
    "        self.concat1        = tfkl.Concatenate()\n",
    "        self.mixt_distn1    = tfpl.MixtureSameFamily(self.num_components, tfpl.IndependentNormal(self.ev_shape))\n",
    "        self.mixt_distg1    = tfpl.MixtureSameFamily(self.num_components, \n",
    "                                                     tfpl.DistributionLambda(\n",
    "                                                         make_distribution_fn=lambda t: tfd.Gamma(\n",
    "                                                             concentration=t[...,0], rate=t[...,1])))\n",
    "        self.dense31        = tfkl.Dense(3, use_bias=True)\n",
    "        self.dense32        = tfkl.Dense(3, activation='relu',\n",
    "                                         bias_initializer = tfk.initializers.RandomUniform(minval=0.5, maxval=10))\n",
    "        self.dense33        = tfkl.Dense(3, activation='relu',\n",
    "                                         bias_initializer = tfk.initializers.RandomUniform(minval=1, maxval=2))\n",
    "        #self.lambdan        = tfkl.Lambda(lambda x: tf.abs(x)+0.001)\n",
    "\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        y     = tfkl.Reshape(target_shape=[1])(inputs)\n",
    "        y1    = self.lambda1(y)\n",
    "        x     = self.dense1(y1)\n",
    "        x     = self.dense2(x)\n",
    "        \n",
    "        # normal mixture\n",
    "        x     = self.dense3(x)\n",
    "        m     = self.dense41(x)\n",
    "        #mu1   = self.dense421(x)\n",
    "        #sig1  = self.dense431(x)\n",
    "        #mu2   = self.dense422(x)\n",
    "        #sig2  = self.dense432(x)\n",
    "        #x     = self.concat1([m,mu1,sig1,mu2,sig2])\n",
    "        #x     = self.lambda2(x)\n",
    "        #x     = self.lambda2(x)\n",
    "        #x     = self.mixt_distn1(x)\n",
    "        \n",
    "        # Gamma mixture \n",
    "        #m     = self.dense1(x)\n",
    "        alpha1 = self.dense421(x*y)\n",
    "        beta1  = self.dense431(x*inputs**2)\n",
    "        alpha2 = self.dense422(x*y)\n",
    "        beta2  = self.dense432(x*inputs**2)\n",
    "        #beta  = self.lambda2(beta*inputs**2)\n",
    "        x     = self.concat1([m,alpha,beta])\n",
    "        x     = self.mixt_distg1(x)\n",
    "        return x\n",
    "    \n",
    "class Ext_Multi_VAE(tfk.Model):\n",
    "    def __init__(self):      \n",
    "        super(Ext_Multi_VAE,self).__init__()\n",
    "        self.encoder = Multi_Encoder()\n",
    "        self.decoder = Multi_Decoder()\n",
    "    \n",
    "    def call(self,inputs):\n",
    "        return self.decoder(self.encoder(inputs))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0.1100897  0.04836159]\n",
      " [0.1274148  0.05839857]\n",
      " [0.20579372 0.10380607]\n",
      " [0.10212936 0.04374989]\n",
      " [0.10668668 0.04639011]], shape=(5, 2), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[[1.1181878]\n",
      "  [8.350915 ]]\n",
      "\n",
      " [[1.1051048]\n",
      "  [8.337824 ]]\n",
      "\n",
      " [[1.0459172]\n",
      "  [8.278602 ]]\n",
      "\n",
      " [[1.124199 ]\n",
      "  [8.356929 ]]\n",
      "\n",
      " [[1.1207576]\n",
      "  [8.353486 ]]], shape=(5, 2, 1), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[[0.804346  ]\n",
      "  [0.7410413 ]]\n",
      "\n",
      " [[0.8142844 ]\n",
      "  [0.7339021 ]]\n",
      "\n",
      " [[0.8602325 ]\n",
      "  [0.7021884 ]]\n",
      "\n",
      " [[0.79980624]\n",
      "  [0.7443372 ]]\n",
      "\n",
      " [[0.8024032 ]\n",
      "  [0.7424491 ]]], shape=(5, 2, 1), dtype=float32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([[-0.74354327, -0.45194972,  0.5091895 ,  0.3424754 ,  0.3378036 ,\n",
       "          0.06386858,  0.7238271 , -0.20873505]], dtype=float32),\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32),\n",
       " array([[ 0.38449967, -0.18625155,  0.4458235 , -0.54245096, -0.4876092 ,\n",
       "          0.05015701,  0.06745744, -0.37833086],\n",
       "        [ 0.15975541,  0.60087675, -0.3013177 , -0.42886442,  0.27348244,\n",
       "          0.3199556 , -0.1881164 ,  0.19850498],\n",
       "        [ 0.20304632,  0.03155375, -0.26148978, -0.39657864, -0.4359741 ,\n",
       "          0.13085455,  0.5750963 , -0.22010055],\n",
       "        [ 0.33445233,  0.33363575, -0.5435102 , -0.45731372,  0.3574416 ,\n",
       "         -0.5490353 , -0.03486896,  0.23551297],\n",
       "        [-0.16668144,  0.31845033,  0.15684521,  0.10484433, -0.26986104,\n",
       "          0.5008115 ,  0.36027128, -0.5385275 ],\n",
       "        [-0.32993913,  0.51221853,  0.01312566, -0.30805182,  0.1464619 ,\n",
       "         -0.26687694, -0.24412853, -0.54726183],\n",
       "        [-0.30281216, -0.5104661 ,  0.4739855 ,  0.3500471 , -0.08805102,\n",
       "         -0.60484   , -0.44995558, -0.549453  ],\n",
       "        [ 0.5460122 ,  0.15101057,  0.11658543, -0.51769185,  0.26050472,\n",
       "         -0.08808631, -0.12637013,  0.2985291 ]], dtype=float32),\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32),\n",
       " array([[ 0.3413164 ,  0.32317728,  0.5783026 ,  0.53240585,  0.16741765,\n",
       "         -0.6414752 ],\n",
       "        [-0.6278999 , -0.59520185, -0.2118575 , -0.4275718 ,  0.28037375,\n",
       "          0.21666938],\n",
       "        [ 0.01940066,  0.4441507 , -0.55297226,  0.29861766,  0.5021421 ,\n",
       "          0.1787737 ],\n",
       "        [ 0.5473778 ,  0.1709696 ,  0.27938467, -0.4588752 , -0.51026756,\n",
       "          0.11692309],\n",
       "        [-0.04136992,  0.17540514,  0.26875657,  0.35398102,  0.17463863,\n",
       "          0.39661598],\n",
       "        [-0.00991118, -0.4840891 , -0.512377  ,  0.2575156 , -0.06814849,\n",
       "          0.39423   ],\n",
       "        [ 0.39722502,  0.31143224, -0.60936147,  0.3610916 ,  0.3027318 ,\n",
       "          0.18970531],\n",
       "        [ 0.34653687,  0.05238426, -0.6469058 ,  0.63748014,  0.18192792,\n",
       "          0.6025808 ]], dtype=float32),\n",
       " array([0., 0., 0., 0., 0., 0.], dtype=float32),\n",
       " array([[ 0.14373451, -0.11487073],\n",
       "        [-0.3126158 , -0.67106754],\n",
       "        [ 0.12998754,  0.76405567],\n",
       "        [-0.27986676,  0.19221061],\n",
       "        [-0.48355442,  0.33047873],\n",
       "        [ 0.82225007, -0.30069488]], dtype=float32),\n",
       " array([-0.06473789, -0.02166712], dtype=float32),\n",
       " array([[ 0.36879694],\n",
       "        [ 0.17663765],\n",
       "        [-0.7370173 ],\n",
       "        [-0.47614032],\n",
       "        [-0.1600191 ],\n",
       "        [-0.50195014]], dtype=float32),\n",
       " array([1.1506798], dtype=float32),\n",
       " array([[ 0.32344627],\n",
       "        [ 0.10996842],\n",
       "        [-0.47833443],\n",
       "        [-0.42583403],\n",
       "        [-0.23232824],\n",
       "        [-0.24278504]], dtype=float32),\n",
       " array([8.383426], dtype=float32),\n",
       " array([[ 0.21383178],\n",
       "        [-0.15103483],\n",
       "        [-0.19661808],\n",
       "        [ 0.22263765],\n",
       "        [ 0.72449505],\n",
       "        [-0.6095699 ]], dtype=float32),\n",
       " array([0.16440535], dtype=float32),\n",
       " array([[-0.67264104],\n",
       "        [ 0.23548675],\n",
       "        [ 0.23903477],\n",
       "        [ 0.42527044],\n",
       "        [-0.3073864 ],\n",
       "        [-0.88314235]], dtype=float32),\n",
       " array([0.12763272], dtype=float32)]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vae = Ext_Multi_VAE()\n",
    "print(vae.decoder(prior_samples[:5]).mixture_distribution.logits)\n",
    "print(vae.decoder(prior_samples[:5]).components_distribution.distribution.loc)\n",
    "print(vae.decoder(prior_samples[:5]).components_distribution.distribution.scale)\n",
    "vae.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "seed = 100\n",
    "random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ber = tfd.Bernoulli(probs=0.3, dtype ='float').sample(1000) \n",
    "IG1 = tfd.InverseGamma(concentration =1.8, scale = 1 ).sample(1000)\n",
    "IG2 = tfd.InverseGamma(concentration =1.8, scale = 1 ).sample(1000)\n",
    "R2 = IG1*Ber +(6+IG2)*(1-Ber)\n",
    "\n",
    "train_dataset = R2[:250]\n",
    "train_dataset = tf.reshape(train_dataset,[250,1])\n",
    "eval_dataset = R2[250:]\n",
    "eval_dataset=tf.reshape(eval_dataset,[750,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_log_likelihood = lambda x, rv_x: -rv_x.log_prob(x)\n",
    "\n",
    "vae.compile(optimizer=tf.optimizers.Adam(learning_rate=1e-4),\n",
    "            loss=negative_log_likelihood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 3.0510 - val_loss: 2.5128\n",
      "Epoch 2/1000\n",
      "8/8 [==============================] - 0s 45ms/step - loss: 3.0368 - val_loss: 2.4908\n",
      "Epoch 3/1000\n",
      "8/8 [==============================] - 0s 52ms/step - loss: 3.1271 - val_loss: 2.5423\n",
      "Epoch 4/1000\n",
      "8/8 [==============================] - 0s 59ms/step - loss: 3.1154 - val_loss: 2.4793\n",
      "Epoch 5/1000\n",
      "8/8 [==============================] - 0s 43ms/step - loss: 2.8327 - val_loss: 2.5147\n",
      "Epoch 6/1000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 3.1301 - val_loss: 2.4982\n",
      "Epoch 7/1000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 2.9464 - val_loss: 2.5347\n",
      "Epoch 8/1000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 2.8546 - val_loss: 2.5435\n",
      "Epoch 9/1000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 3.2277 - val_loss: 2.5309\n",
      "Epoch 10/1000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 3.2239 - val_loss: 2.5414\n",
      "Epoch 11/1000\n",
      "8/8 [==============================] - 0s 50ms/step - loss: 2.8906 - val_loss: 2.5241\n",
      "Epoch 12/1000\n",
      "8/8 [==============================] - 0s 42ms/step - loss: 3.1394 - val_loss: 2.4687\n",
      "Epoch 13/1000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 3.2328 - val_loss: 2.5173\n",
      "Epoch 14/1000\n",
      "8/8 [==============================] - 0s 38ms/step - loss: 2.9202 - val_loss: 2.5260\n",
      "Epoch 15/1000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 3.0129 - val_loss: 2.5166\n",
      "Epoch 16/1000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 3.0400 - val_loss: 2.5482\n",
      "Epoch 17/1000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 3.3162 - val_loss: 2.5612\n",
      "Epoch 18/1000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 3.1825 - val_loss: 2.5285\n",
      "Epoch 19/1000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 3.0487 - val_loss: 2.5355\n",
      "Epoch 20/1000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 2.8809 - val_loss: 2.5065\n",
      "Epoch 21/1000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 2.6983 - val_loss: 2.5083\n",
      "Epoch 22/1000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 2.8925 - val_loss: 2.5516\n",
      "Epoch 23/1000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.9420 - val_loss: 2.4646\n",
      "Epoch 24/1000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 3.1362 - val_loss: 2.5487\n",
      "Epoch 25/1000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 2.9217 - val_loss: 2.5248\n",
      "Epoch 26/1000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 3.0264 - val_loss: 2.5226\n",
      "Epoch 27/1000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.7827 - val_loss: 2.4831\n",
      "Epoch 28/1000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.7978 - val_loss: 2.5096\n",
      "Epoch 29/1000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 3.0654 - val_loss: 2.5518\n",
      "Epoch 30/1000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 3.0828 - val_loss: 2.5637\n",
      "Epoch 31/1000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 2.9285 - val_loss: 2.5278\n",
      "Epoch 32/1000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 3.1617 - val_loss: 2.5307\n",
      "Epoch 33/1000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 2.8509 - val_loss: 2.5412\n",
      "Epoch 34/1000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.9768 - val_loss: 2.4853\n",
      "Epoch 35/1000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 3.1072 - val_loss: 2.5214\n",
      "Epoch 36/1000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 2.8368 - val_loss: 2.4732\n",
      "Epoch 37/1000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 2.9865 - val_loss: 2.4934\n",
      "Epoch 38/1000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 2.8732 - val_loss: 2.5539\n",
      "Epoch 39/1000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 3.1372 - val_loss: 2.5158\n",
      "Epoch 40/1000\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 3.2988 - val_loss: 2.4858\n",
      "Epoch 41/1000\n",
      "8/8 [==============================] - 0s 36ms/step - loss: 2.9285 - val_loss: 2.5358\n",
      "Epoch 42/1000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 2.7142 - val_loss: 2.4889\n",
      "Epoch 43/1000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 3.2403 - val_loss: 2.5039\n",
      "Epoch 44/1000\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 3.1076 - val_loss: 2.5303\n",
      "Epoch 45/1000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 3.0013 - val_loss: 2.5206\n",
      "Epoch 46/1000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 3.1641 - val_loss: 2.4713\n",
      "Epoch 47/1000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 2.6229 - val_loss: 2.5281\n",
      "Epoch 48/1000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 2.8434 - val_loss: 2.5341\n",
      "Epoch 49/1000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.9329 - val_loss: 2.4878\n",
      "Epoch 50/1000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 3.0726 - val_loss: 2.4924\n",
      "Epoch 51/1000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 3.2128 - val_loss: 2.4881\n",
      "Epoch 52/1000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 2.9548 - val_loss: 2.5123\n",
      "Epoch 53/1000\n",
      "8/8 [==============================] - 0s 38ms/step - loss: 2.9866 - val_loss: 2.5082\n",
      "Epoch 54/1000\n",
      "8/8 [==============================] - 0s 36ms/step - loss: 3.0320 - val_loss: 2.4837\n",
      "Epoch 55/1000\n",
      "8/8 [==============================] - 0s 37ms/step - loss: 2.7298 - val_loss: 2.4808\n",
      "Epoch 56/1000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 3.1681 - val_loss: 2.4967\n",
      "Epoch 57/1000\n",
      "8/8 [==============================] - 0s 38ms/step - loss: 3.0786 - val_loss: 2.5388\n",
      "Epoch 58/1000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 2.9221 - val_loss: 2.4672\n",
      "Epoch 59/1000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.6613 - val_loss: 2.5291\n",
      "Epoch 60/1000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 3.1335 - val_loss: 2.5332\n",
      "Epoch 61/1000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 3.0110 - val_loss: 2.5164\n",
      "Epoch 62/1000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 2.8574 - val_loss: 2.5162\n",
      "Epoch 63/1000\n",
      "8/8 [==============================] - 0s 37ms/step - loss: 3.0130 - val_loss: 2.5307\n",
      "Epoch 64/1000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 3.1373 - val_loss: 2.5122\n",
      "Epoch 65/1000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 3.0822 - val_loss: 2.4375\n",
      "Epoch 66/1000\n",
      "8/8 [==============================] - 0s 36ms/step - loss: 3.1543 - val_loss: 2.5307\n",
      "Epoch 67/1000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.9113 - val_loss: 2.5273\n",
      "Epoch 68/1000\n",
      "8/8 [==============================] - 0s 36ms/step - loss: 3.0674 - val_loss: 2.4942\n",
      "Epoch 69/1000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 2.9717 - val_loss: 2.5359\n",
      "Epoch 70/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 3.2187 - val_loss: 2.5279\n",
      "Epoch 71/1000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.6948 - val_loss: 2.5323\n",
      "Epoch 72/1000\n",
      "8/8 [==============================] - 0s 41ms/step - loss: 2.7677 - val_loss: 2.5446\n",
      "Epoch 73/1000\n",
      "8/8 [==============================] - 0s 36ms/step - loss: 2.5837 - val_loss: 2.4972\n",
      "Epoch 74/1000\n",
      "8/8 [==============================] - 0s 38ms/step - loss: 3.1295 - val_loss: 2.5490\n",
      "Epoch 75/1000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 3.0138 - val_loss: 2.5343\n",
      "Epoch 76/1000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 3.3202 - val_loss: 2.5007\n",
      "Epoch 77/1000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 2.8667 - val_loss: 2.4905\n",
      "Epoch 78/1000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 3.0376 - val_loss: 2.4875\n",
      "Epoch 79/1000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.7995 - val_loss: 2.5195\n",
      "Epoch 80/1000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 2.7721 - val_loss: 2.5071\n",
      "Epoch 81/1000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 3.1154 - val_loss: 2.5227\n",
      "Epoch 82/1000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 3.0417 - val_loss: 2.4987\n",
      "Epoch 83/1000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 3.1421 - val_loss: 2.5124\n",
      "Epoch 84/1000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 2.9097 - val_loss: 2.5283\n",
      "Epoch 85/1000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 3.1575 - val_loss: 2.5236\n",
      "Epoch 86/1000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 3.1457 - val_loss: 2.5015\n",
      "Epoch 87/1000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 3.0565 - val_loss: 2.5237\n",
      "Epoch 88/1000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 3.1455 - val_loss: 2.4888\n",
      "Epoch 89/1000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 2.7878 - val_loss: 2.5292\n",
      "Epoch 90/1000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.7442 - val_loss: 2.4791\n",
      "Epoch 91/1000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 2.6927 - val_loss: 2.4685\n",
      "Epoch 92/1000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 2.9282 - val_loss: 2.5047\n",
      "Epoch 93/1000\n",
      "8/8 [==============================] - 0s 36ms/step - loss: 2.9969 - val_loss: 2.4898\n",
      "Epoch 94/1000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 2.9163 - val_loss: 2.5046\n",
      "Epoch 95/1000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 3.0636 - val_loss: 2.4956\n",
      "Epoch 96/1000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 2.9720 - val_loss: 2.5071\n",
      "Epoch 97/1000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 2.9674 - val_loss: 2.4737\n",
      "Epoch 98/1000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 2.7992 - val_loss: 2.5197\n",
      "Epoch 99/1000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 2.8224 - val_loss: 2.5137\n",
      "Epoch 100/1000\n",
      "8/8 [==============================] - 0s 48ms/step - loss: 3.1091 - val_loss: 2.5020\n",
      "Epoch 101/1000\n",
      "8/8 [==============================] - 0s 38ms/step - loss: 2.8702 - val_loss: 2.4914\n",
      "Epoch 102/1000\n",
      "8/8 [==============================] - 0s 38ms/step - loss: 3.0332 - val_loss: 2.4837\n",
      "Epoch 103/1000\n",
      "8/8 [==============================] - 0s 43ms/step - loss: 3.1479 - val_loss: 2.5014\n",
      "Epoch 104/1000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 3.0956 - val_loss: 2.5227\n",
      "Epoch 105/1000\n",
      "8/8 [==============================] - 0s 59ms/step - loss: 3.0614 - val_loss: 2.4923\n",
      "Epoch 106/1000\n",
      "8/8 [==============================] - 0s 59ms/step - loss: 2.8033 - val_loss: 2.5079\n",
      "Epoch 107/1000\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 3.1141 - val_loss: 2.4919\n",
      "Epoch 108/1000\n",
      "8/8 [==============================] - 0s 41ms/step - loss: 2.8712 - val_loss: 2.5152\n",
      "Epoch 109/1000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 2.8491 - val_loss: 2.4832\n",
      "Epoch 110/1000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 2.9507 - val_loss: 2.5018\n",
      "Epoch 111/1000\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 2.9460 - val_loss: 2.5296\n",
      "Epoch 112/1000\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 2.9291 - val_loss: 2.5160\n",
      "Epoch 113/1000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 2.8423 - val_loss: 2.4671\n",
      "Epoch 114/1000\n",
      "8/8 [==============================] - 0s 36ms/step - loss: 3.1443 - val_loss: 2.4849\n",
      "Epoch 115/1000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 2.6922 - val_loss: 2.5211\n",
      "Epoch 116/1000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 3.1159 - val_loss: 2.5127\n",
      "Epoch 117/1000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 2.5555 - val_loss: 2.5086\n",
      "Epoch 118/1000\n",
      "8/8 [==============================] - 0s 53ms/step - loss: 2.8640 - val_loss: 2.4916\n",
      "Epoch 119/1000\n",
      "8/8 [==============================] - 0s 54ms/step - loss: 3.0591 - val_loss: 2.4632\n",
      "Epoch 120/1000\n",
      "8/8 [==============================] - 0s 65ms/step - loss: 2.7690 - val_loss: 2.4889\n",
      "Epoch 121/1000\n",
      "8/8 [==============================] - 0s 58ms/step - loss: 2.8781 - val_loss: 2.4960\n",
      "Epoch 122/1000\n",
      "8/8 [==============================] - 0s 68ms/step - loss: 3.1053 - val_loss: 2.5014\n",
      "Epoch 123/1000\n",
      "8/8 [==============================] - 0s 42ms/step - loss: 2.8114 - val_loss: 2.5397\n",
      "Epoch 124/1000\n",
      "8/8 [==============================] - 0s 40ms/step - loss: 2.8990 - val_loss: 2.4969\n",
      "Epoch 125/1000\n",
      "8/8 [==============================] - 0s 58ms/step - loss: 2.9820 - val_loss: 2.5006\n",
      "Epoch 126/1000\n",
      "8/8 [==============================] - 0s 57ms/step - loss: 3.0193 - val_loss: 2.5247\n",
      "Epoch 127/1000\n",
      "8/8 [==============================] - 0s 57ms/step - loss: 2.8906 - val_loss: 2.5014\n",
      "Epoch 128/1000\n",
      "8/8 [==============================] - 0s 58ms/step - loss: 2.9202 - val_loss: 2.4828\n",
      "Epoch 129/1000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.8538 - val_loss: 2.4791\n",
      "Epoch 130/1000\n",
      "8/8 [==============================] - 0s 41ms/step - loss: 2.9109 - val_loss: 2.5116\n",
      "Epoch 131/1000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 2.7838 - val_loss: 2.4901\n",
      "Epoch 132/1000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 3.0118 - val_loss: 2.4539\n",
      "Epoch 133/1000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 2.9838 - val_loss: 2.5002\n",
      "Epoch 134/1000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 2.6916 - val_loss: 2.4879\n",
      "Epoch 135/1000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 2.7877 - val_loss: 2.5043\n",
      "Epoch 136/1000\n",
      "8/8 [==============================] - 0s 36ms/step - loss: 2.9982 - val_loss: 2.5316\n",
      "Epoch 137/1000\n",
      "8/8 [==============================] - 0s 36ms/step - loss: 2.9412 - val_loss: 2.4820\n",
      "Epoch 138/1000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 2.7501 - val_loss: 2.4755\n",
      "Epoch 139/1000\n",
      "8/8 [==============================] - 0s 40ms/step - loss: 3.0464 - val_loss: 2.4873\n",
      "Epoch 140/1000\n",
      "8/8 [==============================] - 0s 40ms/step - loss: 3.0907 - val_loss: 2.4936\n",
      "Epoch 141/1000\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 3.1592 - val_loss: 2.4854\n",
      "Epoch 142/1000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.9239 - val_loss: 2.5167\n",
      "Epoch 143/1000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 2.9344 - val_loss: 2.5254\n",
      "Epoch 144/1000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 2.7485 - val_loss: 2.4454\n",
      "Epoch 145/1000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 3.0028 - val_loss: 2.4862\n",
      "Epoch 146/1000\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 3.0482 - val_loss: 2.4864\n",
      "Epoch 147/1000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 2.7624 - val_loss: 2.4945\n",
      "Epoch 148/1000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 2.8243 - val_loss: 2.4931\n",
      "Epoch 149/1000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 3.0003 - val_loss: 2.4769\n",
      "Epoch 150/1000\n",
      "8/8 [==============================] - 0s 37ms/step - loss: 2.7520 - val_loss: 2.5031\n",
      "Epoch 151/1000\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 2.7178 - val_loss: 2.4900\n",
      "Epoch 152/1000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.9273 - val_loss: 2.5043\n",
      "Epoch 153/1000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 3.0374 - val_loss: 2.5352\n",
      "Epoch 154/1000\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 2.8983 - val_loss: 2.5200\n",
      "Epoch 155/1000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 2.9850 - val_loss: 2.5126\n",
      "Epoch 156/1000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 2.7453 - val_loss: 2.4735\n",
      "Epoch 157/1000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 2.9358 - val_loss: 2.5153\n",
      "Epoch 158/1000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 2.9045 - val_loss: 2.4539\n",
      "Epoch 159/1000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 2.8103 - val_loss: 2.5082\n",
      "Epoch 160/1000\n",
      "8/8 [==============================] - 0s 36ms/step - loss: 3.0312 - val_loss: 2.5178\n",
      "Epoch 161/1000\n",
      "8/8 [==============================] - 0s 66ms/step - loss: 2.9057 - val_loss: 2.4779\n",
      "Epoch 162/1000\n",
      "8/8 [==============================] - 0s 39ms/step - loss: 3.0472 - val_loss: 2.5163\n",
      "Epoch 163/1000\n",
      "8/8 [==============================] - 0s 42ms/step - loss: 3.0835 - val_loss: 2.4614\n",
      "Epoch 164/1000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 2.7988 - val_loss: 2.5170\n",
      "Epoch 165/1000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 3.0108 - val_loss: 2.4713\n",
      "Epoch 166/1000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.9338 - val_loss: 2.4988\n",
      "Epoch 167/1000\n",
      "8/8 [==============================] - 0s 36ms/step - loss: 2.7244 - val_loss: 2.4834\n",
      "Epoch 168/1000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.9533 - val_loss: 2.4901\n",
      "Epoch 169/1000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.9084 - val_loss: 2.4824\n",
      "Epoch 170/1000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 3.1031 - val_loss: 2.5117\n",
      "Epoch 171/1000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 3.0993 - val_loss: 2.5043\n",
      "Epoch 172/1000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 2.6553 - val_loss: 2.5168\n",
      "Epoch 173/1000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 2.8011 - val_loss: 2.5118\n",
      "Epoch 174/1000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 2.8388 - val_loss: 2.4678\n",
      "Epoch 175/1000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 2.8475 - val_loss: 2.5005\n",
      "Epoch 176/1000\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 3.1672 - val_loss: 2.5287\n",
      "Epoch 177/1000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 3.0025 - val_loss: 2.4993\n",
      "Epoch 178/1000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 2.9730 - val_loss: 2.4683\n",
      "Epoch 179/1000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 2.7825 - val_loss: 2.4652\n",
      "Epoch 180/1000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 2.8104 - val_loss: 2.4751\n",
      "Epoch 181/1000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 2.8665 - val_loss: 2.5090\n",
      "Epoch 182/1000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 2.8148 - val_loss: 2.4962\n",
      "Epoch 183/1000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 2.7904 - val_loss: 2.5127\n",
      "Epoch 184/1000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 2.7731 - val_loss: 2.4839\n",
      "Epoch 185/1000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.8517 - val_loss: 2.5035\n",
      "Epoch 186/1000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 3.0042 - val_loss: 2.4840\n",
      "Epoch 187/1000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 2.9439 - val_loss: 2.4983\n",
      "Epoch 188/1000\n",
      "8/8 [==============================] - 0s 37ms/step - loss: 3.0185 - val_loss: 2.5331\n",
      "Epoch 189/1000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.9384 - val_loss: 2.4937\n",
      "Epoch 190/1000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 2.8369 - val_loss: 2.4918\n",
      "Epoch 191/1000\n",
      "8/8 [==============================] - 0s 39ms/step - loss: 2.9468 - val_loss: 2.5212\n",
      "Epoch 192/1000\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 2.8686 - val_loss: 2.4763\n",
      "Epoch 193/1000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 3.0389 - val_loss: 2.5015\n",
      "Epoch 194/1000\n",
      "8/8 [==============================] - 0s 43ms/step - loss: 2.9930 - val_loss: 2.5215\n",
      "Epoch 195/1000\n",
      "8/8 [==============================] - 0s 40ms/step - loss: 2.7013 - val_loss: 2.4882\n",
      "Epoch 196/1000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 2.8989 - val_loss: 2.4999\n",
      "Epoch 197/1000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 2.9047 - val_loss: 2.4891\n",
      "Epoch 198/1000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 2.7026 - val_loss: 2.5066\n",
      "Epoch 199/1000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 2.6579 - val_loss: 2.5066\n",
      "Epoch 200/1000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 2.7901 - val_loss: 2.5153\n",
      "Epoch 201/1000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 3.1430 - val_loss: 2.5013\n",
      "Epoch 202/1000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 2.9529 - val_loss: 2.5204\n",
      "Epoch 203/1000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 2.9619 - val_loss: 2.4930\n",
      "Epoch 204/1000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 2.8378 - val_loss: 2.4705\n",
      "Epoch 205/1000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 2.9223 - val_loss: 2.4959\n",
      "Epoch 206/1000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.8209 - val_loss: 2.4827\n",
      "Epoch 207/1000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 2.9679 - val_loss: 2.4776\n",
      "Epoch 208/1000\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 3.0149 - val_loss: 2.5288\n",
      "Epoch 209/1000\n",
      "8/8 [==============================] - 0s 39ms/step - loss: 2.9361 - val_loss: 2.5120\n",
      "Epoch 210/1000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 2.9944 - val_loss: 2.5085\n",
      "Epoch 211/1000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 2.7744 - val_loss: 2.5047\n",
      "Epoch 212/1000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 2.9279 - val_loss: 2.4613\n",
      "Epoch 213/1000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 2.9055 - val_loss: 2.4818\n",
      "Epoch 214/1000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 2.6999 - val_loss: 2.4834\n",
      "Epoch 215/1000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 2.9864 - val_loss: 2.5018\n",
      "Epoch 216/1000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 2.9329 - val_loss: 2.5124\n",
      "Epoch 217/1000\n",
      "8/8 [==============================] - 0s 48ms/step - loss: 2.8831 - val_loss: 2.4992\n",
      "Epoch 218/1000\n",
      "8/8 [==============================] - 0s 37ms/step - loss: 3.0049 - val_loss: 2.4846\n",
      "Epoch 219/1000\n",
      "8/8 [==============================] - 0s 62ms/step - loss: 2.9927 - val_loss: 2.5249\n",
      "Epoch 220/1000\n",
      "8/8 [==============================] - 0s 56ms/step - loss: 2.9227 - val_loss: 2.4893\n",
      "Epoch 221/1000\n",
      "8/8 [==============================] - 1s 76ms/step - loss: 2.9611 - val_loss: 2.5197\n",
      "Epoch 222/1000\n",
      "8/8 [==============================] - 0s 36ms/step - loss: 2.9885 - val_loss: 2.4951\n",
      "Epoch 223/1000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 2.8577 - val_loss: 2.4877\n",
      "Epoch 224/1000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 2.7449 - val_loss: 2.5029\n",
      "Epoch 225/1000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.9428 - val_loss: 2.5038\n",
      "Epoch 226/1000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 2.7405 - val_loss: 2.4749\n",
      "Epoch 227/1000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 2.7793 - val_loss: 2.5053\n",
      "Epoch 228/1000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 2.8751 - val_loss: 2.4992\n",
      "Epoch 229/1000\n",
      "8/8 [==============================] - 0s 62ms/step - loss: 2.8956 - val_loss: 2.5282\n",
      "Epoch 230/1000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.8188 - val_loss: 2.5037\n",
      "Epoch 231/1000\n",
      "8/8 [==============================] - 0s 39ms/step - loss: 3.0142 - val_loss: 2.5126\n",
      "Epoch 232/1000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 3.0980 - val_loss: 2.4683\n",
      "Epoch 233/1000\n",
      "8/8 [==============================] - 0s 36ms/step - loss: 2.7173 - val_loss: 2.5055\n",
      "Epoch 234/1000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 2.8007 - val_loss: 2.5288\n",
      "Epoch 235/1000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 2.8132 - val_loss: 2.4785\n",
      "Epoch 236/1000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.7525 - val_loss: 2.4924\n",
      "Epoch 237/1000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 2.9558 - val_loss: 2.5168\n",
      "Epoch 238/1000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.8167 - val_loss: 2.5069\n",
      "Epoch 239/1000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 2.8030 - val_loss: 2.4588\n",
      "Epoch 240/1000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 2.8293 - val_loss: 2.4737\n",
      "Epoch 241/1000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 2.7906 - val_loss: 2.4907\n",
      "Epoch 242/1000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.9631 - val_loss: 2.5113\n",
      "Epoch 243/1000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 2.7889 - val_loss: 2.4816\n",
      "Epoch 244/1000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 2.7689 - val_loss: 2.5096\n",
      "Epoch 245/1000\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 2.8839 - val_loss: 2.5273\n",
      "Epoch 246/1000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 2.8539 - val_loss: 2.4741\n",
      "Epoch 247/1000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 2.8730 - val_loss: 2.4746\n",
      "Epoch 248/1000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 2.9485 - val_loss: 2.4776\n",
      "Epoch 249/1000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 2.9371 - val_loss: 2.5034\n",
      "Epoch 250/1000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 2.9792 - val_loss: 2.4855\n",
      "Epoch 251/1000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 2.8198 - val_loss: 2.4817\n",
      "Epoch 252/1000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.8588 - val_loss: 2.5196\n",
      "Epoch 253/1000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 2.7545 - val_loss: 2.4992\n",
      "Epoch 254/1000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 2.7776 - val_loss: 2.4775\n",
      "Epoch 255/1000\n",
      "8/8 [==============================] - 0s 45ms/step - loss: 2.5861 - val_loss: 2.4777\n",
      "Epoch 256/1000\n",
      "8/8 [==============================] - 0s 36ms/step - loss: 2.9347 - val_loss: 2.4728\n",
      "Epoch 257/1000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 2.9192 - val_loss: 2.4933\n",
      "Epoch 258/1000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 2.7711 - val_loss: 2.4722\n",
      "Epoch 259/1000\n",
      "8/8 [==============================] - 0s 37ms/step - loss: 2.9841 - val_loss: 2.4685\n",
      "Epoch 260/1000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 2.9655 - val_loss: 2.4739\n",
      "Epoch 261/1000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.8211 - val_loss: 2.5046\n",
      "Epoch 262/1000\n",
      "8/8 [==============================] - 0s 39ms/step - loss: 2.9325 - val_loss: 2.5189\n",
      "Epoch 263/1000\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 2.7074 - val_loss: 2.4842\n",
      "Epoch 264/1000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 2.6841 - val_loss: 2.5435\n",
      "Epoch 265/1000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 2.9754 - val_loss: 2.4739\n",
      "Epoch 266/1000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 2.9086 - val_loss: 2.4548\n",
      "Epoch 267/1000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 2.9828 - val_loss: 2.5120\n",
      "Epoch 268/1000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 2.8624 - val_loss: 2.4973\n",
      "Epoch 269/1000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 2.9899 - val_loss: 2.4815\n",
      "Epoch 270/1000\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 2.7622 - val_loss: 2.5313\n",
      "Epoch 271/1000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 2.9419 - val_loss: 2.4830\n",
      "Epoch 272/1000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.9103 - val_loss: 2.5211\n",
      "Epoch 273/1000\n",
      "8/8 [==============================] - 0s 37ms/step - loss: 2.7461 - val_loss: 2.5217\n",
      "Epoch 274/1000\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 2.8518 - val_loss: 2.5181\n",
      "Epoch 275/1000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 2.6727 - val_loss: 2.4971\n",
      "Epoch 276/1000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.8781 - val_loss: 2.5102\n",
      "Epoch 277/1000\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 2.8533 - val_loss: 2.4749\n",
      "Epoch 278/1000\n",
      "8/8 [==============================] - 0s 38ms/step - loss: 2.9980 - val_loss: 2.4753\n",
      "Epoch 279/1000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 2.9154 - val_loss: 2.4734\n",
      "Epoch 280/1000\n",
      "8/8 [==============================] - 0s 44ms/step - loss: 2.9358 - val_loss: 2.4402\n",
      "Epoch 281/1000\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 2.8954 - val_loss: 2.4718\n",
      "Epoch 282/1000\n",
      "8/8 [==============================] - 0s 39ms/step - loss: 2.9097 - val_loss: 2.5243\n",
      "Epoch 283/1000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 2.8598 - val_loss: 2.5211\n",
      "Epoch 284/1000\n",
      "8/8 [==============================] - 0s 36ms/step - loss: 3.0264 - val_loss: 2.5157\n",
      "Epoch 285/1000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 2.7451 - val_loss: 2.5068\n",
      "Epoch 286/1000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 2.9120 - val_loss: 2.5249\n",
      "Epoch 287/1000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 2.8250 - val_loss: 2.5228\n",
      "Epoch 288/1000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 2.8505 - val_loss: 2.5035\n",
      "Epoch 289/1000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.8424 - val_loss: 2.5378\n",
      "Epoch 290/1000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 2.7410 - val_loss: 2.5132\n",
      "Epoch 291/1000\n",
      "8/8 [==============================] - 0s 41ms/step - loss: 2.8632 - val_loss: 2.4847\n",
      "Epoch 292/1000\n",
      "8/8 [==============================] - 0s 36ms/step - loss: 2.8902 - val_loss: 2.5065\n",
      "Epoch 293/1000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 2.9171 - val_loss: 2.4995\n",
      "Epoch 294/1000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 2.8850 - val_loss: 2.5103\n",
      "Epoch 295/1000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 2.9738 - val_loss: 2.4868\n",
      "Epoch 296/1000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.6711 - val_loss: 2.4920\n",
      "Epoch 297/1000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.8268 - val_loss: 2.5092\n",
      "Epoch 298/1000\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 2.6261 - val_loss: 2.5253\n",
      "Epoch 299/1000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 2.8209 - val_loss: 2.5038\n",
      "Epoch 300/1000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.9534 - val_loss: 2.5044\n",
      "Epoch 301/1000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 2.8235 - val_loss: 2.5006\n",
      "Epoch 302/1000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 2.7988 - val_loss: 2.4953\n",
      "Epoch 303/1000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.9789 - val_loss: 2.5138\n",
      "Epoch 304/1000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 2.6928 - val_loss: 2.4854\n",
      "Epoch 305/1000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 3.0344 - val_loss: 2.5210\n",
      "Epoch 306/1000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 2.9575 - val_loss: 2.4959\n",
      "Epoch 307/1000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 2.7048 - val_loss: 2.4940\n",
      "Epoch 308/1000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 2.8635 - val_loss: 2.5168\n",
      "Epoch 309/1000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 2.8470 - val_loss: 2.4958\n",
      "Epoch 310/1000\n",
      "8/8 [==============================] - 0s 58ms/step - loss: 2.8178 - val_loss: 2.5027\n",
      "Epoch 311/1000\n",
      "8/8 [==============================] - 0s 51ms/step - loss: 2.9477 - val_loss: 2.5027\n",
      "Epoch 312/1000\n",
      "8/8 [==============================] - 0s 56ms/step - loss: 2.9686 - val_loss: 2.5306\n",
      "Epoch 313/1000\n",
      "8/8 [==============================] - 0s 49ms/step - loss: 2.8465 - val_loss: 2.5115\n",
      "Epoch 314/1000\n",
      "8/8 [==============================] - 0s 56ms/step - loss: 2.7213 - val_loss: 2.5369\n",
      "Epoch 315/1000\n",
      "8/8 [==============================] - 0s 49ms/step - loss: 2.8004 - val_loss: 2.4988\n",
      "Epoch 316/1000\n",
      "8/8 [==============================] - 0s 52ms/step - loss: 2.7715 - val_loss: 2.5112\n",
      "Epoch 317/1000\n",
      "8/8 [==============================] - 0s 39ms/step - loss: 2.9051 - val_loss: 2.5185\n",
      "Epoch 318/1000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.9086 - val_loss: 2.5289\n",
      "Epoch 319/1000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 2.6584 - val_loss: 2.5099\n",
      "Epoch 320/1000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.8319 - val_loss: 2.5091\n",
      "Epoch 321/1000\n",
      "8/8 [==============================] - 0s 46ms/step - loss: 2.7991 - val_loss: 2.5096\n",
      "Epoch 322/1000\n",
      "8/8 [==============================] - 0s 39ms/step - loss: 2.9528 - val_loss: 2.4899\n",
      "Epoch 323/1000\n",
      "8/8 [==============================] - 0s 46ms/step - loss: 2.8449 - val_loss: 2.5245\n",
      "Epoch 324/1000\n",
      "8/8 [==============================] - 0s 37ms/step - loss: 2.8638 - val_loss: 2.5314\n",
      "Epoch 325/1000\n",
      "8/8 [==============================] - 0s 38ms/step - loss: 2.8496 - val_loss: 2.5437\n",
      "Epoch 326/1000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 2.8033 - val_loss: 2.5186\n",
      "Epoch 327/1000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 2.7960 - val_loss: 2.4931\n",
      "Epoch 328/1000\n",
      "8/8 [==============================] - 0s 36ms/step - loss: 2.6918 - val_loss: 2.4987\n",
      "Epoch 329/1000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 2.8418 - val_loss: 2.4705\n",
      "Epoch 330/1000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.6925 - val_loss: 2.5047\n",
      "Epoch 331/1000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 2.7220 - val_loss: 2.5113\n",
      "Epoch 332/1000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 2.9182 - val_loss: 2.5188\n",
      "Epoch 333/1000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 2.9011 - val_loss: 2.4857\n",
      "Epoch 334/1000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 2.7807 - val_loss: 2.5057\n",
      "Epoch 335/1000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.6642 - val_loss: 2.5284\n",
      "Epoch 336/1000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 2.8867 - val_loss: 2.5318\n",
      "Epoch 337/1000\n",
      "8/8 [==============================] - 0s 39ms/step - loss: 2.6550 - val_loss: 2.5206\n",
      "Epoch 338/1000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.6368 - val_loss: 2.5167\n",
      "Epoch 339/1000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 2.8929 - val_loss: 2.4872\n",
      "Epoch 340/1000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 2.6123 - val_loss: 2.5036\n",
      "Epoch 341/1000\n",
      "8/8 [==============================] - 0s 38ms/step - loss: 2.7223 - val_loss: 2.5044\n",
      "Epoch 342/1000\n",
      "8/8 [==============================] - 0s 38ms/step - loss: 2.9306 - val_loss: 2.4992\n",
      "Epoch 343/1000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 2.8485 - val_loss: 2.5379\n",
      "Epoch 344/1000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 2.7857 - val_loss: 2.5102\n",
      "Epoch 345/1000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 2.6876 - val_loss: 2.5129\n",
      "Epoch 346/1000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 2.8962 - val_loss: 2.5144\n",
      "Epoch 347/1000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 2.9220 - val_loss: 2.5233\n",
      "Epoch 348/1000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 2.8048 - val_loss: 2.4915\n",
      "Epoch 349/1000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 2.8884 - val_loss: 2.5110\n",
      "Epoch 350/1000\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 2.8734 - val_loss: 2.5161\n",
      "Epoch 351/1000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 2.9242 - val_loss: 2.5114\n",
      "Epoch 352/1000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 2.6582 - val_loss: 2.4902\n",
      "Epoch 353/1000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 2.9147 - val_loss: 2.5144\n",
      "Epoch 354/1000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 2.9121 - val_loss: 2.5055\n",
      "Epoch 355/1000\n",
      "8/8 [==============================] - 0s 38ms/step - loss: 2.7264 - val_loss: 2.5301\n",
      "Epoch 356/1000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.9114 - val_loss: 2.5046\n",
      "Epoch 357/1000\n",
      "8/8 [==============================] - 0s 40ms/step - loss: 2.7875 - val_loss: 2.5044\n",
      "Epoch 358/1000\n",
      "8/8 [==============================] - 0s 39ms/step - loss: 2.9070 - val_loss: 2.5236\n",
      "Epoch 359/1000\n",
      "8/8 [==============================] - 0s 48ms/step - loss: 2.7852 - val_loss: 2.5137\n",
      "Epoch 360/1000\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 2.8286 - val_loss: 2.5518\n",
      "Epoch 361/1000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 2.9164 - val_loss: 2.5074\n",
      "Epoch 362/1000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 2.9297 - val_loss: 2.5086\n",
      "Epoch 363/1000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 2.9560 - val_loss: 2.5356\n",
      "Epoch 364/1000\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 2.7630 - val_loss: 2.4966\n",
      "Epoch 365/1000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.7056 - val_loss: 2.5018\n",
      "Epoch 366/1000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.8353 - val_loss: 2.4950\n",
      "Epoch 367/1000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 2.6354 - val_loss: 2.5205\n",
      "Epoch 368/1000\n",
      "8/8 [==============================] - 0s 36ms/step - loss: 2.8490 - val_loss: 2.5085\n",
      "Epoch 369/1000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 2.7722 - val_loss: 2.5011\n",
      "Epoch 370/1000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 2.7893 - val_loss: 2.5339\n",
      "Epoch 371/1000\n",
      "8/8 [==============================] - 0s 36ms/step - loss: 2.8489 - val_loss: 2.5224\n",
      "Epoch 372/1000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 2.9177 - val_loss: 2.4906\n",
      "Epoch 373/1000\n",
      "8/8 [==============================] - 0s 39ms/step - loss: 3.0006 - val_loss: 2.4987\n",
      "Epoch 374/1000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 2.6504 - val_loss: 2.5079\n",
      "Epoch 375/1000\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 2.8321 - val_loss: 2.5366\n",
      "Epoch 376/1000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 2.8323 - val_loss: 2.4764\n",
      "Epoch 377/1000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 2.8326 - val_loss: 2.4995\n",
      "Epoch 378/1000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 2.9889 - val_loss: 2.5021\n",
      "Epoch 379/1000\n",
      "8/8 [==============================] - 0s 37ms/step - loss: 2.6250 - val_loss: 2.5056\n",
      "Epoch 380/1000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 2.6590 - val_loss: 2.4744\n",
      "Epoch 381/1000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 2.8853 - val_loss: 2.4856\n",
      "Epoch 382/1000\n",
      "8/8 [==============================] - 0s 41ms/step - loss: 2.6688 - val_loss: 2.5455\n",
      "Epoch 383/1000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 2.6818 - val_loss: 2.5117\n",
      "Epoch 384/1000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 2.7783 - val_loss: 2.4957\n",
      "Epoch 385/1000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 2.7456 - val_loss: 2.5216\n",
      "Epoch 386/1000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 2.7668 - val_loss: 2.4998\n",
      "Epoch 387/1000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 2.6927 - val_loss: 2.4976\n",
      "Epoch 388/1000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 2.7237 - val_loss: 2.5134\n",
      "Epoch 389/1000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 2.6757 - val_loss: 2.4949\n",
      "Epoch 390/1000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 2.7623 - val_loss: 2.4982\n",
      "Epoch 391/1000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.8149 - val_loss: 2.4995\n",
      "Epoch 392/1000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.6333 - val_loss: 2.4974\n",
      "Epoch 393/1000\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 2.8536 - val_loss: 2.4990\n",
      "Epoch 394/1000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 2.7937 - val_loss: 2.5138\n",
      "Epoch 395/1000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 2.9252 - val_loss: 2.4995\n",
      "Epoch 396/1000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 3.0068 - val_loss: 2.5138\n",
      "Epoch 397/1000\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 2.9417 - val_loss: 2.4779\n",
      "Epoch 398/1000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 2.8859 - val_loss: 2.5110\n",
      "Epoch 399/1000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 2.9228 - val_loss: 2.5067\n",
      "Epoch 400/1000\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 2.7670 - val_loss: 2.5246\n",
      "Epoch 401/1000\n",
      "8/8 [==============================] - 0s 39ms/step - loss: 2.9695 - val_loss: 2.5269\n",
      "Epoch 402/1000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 2.9448 - val_loss: 2.5133\n",
      "Epoch 403/1000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 2.7861 - val_loss: 2.5392\n",
      "Epoch 404/1000\n",
      "8/8 [==============================] - 0s 37ms/step - loss: 2.8251 - val_loss: 2.5417\n",
      "Epoch 405/1000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 2.9397 - val_loss: 2.5108\n",
      "Epoch 406/1000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 2.8166 - val_loss: 2.5039\n",
      "Epoch 407/1000\n",
      "8/8 [==============================] - 0s 38ms/step - loss: 2.7184 - val_loss: 2.5156\n",
      "Epoch 408/1000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 2.9145 - val_loss: 2.5104\n",
      "Epoch 409/1000\n",
      "8/8 [==============================] - 0s 45ms/step - loss: 2.7239 - val_loss: 2.5294\n",
      "Epoch 410/1000\n",
      "8/8 [==============================] - 0s 36ms/step - loss: 3.0250 - val_loss: 2.4860\n",
      "Epoch 411/1000\n",
      "8/8 [==============================] - 0s 38ms/step - loss: 2.9026 - val_loss: 2.5272\n",
      "Epoch 412/1000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 2.8045 - val_loss: 2.5036\n",
      "Epoch 413/1000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 2.8902 - val_loss: 2.4676\n",
      "Epoch 414/1000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.8118 - val_loss: 2.5134\n",
      "Epoch 415/1000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.8738 - val_loss: 2.5472\n",
      "Epoch 416/1000\n",
      "8/8 [==============================] - 0s 36ms/step - loss: 2.7615 - val_loss: 2.5006\n",
      "Epoch 417/1000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.9330 - val_loss: 2.5122\n",
      "Epoch 418/1000\n",
      "8/8 [==============================] - 0s 42ms/step - loss: 2.7455 - val_loss: 2.4999\n",
      "Epoch 419/1000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 2.7553 - val_loss: 2.5001\n",
      "Epoch 420/1000\n",
      "8/8 [==============================] - 0s 36ms/step - loss: 2.7419 - val_loss: 2.5456\n",
      "Epoch 421/1000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.8684 - val_loss: 2.5200\n",
      "Epoch 422/1000\n",
      "8/8 [==============================] - 0s 45ms/step - loss: 2.7265 - val_loss: 2.4907\n",
      "Epoch 423/1000\n",
      "8/8 [==============================] - 0s 22ms/step - loss: 2.6555 - val_loss: 2.5122\n",
      "Epoch 424/1000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 2.7760 - val_loss: 2.5275\n",
      "Epoch 425/1000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 2.7247 - val_loss: 2.4865\n",
      "Epoch 426/1000\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 2.8983 - val_loss: 2.5025\n",
      "Epoch 427/1000\n",
      "8/8 [==============================] - 0s 41ms/step - loss: 2.7785 - val_loss: 2.4847\n",
      "Epoch 428/1000\n",
      "8/8 [==============================] - 0s 39ms/step - loss: 2.7897 - val_loss: 2.5138\n",
      "Epoch 429/1000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 2.8021 - val_loss: 2.4966\n",
      "Epoch 430/1000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 2.9422 - val_loss: 2.5175\n",
      "Epoch 431/1000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 2.8989 - val_loss: 2.5411\n",
      "Epoch 432/1000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 2.7600 - val_loss: 2.4922\n",
      "Epoch 433/1000\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 2.8398 - val_loss: 2.4880\n",
      "Epoch 434/1000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.6463 - val_loss: 2.5128\n",
      "Epoch 435/1000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.8046 - val_loss: 2.5235\n",
      "Epoch 436/1000\n",
      "8/8 [==============================] - 0s 40ms/step - loss: 2.7913 - val_loss: 2.4956\n",
      "Epoch 437/1000\n",
      "8/8 [==============================] - 0s 43ms/step - loss: 2.9084 - val_loss: 2.5170\n",
      "Epoch 438/1000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 2.8887 - val_loss: 2.5437\n",
      "Epoch 439/1000\n",
      "8/8 [==============================] - 0s 57ms/step - loss: 2.7880 - val_loss: 2.5104\n",
      "Epoch 440/1000\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 2.7587 - val_loss: 2.5166\n",
      "Epoch 441/1000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 2.7897 - val_loss: 2.5401\n",
      "Epoch 442/1000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.8755 - val_loss: 2.5172\n",
      "Epoch 443/1000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.8077 - val_loss: 2.5024\n",
      "Epoch 444/1000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 2.7933 - val_loss: 2.5008\n",
      "Epoch 445/1000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 2.6775 - val_loss: 2.5004\n",
      "Epoch 446/1000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 2.7964 - val_loss: 2.5140\n",
      "Epoch 447/1000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.7867 - val_loss: 2.5249\n",
      "Epoch 448/1000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 2.8576 - val_loss: 2.4992\n",
      "Epoch 449/1000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 2.8622 - val_loss: 2.4780\n",
      "Epoch 450/1000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.8970 - val_loss: 2.5357\n",
      "Epoch 451/1000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 2.5300 - val_loss: 2.4965\n",
      "Epoch 452/1000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 2.8802 - val_loss: 2.5380\n",
      "Epoch 453/1000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 2.9724 - val_loss: 2.5222\n",
      "Epoch 454/1000\n",
      "8/8 [==============================] - 0s 45ms/step - loss: 2.8129 - val_loss: 2.5073\n",
      "Epoch 455/1000\n",
      "8/8 [==============================] - 0s 55ms/step - loss: 2.8411 - val_loss: 2.5007\n",
      "Epoch 456/1000\n",
      "8/8 [==============================] - 0s 58ms/step - loss: 2.6810 - val_loss: 2.4973\n",
      "Epoch 457/1000\n",
      "8/8 [==============================] - 0s 59ms/step - loss: 2.7778 - val_loss: 2.5247\n",
      "Epoch 458/1000\n",
      "8/8 [==============================] - 0s 56ms/step - loss: 2.8907 - val_loss: 2.5261\n",
      "Epoch 459/1000\n",
      "8/8 [==============================] - 0s 53ms/step - loss: 2.8061 - val_loss: 2.4972\n",
      "Epoch 460/1000\n",
      "8/8 [==============================] - 0s 45ms/step - loss: 2.8995 - val_loss: 2.5304\n",
      "Epoch 461/1000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.8080 - val_loss: 2.4885\n",
      "Epoch 462/1000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 2.7038 - val_loss: 2.4978\n",
      "Epoch 463/1000\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 2.7832 - val_loss: 2.5265\n",
      "Epoch 464/1000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 2.8736 - val_loss: 2.4834\n",
      "Epoch 465/1000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 2.8166 - val_loss: 2.5354\n",
      "Epoch 466/1000\n",
      "8/8 [==============================] - 0s 52ms/step - loss: 2.8513 - val_loss: 2.5007\n",
      "Epoch 467/1000\n",
      "8/8 [==============================] - 0s 53ms/step - loss: 2.6997 - val_loss: 2.4809\n",
      "Epoch 468/1000\n",
      "8/8 [==============================] - 0s 58ms/step - loss: 2.7898 - val_loss: 2.5200\n",
      "Epoch 469/1000\n",
      "8/8 [==============================] - 0s 51ms/step - loss: 2.8549 - val_loss: 2.5324\n",
      "Epoch 470/1000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.7746 - val_loss: 2.5120\n",
      "Epoch 471/1000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 2.8743 - val_loss: 2.5153\n",
      "Epoch 472/1000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 2.9042 - val_loss: 2.5363\n",
      "Epoch 473/1000\n",
      "8/8 [==============================] - 0s 39ms/step - loss: 2.9231 - val_loss: 2.5261\n",
      "Epoch 474/1000\n",
      "8/8 [==============================] - 0s 39ms/step - loss: 2.8968 - val_loss: 2.4933\n",
      "Epoch 475/1000\n",
      "8/8 [==============================] - 0s 43ms/step - loss: 2.8659 - val_loss: 2.5343\n",
      "Epoch 476/1000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.6922 - val_loss: 2.5117\n",
      "Epoch 477/1000\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 2.7495 - val_loss: 2.5276\n",
      "Epoch 478/1000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 2.8053 - val_loss: 2.4791\n",
      "Epoch 479/1000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 2.8995 - val_loss: 2.5247\n",
      "Epoch 480/1000\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 2.9136 - val_loss: 2.5087\n",
      "Epoch 481/1000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 2.8181 - val_loss: 2.5122\n",
      "Epoch 482/1000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 2.8329 - val_loss: 2.5155\n",
      "Epoch 483/1000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.8397 - val_loss: 2.4951\n",
      "Epoch 484/1000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 2.9324 - val_loss: 2.5353\n",
      "Epoch 485/1000\n",
      "8/8 [==============================] - 0s 36ms/step - loss: 2.9136 - val_loss: 2.4997\n",
      "Epoch 486/1000\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 2.8933 - val_loss: 2.5386\n",
      "Epoch 487/1000\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 2.3889"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [146]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mvae\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\n\u001b[1;32m      5\u001b[0m \u001b[43m       \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/tf/lib/python3.9/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.conda/envs/tf/lib/python3.9/site-packages/keras/engine/training.py:1606\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1591\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_eval_data_handler\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1592\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_eval_data_handler \u001b[38;5;241m=\u001b[39m data_adapter\u001b[38;5;241m.\u001b[39mget_data_handler(\n\u001b[1;32m   1593\u001b[0m         x\u001b[38;5;241m=\u001b[39mval_x,\n\u001b[1;32m   1594\u001b[0m         y\u001b[38;5;241m=\u001b[39mval_y,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1604\u001b[0m         steps_per_execution\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_steps_per_execution,\n\u001b[1;32m   1605\u001b[0m     )\n\u001b[0;32m-> 1606\u001b[0m val_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1607\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_x\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1608\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_y\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1609\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_sample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1610\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_batch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1611\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1612\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1613\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_queue_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_queue_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1614\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1615\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_multiprocessing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_multiprocessing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1616\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1617\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_use_cached_eval_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1618\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1619\u001b[0m val_logs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m   1620\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m name: val \u001b[38;5;28;01mfor\u001b[39;00m name, val \u001b[38;5;129;01min\u001b[39;00m val_logs\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m   1621\u001b[0m }\n\u001b[1;32m   1622\u001b[0m epoch_logs\u001b[38;5;241m.\u001b[39mupdate(val_logs)\n",
      "File \u001b[0;32m~/.conda/envs/tf/lib/python3.9/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.conda/envs/tf/lib/python3.9/site-packages/keras/engine/training.py:1939\u001b[0m, in \u001b[0;36mModel.evaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m   1937\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_test_counter\u001b[38;5;241m.\u001b[39massign(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m   1938\u001b[0m callbacks\u001b[38;5;241m.\u001b[39mon_test_begin()\n\u001b[0;32m-> 1939\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, iterator \u001b[38;5;129;01min\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39menumerate_epochs():  \u001b[38;5;66;03m# Single epoch.\u001b[39;00m\n\u001b[1;32m   1940\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreset_metrics()\n\u001b[1;32m   1941\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mcatch_stop_iteration():\n",
      "File \u001b[0;32m~/.conda/envs/tf/lib/python3.9/site-packages/keras/engine/data_adapter.py:1307\u001b[0m, in \u001b[0;36mDataHandler.enumerate_epochs\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1305\u001b[0m \u001b[38;5;124;03m\"\"\"Yields `(epoch, tf.data.Iterator)`.\"\"\"\u001b[39;00m\n\u001b[1;32m   1306\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_truncate_execution_to_epoch():\n\u001b[0;32m-> 1307\u001b[0m     data_iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1308\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initial_epoch, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_epochs):\n\u001b[1;32m   1309\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_insufficient_data:  \u001b[38;5;66;03m# Set by `catch_stop_iteration`.\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/tf/lib/python3.9/site-packages/tensorflow/python/data/ops/dataset_ops.py:499\u001b[0m, in \u001b[0;36mDatasetV2.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    497\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m context\u001b[38;5;241m.\u001b[39mexecuting_eagerly() \u001b[38;5;129;01mor\u001b[39;00m ops\u001b[38;5;241m.\u001b[39minside_function():\n\u001b[1;32m    498\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mcolocate_with(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variant_tensor):\n\u001b[0;32m--> 499\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43miterator_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mOwnedIterator\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    501\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`tf.data.Dataset` only supports Python-style \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    502\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miteration in eager mode or within tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.conda/envs/tf/lib/python3.9/site-packages/tensorflow/python/data/ops/iterator_ops.py:696\u001b[0m, in \u001b[0;36mOwnedIterator.__init__\u001b[0;34m(self, dataset, components, element_spec)\u001b[0m\n\u001b[1;32m    692\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m (components \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m element_spec \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    693\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    694\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhen `dataset` is provided, `element_spec` and `components` must \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    695\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnot be specified.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 696\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_next_call_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m~/.conda/envs/tf/lib/python3.9/site-packages/tensorflow/python/data/ops/iterator_ops.py:721\u001b[0m, in \u001b[0;36mOwnedIterator._create_iterator\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    716\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mcolocate_with(ds_variant):\n\u001b[1;32m    717\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator_resource \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    718\u001b[0m       gen_dataset_ops\u001b[38;5;241m.\u001b[39manonymous_iterator_v3(\n\u001b[1;32m    719\u001b[0m           output_types\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_output_types,\n\u001b[1;32m    720\u001b[0m           output_shapes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_output_shapes))\n\u001b[0;32m--> 721\u001b[0m   \u001b[43mgen_dataset_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mds_variant\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_iterator_resource\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/tf/lib/python3.9/site-packages/tensorflow/python/ops/gen_dataset_ops.py:3409\u001b[0m, in \u001b[0;36mmake_iterator\u001b[0;34m(dataset, iterator, name)\u001b[0m\n\u001b[1;32m   3407\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tld\u001b[38;5;241m.\u001b[39mis_eager:\n\u001b[1;32m   3408\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3409\u001b[0m     _result \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_FastPathExecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3410\u001b[0m \u001b[43m      \u001b[49m\u001b[43m_ctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMakeIterator\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3411\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[1;32m   3412\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "vae.fit(train_dataset,train_dataset, \n",
    "        validation_data=(eval_dataset,eval_dataset), \n",
    "        batch_size=32,\n",
    "        epochs=1000\n",
    "       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_samples = 10000\n",
    "Ber = tfd.Bernoulli(probs=0.3, dtype ='float').sample(N_samples) \n",
    "IG1 = tfd.InverseGamma(concentration =1.8, scale = 1 ).sample(N_samples)\n",
    "IG2 = tfd.InverseGamma(concentration =1.8, scale = 1 ).sample(N_samples)\n",
    "R2 = IG1*Ber +(6+IG2)*(1-Ber)\n",
    "\n",
    "test_set = R2.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(15.017098, shape=(), dtype=float32)\n",
      "tf.Tensor(0.0033593029, shape=(), dtype=float32)\n",
      "tf.Tensor(\n",
      "[ -0.4391268   -0.43212909  -0.42935032 ...  79.94554    109.6181\n",
      " 288.221     ], shape=(9600,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "N_samples = 10000\n",
    "prior_samples = vae.encoder.prior.sample(N_samples)\n",
    "print(tf.reduce_max(prior_samples))\n",
    "print(tf.reduce_min(prior_samples))\n",
    "samples_vae = vae.decoder(prior_samples).sample()\n",
    "print(tf.sort(samples_vae[:,0])[400:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_63828/1683879684.py:4: UserWarning: \n",
      "\n",
      "`distplot` is a deprecated function and will be removed in seaborn v0.14.0.\n",
      "\n",
      "Please adapt your code to use either `displot` (a figure-level function with\n",
      "similar flexibility) or `histplot` (an axes-level function for histograms).\n",
      "\n",
      "For a guide to updating your code to use the new functions, please see\n",
      "https://gist.github.com/mwaskom/de44147ed2974457ad6372750bbe5751\n",
      "\n",
      "  sns.distplot(tf.sort(test_set)[:10000],color = 'b')\n",
      "/tmp/ipykernel_63828/1683879684.py:5: UserWarning: \n",
      "\n",
      "`distplot` is a deprecated function and will be removed in seaborn v0.14.0.\n",
      "\n",
      "Please adapt your code to use either `displot` (a figure-level function with\n",
      "similar flexibility) or `histplot` (an axes-level function for histograms).\n",
      "\n",
      "For a guide to updating your code to use the new functions, please see\n",
      "https://gist.github.com/mwaskom/de44147ed2974457ad6372750bbe5751\n",
      "\n",
      "  sns.distplot(tf.sort(samples_vae[:,0])[400:10000],color='r')\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.distplot(tf.sort(test_set)[:10000],color = 'b')\n",
    "sns.distplot(tf.sort(samples_vae[:,0])[400:10000],color='r')\n",
    "#plt.title(\"empirical density from true samples and generated samples\")\n",
    "\n",
    "plt.title(\"empirical density from true samples\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0.67066544 1.6417731 ]\n",
      " [0.8988993  1.943386  ]\n",
      " [0.19632523 1.0375935 ]\n",
      " [0.3475207  1.2301751 ]\n",
      " [0.4421476  1.3507041 ]], shape=(5, 2), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[[0.6987802 ]\n",
      "  [3.017632  ]]\n",
      "\n",
      " [[0.70095277]\n",
      "  [3.5716815 ]]\n",
      "\n",
      " [[0.6988378 ]\n",
      "  [1.9456846 ]]\n",
      "\n",
      " [[0.69554406]\n",
      "  [2.274832  ]]\n",
      "\n",
      " [[0.69380647]\n",
      "  [2.4876962 ]]], shape=(5, 2, 1), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[[0.6919679]\n",
      "  [7.2295675]]\n",
      "\n",
      " [[0.614556 ]\n",
      "  [6.98542  ]]\n",
      "\n",
      " [[0.8430016]\n",
      "  [7.7209167]]\n",
      "\n",
      " [[0.7948597]\n",
      "  [7.5642996]]\n",
      "\n",
      " [[0.7647298]\n",
      "  [7.4662795]]], shape=(5, 2, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(vae.decoder(prior_samples[:5]).mixture_distribution.logits)\n",
    "print(vae.decoder(prior_samples[:5]).components_distribution.distribution.scale)\n",
    "print(vae.decoder(prior_samples[:5]).components_distribution.distribution.loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "p =tfpl.MixtureSameFamily.params_size(2,component_params_size=tfpl.IndependentNormal.params_size([1]))\n",
    "print(p\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae.save_weights(\"Ext_Multi_VAE_checkpoint\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
