{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-18 15:42:41.386351: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import tensorboard\n",
    "tfk = tf.keras\n",
    "tfkl = tf.keras.layers\n",
    "tfpl = tfp.layers\n",
    "tfd = tfp.distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUCCESS: Found GPU: /device:GPU:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-03 12:34:53.120917: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-03 12:34:53.121587: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2022-11-03 12:34:53.125007: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\n",
      "2022-11-03 12:34:53.155509: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:3b:00.0 name: NVIDIA GeForce RTX 2080 Ti computeCapability: 7.5\n",
      "coreClock: 1.635GHz coreCount: 68 deviceMemorySize: 10.76GiB deviceMemoryBandwidth: 573.69GiB/s\n",
      "2022-11-03 12:34:53.155556: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2022-11-03 12:34:53.159790: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n",
      "2022-11-03 12:34:53.159897: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.10\n",
      "2022-11-03 12:34:53.164071: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2022-11-03 12:34:53.164960: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2022-11-03 12:34:53.169313: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2022-11-03 12:34:53.171127: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.10\n",
      "2022-11-03 12:34:53.177243: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\n",
      "2022-11-03 12:34:53.178023: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n",
      "2022-11-03 12:34:53.178074: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2022-11-03 12:34:53.930312: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2022-11-03 12:34:53.930359: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 \n",
      "2022-11-03 12:34:53.930372: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N \n",
      "2022-11-03 12:34:53.931410: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/device:GPU:0 with 120 MB memory) -> physical GPU (device: 0, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:3b:00.0, compute capability: 7.5)\n",
      "2022-11-03 12:34:53.932425: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2022-11-03 12:34:53.932830: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:3b:00.0 name: NVIDIA GeForce RTX 2080 Ti computeCapability: 7.5\n",
      "coreClock: 1.635GHz coreCount: 68 deviceMemorySize: 10.76GiB deviceMemoryBandwidth: 573.69GiB/s\n",
      "2022-11-03 12:34:53.932867: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2022-11-03 12:34:53.932901: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n",
      "2022-11-03 12:34:53.932914: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.10\n",
      "2022-11-03 12:34:53.932927: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2022-11-03 12:34:53.932940: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2022-11-03 12:34:53.932953: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2022-11-03 12:34:53.932966: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.10\n",
      "2022-11-03 12:34:53.932979: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\n",
      "2022-11-03 12:34:53.933441: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n",
      "2022-11-03 12:34:53.933464: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2022-11-03 12:34:53.933472: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 \n",
      "2022-11-03 12:34:53.933479: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N \n",
      "2022-11-03 12:34:53.933974: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/device:GPU:0 with 120 MB memory) -> physical GPU (device: 0, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:3b:00.0, compute capability: 7.5)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "if tf.test.gpu_device_name() != '/device:GPU:0':\n",
    "  print('WARNING: GPU device not found.')\n",
    "else:\n",
    "  print('SUCCESS: Found GPU: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_102476/3810096837.py:2: UserWarning: \n",
      "\n",
      "`distplot` is a deprecated function and will be removed in seaborn v0.14.0.\n",
      "\n",
      "Please adapt your code to use either `displot` (a figure-level function with\n",
      "similar flexibility) or `histplot` (an axes-level function for histograms).\n",
      "\n",
      "For a guide to updating your code to use the new functions, please see\n",
      "https://gist.github.com/mwaskom/de44147ed2974457ad6372750bbe5751\n",
      "\n",
      "  sns.distplot(Z)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Z = tfd.InverseGamma(3,1).sample(1000,1)\n",
    "sns.distplot(Z)\n",
    "plt.title(\"Samples from an inverse gamma\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "Cannot convert a symbolic Tensor (gradients/stateless_random_gamma/StatelessRandomGammaV2_grad/sub:0) to a numpy array. This error may indicate that you're trying to pass a Tensor to a NumPy call, which is not supported",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m U \u001b[38;5;241m=\u001b[39m \u001b[43mtfd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGamma\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m sns\u001b[38;5;241m.\u001b[39mdistplot(U)\n\u001b[1;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSamples from an inverse gamma\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.conda/envs/tf-gpu/lib/python3.9/site-packages/tensorflow_probability/python/distributions/distribution.py:939\u001b[0m, in \u001b[0;36mDistribution.sample\u001b[0;34m(self, sample_shape, seed, name, **kwargs)\u001b[0m\n\u001b[1;32m    924\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msample\u001b[39m(\u001b[38;5;28mself\u001b[39m, sample_shape\u001b[38;5;241m=\u001b[39m(), seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msample\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    925\u001b[0m   \u001b[38;5;124;03m\"\"\"Generate samples of the specified shape.\u001b[39;00m\n\u001b[1;32m    926\u001b[0m \n\u001b[1;32m    927\u001b[0m \u001b[38;5;124;03m  Note that a call to `sample()` without arguments will generate a single\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    937\u001b[0m \u001b[38;5;124;03m    samples: a `Tensor` with prepended dimensions `sample_shape`.\u001b[39;00m\n\u001b[1;32m    938\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 939\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_sample_n\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/tf-gpu/lib/python3.9/site-packages/tensorflow_probability/python/distributions/distribution.py:916\u001b[0m, in \u001b[0;36mDistribution._call_sample_n\u001b[0;34m(self, sample_shape, seed, name, **kwargs)\u001b[0m\n\u001b[1;32m    913\u001b[0m sample_shape \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mcast(sample_shape, tf\u001b[38;5;241m.\u001b[39mint32, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msample_shape\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    914\u001b[0m sample_shape, n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_sample_shape_to_vector(\n\u001b[1;32m    915\u001b[0m     sample_shape, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msample_shape\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 916\u001b[0m samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample_n\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    917\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcallable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    918\u001b[0m batch_event_shape \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mshape(samples)[\u001b[38;5;241m1\u001b[39m:]\n\u001b[1;32m    919\u001b[0m final_shape \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconcat([sample_shape, batch_event_shape], \u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/.conda/envs/tf-gpu/lib/python3.9/site-packages/tensorflow_probability/python/internal/distribution_util.py:1324\u001b[0m, in \u001b[0;36mAppendDocstring.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1322\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(fn)\n\u001b[1;32m   1323\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_fn\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m-> 1324\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/tf-gpu/lib/python3.9/site-packages/tensorflow_probability/python/distributions/gamma.py:211\u001b[0m, in \u001b[0;36mGamma._sample_n\u001b[0;34m(self, n, seed)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;129m@distribution_util\u001b[39m\u001b[38;5;241m.\u001b[39mAppendDocstring(\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;124;03m\"\"\"Note: See `tf.random.gamma` docstring for sampling details and\u001b[39;00m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;124;03m    caveats.\"\"\"\u001b[39;00m)\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_sample_n\u001b[39m(\u001b[38;5;28mself\u001b[39m, n, seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    209\u001b[0m   seed \u001b[38;5;241m=\u001b[39m samplers\u001b[38;5;241m.\u001b[39msanitize_seed(seed, salt\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgamma\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 211\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrandom_gamma\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    212\u001b[0m \u001b[43m      \u001b[49m\u001b[43mshape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_to_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mn\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    213\u001b[0m \u001b[43m      \u001b[49m\u001b[43mconcentration\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_to_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcentration\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[43m      \u001b[49m\u001b[43mrate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_to_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/.conda/envs/tf-gpu/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py:828\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    827\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trace\u001b[38;5;241m.\u001b[39mTrace(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_name) \u001b[38;5;28;01mas\u001b[39;00m tm:\n\u001b[0;32m--> 828\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    829\u001b[0m   compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_experimental_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    830\u001b[0m   new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n",
      "File \u001b[0;32m~/.conda/envs/tf-gpu/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py:862\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    859\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    860\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[1;32m    861\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[0;32m--> 862\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stateful_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    863\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[1;32m    864\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    865\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.conda/envs/tf-gpu/lib/python3.9/site-packages/tensorflow/python/eager/function.py:2941\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2938\u001b[0m \u001b[38;5;124;03m\"\"\"Calls a graph function specialized to the inputs.\"\"\"\u001b[39;00m\n\u001b[1;32m   2939\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m   2940\u001b[0m   (graph_function,\n\u001b[0;32m-> 2941\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_define_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2942\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m graph_function\u001b[38;5;241m.\u001b[39m_call_flat(\n\u001b[1;32m   2943\u001b[0m     filtered_flat_args, captured_inputs\u001b[38;5;241m=\u001b[39mgraph_function\u001b[38;5;241m.\u001b[39mcaptured_inputs)\n",
      "File \u001b[0;32m~/.conda/envs/tf-gpu/lib/python3.9/site-packages/tensorflow/python/eager/function.py:3361\u001b[0m, in \u001b[0;36mFunction._maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3357\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_define_function_with_shape_relaxation(\n\u001b[1;32m   3358\u001b[0m       args, kwargs, flat_args, filtered_flat_args, cache_key_context)\n\u001b[1;32m   3360\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_function_cache\u001b[38;5;241m.\u001b[39mmissed\u001b[38;5;241m.\u001b[39madd(call_context_key)\n\u001b[0;32m-> 3361\u001b[0m graph_function \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_graph_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3362\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_function_cache\u001b[38;5;241m.\u001b[39mprimary[cache_key] \u001b[38;5;241m=\u001b[39m graph_function\n\u001b[1;32m   3364\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m graph_function, filtered_flat_args\n",
      "File \u001b[0;32m~/.conda/envs/tf-gpu/lib/python3.9/site-packages/tensorflow/python/eager/function.py:3196\u001b[0m, in \u001b[0;36mFunction._create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3191\u001b[0m missing_arg_names \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m   3192\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (arg, i) \u001b[38;5;28;01mfor\u001b[39;00m i, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(missing_arg_names)\n\u001b[1;32m   3193\u001b[0m ]\n\u001b[1;32m   3194\u001b[0m arg_names \u001b[38;5;241m=\u001b[39m base_arg_names \u001b[38;5;241m+\u001b[39m missing_arg_names\n\u001b[1;32m   3195\u001b[0m graph_function \u001b[38;5;241m=\u001b[39m ConcreteFunction(\n\u001b[0;32m-> 3196\u001b[0m     \u001b[43mfunc_graph_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc_graph_from_py_func\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3197\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3198\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_python_function\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3199\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3200\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3201\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_signature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3202\u001b[0m \u001b[43m        \u001b[49m\u001b[43mautograph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_autograph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3203\u001b[0m \u001b[43m        \u001b[49m\u001b[43mautograph_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_autograph_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3204\u001b[0m \u001b[43m        \u001b[49m\u001b[43marg_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43marg_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3205\u001b[0m \u001b[43m        \u001b[49m\u001b[43moverride_flat_arg_shapes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverride_flat_arg_shapes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3206\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapture_by_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_capture_by_value\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m   3207\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_function_attributes,\n\u001b[1;32m   3208\u001b[0m     function_spec\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_spec,\n\u001b[1;32m   3209\u001b[0m     \u001b[38;5;66;03m# Tell the ConcreteFunction to clean up its graph once it goes out of\u001b[39;00m\n\u001b[1;32m   3210\u001b[0m     \u001b[38;5;66;03m# scope. This is not the default behavior since it gets used in some\u001b[39;00m\n\u001b[1;32m   3211\u001b[0m     \u001b[38;5;66;03m# places (like Keras) where the FuncGraph lives longer than the\u001b[39;00m\n\u001b[1;32m   3212\u001b[0m     \u001b[38;5;66;03m# ConcreteFunction.\u001b[39;00m\n\u001b[1;32m   3213\u001b[0m     shared_func_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   3214\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m graph_function\n",
      "File \u001b[0;32m~/.conda/envs/tf-gpu/lib/python3.9/site-packages/tensorflow/python/framework/func_graph.py:990\u001b[0m, in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    987\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    988\u001b[0m   _, original_func \u001b[38;5;241m=\u001b[39m tf_decorator\u001b[38;5;241m.\u001b[39munwrap(python_func)\n\u001b[0;32m--> 990\u001b[0m func_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mpython_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfunc_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfunc_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    992\u001b[0m \u001b[38;5;66;03m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[39;00m\n\u001b[1;32m    993\u001b[0m \u001b[38;5;66;03m# TensorArrays and `None`s.\u001b[39;00m\n\u001b[1;32m    994\u001b[0m func_outputs \u001b[38;5;241m=\u001b[39m nest\u001b[38;5;241m.\u001b[39mmap_structure(convert, func_outputs,\n\u001b[1;32m    995\u001b[0m                                   expand_composites\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/.conda/envs/tf-gpu/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py:634\u001b[0m, in \u001b[0;36mFunction._defun_with_scope.<locals>.wrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    632\u001b[0m     xla_context\u001b[38;5;241m.\u001b[39mExit()\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 634\u001b[0m   out \u001b[38;5;241m=\u001b[39m \u001b[43mweak_wrapped_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__wrapped__\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/.conda/envs/tf-gpu/lib/python3.9/site-packages/tensorflow_probability/python/distributions/gamma.py:421\u001b[0m, in \u001b[0;36mrandom_gamma\u001b[0;34m(shape, concentration, rate, seed)\u001b[0m\n\u001b[1;32m    419\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [grad_a, grad_b]\n\u001b[1;32m    420\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m samples, grad\n\u001b[0;32m--> 421\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconcentration\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrate\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/tf-gpu/lib/python3.9/site-packages/tensorflow/python/ops/custom_gradient.py:261\u001b[0m, in \u001b[0;36mBind.__call__\u001b[0;34m(self, *a, **k)\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mk):\n\u001b[0;32m--> 261\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_f\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/tf-gpu/lib/python3.9/site-packages/tensorflow/python/ops/custom_gradient.py:217\u001b[0m, in \u001b[0;36mcustom_gradient.<locals>.decorated\u001b[0;34m(wrapped, args, kwargs)\u001b[0m\n\u001b[1;32m    215\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m _eager_mode_decorator(wrapped, args, kwargs)\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_graph_mode_decorator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwrapped\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/tf-gpu/lib/python3.9/site-packages/tensorflow/python/ops/custom_gradient.py:330\u001b[0m, in \u001b[0;36m_graph_mode_decorator\u001b[0;34m(f, args, kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m before_vars \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m([\n\u001b[1;32m    326\u001b[0m     v\u001b[38;5;241m.\u001b[39mref() \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m current_var_scope\u001b[38;5;241m.\u001b[39mglobal_variables() \u001b[38;5;241m+\u001b[39m\n\u001b[1;32m    327\u001b[0m     current_var_scope\u001b[38;5;241m.\u001b[39mlocal_variables()\n\u001b[1;32m    328\u001b[0m ])\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tape_lib\u001b[38;5;241m.\u001b[39mVariableWatcher() \u001b[38;5;28;01mas\u001b[39;00m variable_watcher:\n\u001b[0;32m--> 330\u001b[0m   result, grad_fn \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    332\u001b[0m args \u001b[38;5;241m=\u001b[39m nest\u001b[38;5;241m.\u001b[39mflatten(args)\n\u001b[1;32m    333\u001b[0m flat_result \u001b[38;5;241m=\u001b[39m nest\u001b[38;5;241m.\u001b[39mflatten(result)\n",
      "File \u001b[0;32m~/.conda/envs/tf-gpu/lib/python3.9/site-packages/tensorflow_probability/python/distributions/gamma.py:390\u001b[0m, in \u001b[0;36mrandom_gamma.<locals>.sample\u001b[0;34m(concentration, rate)\u001b[0m\n\u001b[1;32m    383\u001b[0m \u001b[38;5;129m@tf\u001b[39m\u001b[38;5;241m.\u001b[39mcustom_gradient\n\u001b[1;32m    384\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msample\u001b[39m(concentration, rate):\n\u001b[1;32m    385\u001b[0m   sampler_impl \u001b[38;5;241m=\u001b[39m implementation_selection\u001b[38;5;241m.\u001b[39mimplementation_selecting(\n\u001b[1;32m    386\u001b[0m       fn_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgamma\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    387\u001b[0m       default_fn\u001b[38;5;241m=\u001b[39m_random_gamma_noncpu,\n\u001b[1;32m    388\u001b[0m       cpu_fn\u001b[38;5;241m=\u001b[39m_random_gamma_cpu)\n\u001b[0;32m--> 390\u001b[0m   samples \u001b[38;5;241m=\u001b[39m \u001b[43msampler_impl\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    391\u001b[0m \u001b[43m      \u001b[49m\u001b[43mshape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconcentration\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconcentration\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    393\u001b[0m   \u001b[38;5;66;03m# Ignore any gradient contributions that come from the implementation enum.\u001b[39;00m\n\u001b[1;32m    394\u001b[0m   \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgrad\u001b[39m(dy, _):\n",
      "File \u001b[0;32m~/.conda/envs/tf-gpu/lib/python3.9/site-packages/tensorflow_probability/python/internal/implementation_selection.py:148\u001b[0m, in \u001b[0;36mimplementation_selecting.<locals>.impl_selecting_fn\u001b[0;34m(**kwargs)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;66;03m# Call the default sampling impl and register the CPU-specialized impl.\u001b[39;00m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;66;03m# Grappler will kick in during session execution to optimize the graph.\u001b[39;00m\n\u001b[1;32m    147\u001b[0m samples, runtime \u001b[38;5;241m=\u001b[39m defun_default_fn(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 148\u001b[0m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mregister\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdefun_cpu_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m samples, runtime\n",
      "File \u001b[0;32m~/.conda/envs/tf-gpu/lib/python3.9/site-packages/tensorflow/python/eager/function.py:3390\u001b[0m, in \u001b[0;36mregister\u001b[0;34m(func, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3388\u001b[0m concrete_func \u001b[38;5;241m=\u001b[39m func\u001b[38;5;241m.\u001b[39mget_concrete_function(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   3389\u001b[0m concrete_func\u001b[38;5;241m.\u001b[39madd_to_graph()\n\u001b[0;32m-> 3390\u001b[0m \u001b[43mconcrete_func\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_gradient_functions_to_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3391\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m concrete_func\n",
      "File \u001b[0;32m~/.conda/envs/tf-gpu/lib/python3.9/site-packages/tensorflow/python/eager/function.py:2057\u001b[0m, in \u001b[0;36mConcreteFunction.add_gradient_functions_to_graph\u001b[0;34m(self, g)\u001b[0m\n\u001b[1;32m   2054\u001b[0m   g \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39mget_default_graph()\n\u001b[1;32m   2055\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_delayed_rewrite_functions\u001b[38;5;241m.\u001b[39mforward()\u001b[38;5;241m.\u001b[39madd_to_graph(g)\n\u001b[1;32m   2056\u001b[0m forward_function, backward_function \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m-> 2057\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_delayed_rewrite_functions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   2058\u001b[0m forward_function\u001b[38;5;241m.\u001b[39madd_to_graph(g)\n\u001b[1;32m   2059\u001b[0m backward_function\u001b[38;5;241m.\u001b[39madd_to_graph(g)\n",
      "File \u001b[0;32m~/.conda/envs/tf-gpu/lib/python3.9/site-packages/tensorflow/python/eager/function.py:631\u001b[0m, in \u001b[0;36m_DelayedRewriteGradientFunctions.forward_backward\u001b[0;34m(self, num_doutputs)\u001b[0m\n\u001b[1;32m    629\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m forward_backward \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    630\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m forward_backward\n\u001b[0;32m--> 631\u001b[0m forward, backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_construct_forward_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_doutputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cached_function_pairs[num_doutputs] \u001b[38;5;241m=\u001b[39m (forward, backward)\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m forward, backward\n",
      "File \u001b[0;32m~/.conda/envs/tf-gpu/lib/python3.9/site-packages/tensorflow/python/eager/function.py:674\u001b[0m, in \u001b[0;36m_DelayedRewriteGradientFunctions._construct_forward_backward\u001b[0;34m(self, num_doutputs)\u001b[0m\n\u001b[1;32m    671\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_func_graph\u001b[38;5;241m.\u001b[39mas_default():\n\u001b[1;32m    672\u001b[0m   backwards_graph \u001b[38;5;241m=\u001b[39m func_graph_module\u001b[38;5;241m.\u001b[39mFuncGraph(\n\u001b[1;32m    673\u001b[0m       _backward_name(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_func_graph\u001b[38;5;241m.\u001b[39mname))\n\u001b[0;32m--> 674\u001b[0m   \u001b[43mfunc_graph_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc_graph_from_py_func\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbackwards_graph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m      \u001b[49m\u001b[43mpython_func\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_backprop_function\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m      \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[43m      \u001b[49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[43m      \u001b[49m\u001b[43mfunc_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbackwards_graph\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    680\u001b[0m   backwards_graph_captures \u001b[38;5;241m=\u001b[39m backwards_graph\u001b[38;5;241m.\u001b[39mexternal_captures\n\u001b[1;32m    681\u001b[0m   captures_from_forward \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    682\u001b[0m       c \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m backwards_graph_captures \u001b[38;5;28;01mif\u001b[39;00m\n\u001b[1;32m    683\u001b[0m       \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(c, ops\u001b[38;5;241m.\u001b[39mEagerTensor) \u001b[38;5;129;01mand\u001b[39;00m c\u001b[38;5;241m.\u001b[39mgraph \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_func_graph]\n",
      "File \u001b[0;32m~/.conda/envs/tf-gpu/lib/python3.9/site-packages/tensorflow/python/framework/func_graph.py:990\u001b[0m, in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    987\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    988\u001b[0m   _, original_func \u001b[38;5;241m=\u001b[39m tf_decorator\u001b[38;5;241m.\u001b[39munwrap(python_func)\n\u001b[0;32m--> 990\u001b[0m func_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mpython_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfunc_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfunc_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    992\u001b[0m \u001b[38;5;66;03m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[39;00m\n\u001b[1;32m    993\u001b[0m \u001b[38;5;66;03m# TensorArrays and `None`s.\u001b[39;00m\n\u001b[1;32m    994\u001b[0m func_outputs \u001b[38;5;241m=\u001b[39m nest\u001b[38;5;241m.\u001b[39mmap_structure(convert, func_outputs,\n\u001b[1;32m    995\u001b[0m                                   expand_composites\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/.conda/envs/tf-gpu/lib/python3.9/site-packages/tensorflow/python/eager/function.py:665\u001b[0m, in \u001b[0;36m_DelayedRewriteGradientFunctions._construct_forward_backward.<locals>._backprop_function\u001b[0;34m(*grad_ys)\u001b[0m\n\u001b[1;32m    663\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_backprop_function\u001b[39m(\u001b[38;5;241m*\u001b[39mgrad_ys):\n\u001b[1;32m    664\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 665\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgradients_util\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_GradientsHelper\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m    666\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrainable_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    667\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_func_graph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_ys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_ys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m        \u001b[49m\u001b[43msrc_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_func_graph\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/tf-gpu/lib/python3.9/site-packages/tensorflow/python/ops/gradients_util.py:683\u001b[0m, in \u001b[0;36m_GradientsHelper\u001b[0;34m(ys, xs, grad_ys, name, colocate_gradients_with_ops, gate_gradients, aggregation_method, stop_gradients, unconnected_gradients, src_graph)\u001b[0m\n\u001b[1;32m    678\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m src_graph\u001b[38;5;241m.\u001b[39m_original_op(op):\n\u001b[1;32m    679\u001b[0m   \u001b[38;5;66;03m# pylint: enable=protected-access\u001b[39;00m\n\u001b[1;32m    680\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m grad_fn:\n\u001b[1;32m    681\u001b[0m     \u001b[38;5;66;03m# If grad_fn was found, do not use SymbolicGradient even for\u001b[39;00m\n\u001b[1;32m    682\u001b[0m     \u001b[38;5;66;03m# functions.\u001b[39;00m\n\u001b[0;32m--> 683\u001b[0m     in_grads \u001b[38;5;241m=\u001b[39m \u001b[43m_MaybeCompile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrad_scope\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    684\u001b[0m \u001b[43m                             \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mout_grads\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    685\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    686\u001b[0m     \u001b[38;5;66;03m# For function call ops, we add a 'SymbolicGradient'\u001b[39;00m\n\u001b[1;32m    687\u001b[0m     \u001b[38;5;66;03m# node to the graph to compute gradients.\u001b[39;00m\n\u001b[1;32m    688\u001b[0m     in_grads \u001b[38;5;241m=\u001b[39m _MaybeCompile(grad_scope, op, func_call,\n\u001b[1;32m    689\u001b[0m                              \u001b[38;5;28;01mlambda\u001b[39;00m: _SymGrad(op, out_grads))\n",
      "File \u001b[0;32m~/.conda/envs/tf-gpu/lib/python3.9/site-packages/tensorflow/python/ops/gradients_util.py:340\u001b[0m, in \u001b[0;36m_MaybeCompile\u001b[0;34m(scope, op, func, grad_fn)\u001b[0m\n\u001b[1;32m    337\u001b[0m     xla_compile \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    339\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m xla_compile:\n\u001b[0;32m--> 340\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgrad_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Exit early\u001b[39;00m\n\u001b[1;32m    342\u001b[0m \u001b[38;5;66;03m# If the gradients are supposed to be compiled separately, we give them a\u001b[39;00m\n\u001b[1;32m    343\u001b[0m \u001b[38;5;66;03m# _XlaScope name that is based on the name_scope of the gradients.  Otherwise\u001b[39;00m\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# they just inherit the existing _XlaScope name, which lets them be merged\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# together with the non-gradient computation.\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m xla_separate_compiled_gradients:\n",
      "File \u001b[0;32m~/.conda/envs/tf-gpu/lib/python3.9/site-packages/tensorflow/python/ops/gradients_util.py:684\u001b[0m, in \u001b[0;36m_GradientsHelper.<locals>.<lambda>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    678\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m src_graph\u001b[38;5;241m.\u001b[39m_original_op(op):\n\u001b[1;32m    679\u001b[0m   \u001b[38;5;66;03m# pylint: enable=protected-access\u001b[39;00m\n\u001b[1;32m    680\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m grad_fn:\n\u001b[1;32m    681\u001b[0m     \u001b[38;5;66;03m# If grad_fn was found, do not use SymbolicGradient even for\u001b[39;00m\n\u001b[1;32m    682\u001b[0m     \u001b[38;5;66;03m# functions.\u001b[39;00m\n\u001b[1;32m    683\u001b[0m     in_grads \u001b[38;5;241m=\u001b[39m _MaybeCompile(grad_scope, op, func_call,\n\u001b[0;32m--> 684\u001b[0m                              \u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[43mgrad_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mout_grads\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    685\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    686\u001b[0m     \u001b[38;5;66;03m# For function call ops, we add a 'SymbolicGradient'\u001b[39;00m\n\u001b[1;32m    687\u001b[0m     \u001b[38;5;66;03m# node to the graph to compute gradients.\u001b[39;00m\n\u001b[1;32m    688\u001b[0m     in_grads \u001b[38;5;241m=\u001b[39m _MaybeCompile(grad_scope, op, func_call,\n\u001b[1;32m    689\u001b[0m                              \u001b[38;5;28;01mlambda\u001b[39;00m: _SymGrad(op, out_grads))\n",
      "File \u001b[0;32m~/.conda/envs/tf-gpu/lib/python3.9/site-packages/tensorflow/python/ops/random_grad.py:114\u001b[0m, in \u001b[0;36m_StatelessRandomGammaV2Grad\u001b[0;34m(op, grad)\u001b[0m\n\u001b[1;32m    111\u001b[0m num_sample_dimensions \u001b[38;5;241m=\u001b[39m array_ops\u001b[38;5;241m.\u001b[39mshape(shape)[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m-\u001b[39m array_ops\u001b[38;5;241m.\u001b[39mrank(alpha)\n\u001b[1;32m    112\u001b[0m \u001b[38;5;66;03m# Make the parameters alpha broadcastable with samples by appending\u001b[39;00m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;66;03m# unit dimensions.\u001b[39;00m\n\u001b[0;32m--> 114\u001b[0m alpha_broadcastable \u001b[38;5;241m=\u001b[39m \u001b[43madd_leading_unit_dimensions\u001b[49m\u001b[43m(\u001b[49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[43m                                                  \u001b[49m\u001b[43mnum_sample_dimensions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    116\u001b[0m partial_a \u001b[38;5;241m=\u001b[39m gen_random_ops\u001b[38;5;241m.\u001b[39mrandom_gamma_grad(alpha_broadcastable, sample)\n\u001b[1;32m    118\u001b[0m \u001b[38;5;66;03m# The first two inputs are shape, seed, third input is alpha.\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/tf-gpu/lib/python3.9/site-packages/tensorflow/python/ops/random_grad.py:35\u001b[0m, in \u001b[0;36madd_leading_unit_dimensions\u001b[0;34m(x, num_dimensions)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21madd_leading_unit_dimensions\u001b[39m(x, num_dimensions):  \u001b[38;5;66;03m# pylint: disable=invalid-name\u001b[39;00m\n\u001b[1;32m     34\u001b[0m   new_shape \u001b[38;5;241m=\u001b[39m array_ops\u001b[38;5;241m.\u001b[39mconcat(\n\u001b[0;32m---> 35\u001b[0m       [\u001b[43marray_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mones\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnum_dimensions\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mint32\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m     36\u001b[0m        array_ops\u001b[38;5;241m.\u001b[39mshape(x)], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     37\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m array_ops\u001b[38;5;241m.\u001b[39mreshape(x, new_shape)\n",
      "File \u001b[0;32m~/.conda/envs/tf-gpu/lib/python3.9/site-packages/tensorflow/python/util/dispatch.py:201\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;124;03m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[39;00m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 201\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtarget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[1;32m    203\u001b[0m   \u001b[38;5;66;03m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[1;32m    204\u001b[0m   \u001b[38;5;66;03m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[1;32m    205\u001b[0m   result \u001b[38;5;241m=\u001b[39m dispatch(wrapper, args, kwargs)\n",
      "File \u001b[0;32m~/.conda/envs/tf-gpu/lib/python3.9/site-packages/tensorflow/python/ops/array_ops.py:3120\u001b[0m, in \u001b[0;36mones\u001b[0;34m(shape, dtype, name)\u001b[0m\n\u001b[1;32m   3116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   3117\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m   3118\u001b[0m     \u001b[38;5;66;03m# Create a constant if it won't be very big. Otherwise create a fill\u001b[39;00m\n\u001b[1;32m   3119\u001b[0m     \u001b[38;5;66;03m# op to prevent serialized GraphDefs from becoming too large.\u001b[39;00m\n\u001b[0;32m-> 3120\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43m_constant_if_small\u001b[49m\u001b[43m(\u001b[49m\u001b[43mone\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3121\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m output \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3122\u001b[0m       \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m~/.conda/envs/tf-gpu/lib/python3.9/site-packages/tensorflow/python/ops/array_ops.py:2804\u001b[0m, in \u001b[0;36m_constant_if_small\u001b[0;34m(value, shape, dtype, name)\u001b[0m\n\u001b[1;32m   2802\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_constant_if_small\u001b[39m(value, shape, dtype, name):\n\u001b[1;32m   2803\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 2804\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprod\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1000\u001b[39m:\n\u001b[1;32m   2805\u001b[0m       \u001b[38;5;28;01mreturn\u001b[39;00m constant(value, shape\u001b[38;5;241m=\u001b[39mshape, dtype\u001b[38;5;241m=\u001b[39mdtype, name\u001b[38;5;241m=\u001b[39mname)\n\u001b[1;32m   2806\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   2807\u001b[0m     \u001b[38;5;66;03m# Happens when shape is a Tensor, list with Tensor elements, etc.\u001b[39;00m\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mprod\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/.conda/envs/tf-gpu/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3045\u001b[0m, in \u001b[0;36mprod\u001b[0;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m   2927\u001b[0m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_prod_dispatcher)\n\u001b[1;32m   2928\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprod\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, keepdims\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue,\n\u001b[1;32m   2929\u001b[0m          initial\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue, where\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue):\n\u001b[1;32m   2930\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2931\u001b[0m \u001b[38;5;124;03m    Return the product of array elements over a given axis.\u001b[39;00m\n\u001b[1;32m   2932\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3043\u001b[0m \u001b[38;5;124;03m    10\u001b[39;00m\n\u001b[1;32m   3044\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 3045\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapreduction\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmultiply\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mprod\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3046\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeepdims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhere\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/tf-gpu/lib/python3.9/site-packages/numpy/core/fromnumeric.py:86\u001b[0m, in \u001b[0;36m_wrapreduction\u001b[0;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     84\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m reduction(axis\u001b[38;5;241m=\u001b[39maxis, out\u001b[38;5;241m=\u001b[39mout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpasskwargs)\n\u001b[0;32m---> 86\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mufunc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpasskwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/tf-gpu/lib/python3.9/site-packages/tensorflow/python/framework/ops.py:852\u001b[0m, in \u001b[0;36mTensor.__array__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    851\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__array__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 852\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[1;32m    853\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot convert a symbolic Tensor (\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m) to a numpy array.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    854\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m This error may indicate that you\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mre trying to pass a Tensor to\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    855\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m a NumPy call, which is not supported\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname))\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: Cannot convert a symbolic Tensor (gradients/stateless_random_gamma/StatelessRandomGammaV2_grad/sub:0) to a numpy array. This error may indicate that you're trying to pass a Tensor to a NumPy call, which is not supported"
     ]
    }
   ],
   "source": [
    "U = tfd.Gamma(8,1).sample(1000)\n",
    "sns.distplot(U)\n",
    "plt.title(\"Samples from an inverse gamma\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nlafon/.conda/envs/tf/lib/python3.7/site-packages/ipykernel_launcher.py:20: UserWarning: \n",
      "\n",
      "`distplot` is a deprecated function and will be removed in seaborn v0.14.0.\n",
      "\n",
      "Please adapt your code to use either `displot` (a figure-level function with\n",
      "similar flexibility) or `histplot` (an axes-level function for histograms).\n",
      "\n",
      "For a guide to updating your code to use the new functions, please see\n",
      "https://gist.github.com/mwaskom/de44147ed2974457ad6372750bbe5751\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def esc_alpha(x):\n",
    "    if x<0.3:\n",
    "        return 0.5\n",
    "    else :\n",
    "        return 8\n",
    "    \n",
    "\n",
    "def esc_beta(x):\n",
    "    if x<0.3:\n",
    "        return 50\n",
    "    else :\n",
    "        return 1\n",
    "\n",
    "alpha_beta = np.zeros((1000,2))\n",
    "alpha_beta[...,0] = tf.map_fn(esc_alpha,Z)\n",
    "alpha_beta[...,1] = tf.map_fn(esc_beta, Z)\n",
    "\n",
    "X_new = tfd.Gamma(alpha_beta[...,0],alpha_beta[...,1]).sample()\n",
    "print(X_new.shape)\n",
    "sns.distplot(X_new)\n",
    "plt.title(\"Samples from an inverse gamma\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_93760/3053886004.py:7: UserWarning: \n",
      "\n",
      "`distplot` is a deprecated function and will be removed in seaborn v0.14.0.\n",
      "\n",
      "Please adapt your code to use either `displot` (a figure-level function with\n",
      "similar flexibility) or `histplot` (an axes-level function for histograms).\n",
      "\n",
      "For a guide to updating your code to use the new functions, please see\n",
      "https://gist.github.com/mwaskom/de44147ed2974457ad6372750bbe5751\n",
      "\n",
      "  sns.distplot(Y_sort)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Ber = tfd.Bernoulli(probs=0.3, dtype ='float').sample(1000) \n",
    "IG1 = tfd.InverseGamma(concentration =1.8, scale = 1 ).sample(1000)\n",
    "IG2 = tfd.InverseGamma(concentration =1.8, scale = 1 ).sample(1000)\n",
    "Y = IG1*Ber +(6+IG2)*(1-Ber)\n",
    "\n",
    "Y_sort=tf.sort(Y)\n",
    "sns.distplot(Y_sort)\n",
    "plt.title(\"Samples from Y\")\n",
    "plt.show()\n",
    "train_dataset = Y[:250]\n",
    "train_dataset = tf.reshape(train_dataset,[250,1])\n",
    "eval_dataset = Y[250:]\n",
    "eval_dataset = tf.reshape(eval_dataset,[750,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_size = 1\n",
    "input_shape =(1,)\n",
    "nbparamsl = 2\n",
    "nbparamsobs = 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior = tfd.InverseGamma(concentration =3, scale = 1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder3 = tfk.Sequential([\n",
    "    tfkl.InputLayer(input_shape=[1]),\n",
    "    tfkl.Dense(4,\n",
    "               activation='relu'),\n",
    "    tfkl.Dense(8,\n",
    "               activation='relu'),\n",
    "    tfkl.Dense(8,\n",
    "               activation='relu'),\n",
    "    tfkl.Dense(2,\n",
    "               activation=None,\n",
    "              name = 'inv_gamma_beta_posterior'),\n",
    "    tfkl.Lambda(lambda x: tf.abs(x)+0.001),\n",
    "])    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder3.add( tfpl.DistributionLambda(\n",
    "    make_distribution_fn=lambda t: tfd.Normal(\n",
    "        loc=t[...,0], scale=t[...,1]),\n",
    "    activity_regularizer=tfpl.KLDivergenceRegularizer(prior)))\n",
    "encoder3.add(tfkl.Reshape(target_shape=[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_40 (Dense)            (None, 4)                 8         \n",
      "                                                                 \n",
      " dense_41 (Dense)            (None, 8)                 40        \n",
      "                                                                 \n",
      " dense_42 (Dense)            (None, 8)                 72        \n",
      "                                                                 \n",
      " inv_gamma_beta_posterior (D  (None, 2)                18        \n",
      " ense)                                                           \n",
      "                                                                 \n",
      " lambda_10 (Lambda)          (None, 2)                 0         \n",
      "                                                                 \n",
      " distribution_lambda_5 (Dist  ((None,),                0         \n",
      " ributionLambda)              (None,))                           \n",
      "                                                                 \n",
      " reshape_5 (Reshape)         (None, 1)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 138\n",
      "Trainable params: 138\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder3 = tfk.Sequential([\n",
    "    tfkl.InputLayer(input_shape=[1]),\n",
    "    tfkl.Dense(4, use_bias=True, activation='relu'),\n",
    "    tfkl.Dense(8, use_bias=True, activation='relu'),\n",
    "    tfkl.Dense(8, use_bias=True, activation='relu'),\n",
    "    tfkl.Dense(4, use_bias=True),\n",
    "    tfkl.Lambda(lambda x: tf.abs(x)+0.001),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder3.add(tfkl.Dense(tfpl.IndependentNormal.params_size(1)))\n",
    "decoder3.add(tfpl.IndependentNormal(1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae3 = tfk.Model(inputs=encoder3.inputs,\n",
    "                outputs=decoder3(encoder3.outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_log_likelihood = lambda x, rv_x: -rv_x.log_prob(x)\n",
    "\n",
    "vae3.compile(optimizer=tf.optimizers.Adam(learning_rate=1e-4),\n",
    "            loss=negative_log_likelihood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 3.9019\n",
      "Epoch 2/100\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 3.9784\n",
      "Epoch 3/100\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 3.7832\n",
      "Epoch 4/100\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 3.7688\n",
      "Epoch 5/100\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 4.0504\n",
      "Epoch 6/100\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 3.7335\n",
      "Epoch 7/100\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 4.0603\n",
      "Epoch 8/100\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.0039\n",
      "Epoch 9/100\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 3.8970\n",
      "Epoch 10/100\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 4.0519\n",
      "Epoch 11/100\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 3.8722\n",
      "Epoch 12/100\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 4.2319\n",
      "Epoch 13/100\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 4.0581\n",
      "Epoch 14/100\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 3.8243\n",
      "Epoch 15/100\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 3.8609\n",
      "Epoch 16/100\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 3.9974\n",
      "Epoch 17/100\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 4.0643\n",
      "Epoch 18/100\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 3.9054\n",
      "Epoch 19/100\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 3.9145\n",
      "Epoch 20/100\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 3.9879\n",
      "Epoch 21/100\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 3.8004\n",
      "Epoch 22/100\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 3.9925\n",
      "Epoch 23/100\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 3.9645\n",
      "Epoch 24/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 3.7640\n",
      "Epoch 25/100\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 3.9918\n",
      "Epoch 26/100\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.0826\n",
      "Epoch 27/100\n",
      "16/16 [==============================] - 0s 5ms/step - loss: nan\n",
      "Epoch 28/100\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 3.8832\n",
      "Epoch 29/100\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 3.8366\n",
      "Epoch 30/100\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.0335\n",
      "Epoch 31/100\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.0660\n",
      "Epoch 32/100\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 3.7278\n",
      "Epoch 33/100\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.0052\n",
      "Epoch 34/100\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 3.8720\n",
      "Epoch 35/100\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.0942\n",
      "Epoch 36/100\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 3.8581\n",
      "Epoch 37/100\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 3.8813\n",
      "Epoch 38/100\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 4.0332\n",
      "Epoch 39/100\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.0444\n",
      "Epoch 40/100\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.3124\n",
      "Epoch 41/100\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.0222\n",
      "Epoch 42/100\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 3.8964\n",
      "Epoch 43/100\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.0922\n",
      "Epoch 44/100\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.0530\n",
      "Epoch 45/100\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 3.9518\n",
      "Epoch 46/100\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.5938\n",
      "Epoch 47/100\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.0389\n",
      "Epoch 48/100\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 3.9297\n",
      "Epoch 49/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.0832\n",
      "Epoch 50/100\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 3.9126\n",
      "Epoch 51/100\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 3.8683\n",
      "Epoch 52/100\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 3.8296\n",
      "Epoch 53/100\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.1087\n",
      "Epoch 54/100\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 3.9025\n",
      "Epoch 55/100\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 3.7923\n",
      "Epoch 56/100\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 3.8988\n",
      "Epoch 57/100\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 3.9607\n",
      "Epoch 58/100\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 4.0002\n",
      "Epoch 59/100\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 3.9909\n",
      "Epoch 60/100\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.3192\n",
      "Epoch 61/100\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.1051\n",
      "Epoch 62/100\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 4.0644\n",
      "Epoch 63/100\n",
      "16/16 [==============================] - 0s 6ms/step - loss: nan\n",
      "Epoch 64/100\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 4.2665\n",
      "Epoch 65/100\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.2473\n",
      "Epoch 66/100\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.1603\n",
      "Epoch 67/100\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 4.2325\n",
      "Epoch 68/100\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.2636\n",
      "Epoch 69/100\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.3672\n",
      "Epoch 70/100\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.1152\n",
      "Epoch 71/100\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.2923\n",
      "Epoch 72/100\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.3097\n",
      "Epoch 73/100\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.2835\n",
      "Epoch 74/100\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 4.3549\n",
      "Epoch 75/100\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 4.2683\n",
      "Epoch 76/100\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 4.3555\n",
      "Epoch 77/100\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 4.1356\n",
      "Epoch 78/100\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.1918\n",
      "Epoch 79/100\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.4254\n",
      "Epoch 80/100\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.4629\n",
      "Epoch 81/100\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.3321\n",
      "Epoch 82/100\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.2853\n",
      "Epoch 83/100\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.3338\n",
      "Epoch 84/100\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.1052\n",
      "Epoch 85/100\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.2933\n",
      "Epoch 86/100\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 4.3518\n",
      "Epoch 87/100\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 4.2310\n",
      "Epoch 88/100\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.2672\n",
      "Epoch 89/100\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 4.2579\n",
      "Epoch 90/100\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.3162\n",
      "Epoch 91/100\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 4.3125\n",
      "Epoch 92/100\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 4.2819\n",
      "Epoch 93/100\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.3847\n",
      "Epoch 94/100\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.3821\n",
      "Epoch 95/100\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.2967\n",
      "Epoch 96/100\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.3409\n",
      "Epoch 97/100\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.4137\n",
      "Epoch 98/100\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.0224\n",
      "Epoch 99/100\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.2157\n",
      "Epoch 100/100\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.3133\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f3eb39ad910>"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vae3.fit(train_dataset,train_dataset,batch_size=16,epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nlafon/.conda/envs/tf/lib/python3.7/site-packages/ipykernel_launcher.py:4: UserWarning: \n",
      "\n",
      "`distplot` is a deprecated function and will be removed in seaborn v0.14.0.\n",
      "\n",
      "Please adapt your code to use either `displot` (a figure-level function with\n",
      "similar flexibility) or `histplot` (an axes-level function for histograms).\n",
      "\n",
      "For a guide to updating your code to use the new functions, please see\n",
      "https://gist.github.com/mwaskom/de44147ed2974457ad6372750bbe5751\n",
      "\n",
      "  after removing the cwd from sys.path.\n",
      "/home/nlafon/.conda/envs/tf/lib/python3.7/site-packages/ipykernel_launcher.py:6: UserWarning: \n",
      "\n",
      "`distplot` is a deprecated function and will be removed in seaborn v0.14.0.\n",
      "\n",
      "Please adapt your code to use either `displot` (a figure-level function with\n",
      "similar flexibility) or `histplot` (an axes-level function for histograms).\n",
      "\n",
      "For a guide to updating your code to use the new functions, please see\n",
      "https://gist.github.com/mwaskom/de44147ed2974457ad6372750bbe5751\n",
      "\n",
      "  \n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nlafon/.conda/envs/tf/lib/python3.7/site-packages/ipykernel_launcher.py:12: UserWarning: \n",
      "\n",
      "`distplot` is a deprecated function and will be removed in seaborn v0.14.0.\n",
      "\n",
      "Please adapt your code to use either `displot` (a figure-level function with\n",
      "similar flexibility) or `histplot` (an axes-level function for histograms).\n",
      "\n",
      "For a guide to updating your code to use the new functions, please see\n",
      "https://gist.github.com/mwaskom/de44147ed2974457ad6372750bbe5751\n",
      "\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "samples_y = Y\n",
    "X_gen = prior.sample((1000))\n",
    "samples_y_gen = tf.reshape(decoder3(X_gen),(1000,))\n",
    "sns.distplot(samples_y)\n",
    "plt.figure()\n",
    "sns.distplot(samples_y_gen)\n",
    "plt.title(\"Samples from Y and our generative approach\")\n",
    "plt.show()\n",
    "plt.figure()\n",
    "samples_y_gen_sort = tf.sort(samples_y_gen)\n",
    "plt.figure()\n",
    "sns.distplot(samples_y_gen_sort[:950])\n",
    "plt.title(\"Samples from Y and our generative approach\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "zeros_traindataset=tf.zeros(train_dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 1)\n"
     ]
    }
   ],
   "source": [
    "Y_input = tfk.Input(\n",
    "    shape=[1], name=\"Y_train\"\n",
    ")  \n",
    "zeros_input = tfk.Input([1], name=\"Zeros_train\")\n",
    "dist = tfkl.Dense(4, use_bias=True, activation='relu')(Y_input)\n",
    "dist = tfkl.Dense(8, use_bias=True, activation='relu')(dist)\n",
    "dist = tfkl.Dense(8, use_bias=True, activation='relu')(dist)\n",
    "dist = tfkl.Dense(2, use_bias=True)(dist)\n",
    "dist = tfkl.Lambda(lambda x: tf.abs(x)+0.001)(dist)\n",
    "prior = tfkl.Dense(2,use_bias=True,activation=None, bias_initializer=tfk.initializers.Ones()\n",
    ")(zeros_input)\n",
    "prior = tfkl.Lambda(lambda x: tf.abs(x)+0.001)(prior)\n",
    "#prior = tfpl.DistributionLambda(\n",
    " #   make_distribution_fn=lambda t: tfd.InverseGamma(\n",
    "  #      concentration=t[... , 0], scale=t[...,1])\n",
    "#)(prior)\n",
    "prior = tfd.InverseGamma(concentration=prior[... , 0], scale = prior[..., 0])\n",
    "\n",
    "posterior = tfpl.DistributionLambda(\n",
    "    make_distribution_fn=lambda t: tfd.InverseGamma(\n",
    "        concentration=t[... , 0], scale=t[...,1]),\n",
    "    activity_regularizer=tfpl.KLDivergenceRegularizer(prior)\n",
    ")(dist)\n",
    "posterior = tfkl.Reshape(target_shape=[1])(posterior) \n",
    "print(posterior.shape)\n",
    "encoder2 = tfk.Model(\n",
    "    inputs=[Y_input,zeros_input],\n",
    "    outputs=[posterior],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_10\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " Y_train (InputLayer)           [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " dense_93 (Dense)               (None, 4)            8           ['Y_train[0][0]']                \n",
      "                                                                                                  \n",
      " dense_94 (Dense)               (None, 8)            40          ['dense_93[0][0]']               \n",
      "                                                                                                  \n",
      " dense_95 (Dense)               (None, 8)            72          ['dense_94[0][0]']               \n",
      "                                                                                                  \n",
      " dense_96 (Dense)               (None, 2)            18          ['dense_95[0][0]']               \n",
      "                                                                                                  \n",
      " lambda_28 (Lambda)             (None, 2)            0           ['dense_96[0][0]']               \n",
      "                                                                                                  \n",
      " distribution_lambda_16 (Distri  ((None,),           0           ['lambda_28[0][0]']              \n",
      " butionLambda)                   (None,))                                                         \n",
      "                                                                                                  \n",
      " Zeros_train (InputLayer)       [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " reshape_8 (Reshape)            (None, 1)            0           ['distribution_lambda_16[0][0]'] \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 138\n",
      "Trainable params: 138\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder2 = tfk.Sequential([\n",
    "    tfkl.InputLayer(input_shape=[1]),\n",
    "    tfkl.Dense(4, use_bias=True, activation='relu'),\n",
    "    tfkl.Dense(8, use_bias=True, activation='relu'),\n",
    "    tfkl.Dense(8, use_bias=True, activation='relu'),\n",
    "    tfkl.Dense(4, use_bias=True),\n",
    "    tfkl.Lambda(lambda x: tf.abs(x)+0.001),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder2.add(tfkl.Dense(tfpl.IndependentNormal.params_size(1)))\n",
    "decoder2.add(tfpl.IndependentNormal(1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_14\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_98 (Dense)            (None, 4)                 8         \n",
      "                                                                 \n",
      " dense_99 (Dense)            (None, 8)                 40        \n",
      "                                                                 \n",
      " dense_100 (Dense)           (None, 8)                 72        \n",
      "                                                                 \n",
      " dense_101 (Dense)           (None, 4)                 36        \n",
      "                                                                 \n",
      " lambda_30 (Lambda)          (None, 4)                 0         \n",
      "                                                                 \n",
      " dense_102 (Dense)           (None, 2)                 10        \n",
      "                                                                 \n",
      " independent_normal_7 (Indep  ((None, 1),              0         \n",
      " endentNormal)                (None, 1))                         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 166\n",
      "Trainable params: 166\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "decoder2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae2 = tfk.Model(inputs=encoder2.inputs,\n",
    "                outputs=decoder2(encoder2.outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_log_likelihood = lambda x, rv_x: -rv_x.log_prob(x)\n",
    "\n",
    "vae2.compile(optimizer=tf.optimizers.Adam(learning_rate=1e-4),\n",
    "            loss=negative_log_likelihood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "in user code:\n\n    File \"/home/nlafon/.conda/envs/tf/lib/python3.7/site-packages/keras/engine/training.py\", line 1051, in train_function  *\n        return step_function(self, iterator)\n    File \"/home/nlafon/.conda/envs/tf/lib/python3.7/site-packages/keras/engine/training.py\", line 1040, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/nlafon/.conda/envs/tf/lib/python3.7/site-packages/keras/engine/training.py\", line 1030, in run_step  **\n        outputs = model.train_step(data)\n    File \"/home/nlafon/.conda/envs/tf/lib/python3.7/site-packages/keras/engine/training.py\", line 890, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/home/nlafon/.conda/envs/tf/lib/python3.7/site-packages/keras/engine/training.py\", line 949, in compute_loss\n        y, y_pred, sample_weight, regularization_losses=self.losses)\n    File \"/home/nlafon/.conda/envs/tf/lib/python3.7/site-packages/keras/engine/compile_utils.py\", line 240, in __call__\n        total_loss_metric_value, sample_weight=batch_dim)\n    File \"/home/nlafon/.conda/envs/tf/lib/python3.7/site-packages/keras/utils/metrics_utils.py\", line 70, in decorated\n        update_op = update_state_fn(*args, **kwargs)\n    File \"/home/nlafon/.conda/envs/tf/lib/python3.7/site-packages/keras/metrics/base_metric.py\", line 140, in update_state_fn\n        return ag_update_state(*args, **kwargs)\n    File \"/home/nlafon/.conda/envs/tf/lib/python3.7/site-packages/keras/metrics/base_metric.py\", line 450, in update_state  **\n        sample_weight, values)\n    File \"/home/nlafon/.conda/envs/tf/lib/python3.7/site-packages/keras/engine/keras_tensor.py\", line 255, in __array__\n        f'You are passing {self}, an intermediate Keras symbolic input/output, '\n\n    TypeError: You are passing KerasTensor(type_spec=TensorSpec(shape=(), dtype=tf.float32, name=None), name='Placeholder:0', description=\"created by layer 'tf.cast_3'\"), an intermediate Keras symbolic input/output, to a TF API that does not allow registering custom dispatchers, such as `tf.cond`, `tf.function`, gradient tapes, or `tf.map_fn`. Keras Functional model construction only supports TF API calls that *do* support dispatching, such as `tf.math.add` or `tf.reshape`. Other APIs cannot be called directly on symbolic Kerasinputs/outputs. You can work around this limitation by putting the operation in a custom Keras layer `call` and calling that layer on this symbolic input/output.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_72129/2994830293.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0meval_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m          batch_size=16,epochs=100)\n\u001b[0m",
      "\u001b[0;32m~/.conda/envs/tf/lib/python3.7/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tf/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                     \u001b[0mretval_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m                 \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: in user code:\n\n    File \"/home/nlafon/.conda/envs/tf/lib/python3.7/site-packages/keras/engine/training.py\", line 1051, in train_function  *\n        return step_function(self, iterator)\n    File \"/home/nlafon/.conda/envs/tf/lib/python3.7/site-packages/keras/engine/training.py\", line 1040, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/nlafon/.conda/envs/tf/lib/python3.7/site-packages/keras/engine/training.py\", line 1030, in run_step  **\n        outputs = model.train_step(data)\n    File \"/home/nlafon/.conda/envs/tf/lib/python3.7/site-packages/keras/engine/training.py\", line 890, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/home/nlafon/.conda/envs/tf/lib/python3.7/site-packages/keras/engine/training.py\", line 949, in compute_loss\n        y, y_pred, sample_weight, regularization_losses=self.losses)\n    File \"/home/nlafon/.conda/envs/tf/lib/python3.7/site-packages/keras/engine/compile_utils.py\", line 240, in __call__\n        total_loss_metric_value, sample_weight=batch_dim)\n    File \"/home/nlafon/.conda/envs/tf/lib/python3.7/site-packages/keras/utils/metrics_utils.py\", line 70, in decorated\n        update_op = update_state_fn(*args, **kwargs)\n    File \"/home/nlafon/.conda/envs/tf/lib/python3.7/site-packages/keras/metrics/base_metric.py\", line 140, in update_state_fn\n        return ag_update_state(*args, **kwargs)\n    File \"/home/nlafon/.conda/envs/tf/lib/python3.7/site-packages/keras/metrics/base_metric.py\", line 450, in update_state  **\n        sample_weight, values)\n    File \"/home/nlafon/.conda/envs/tf/lib/python3.7/site-packages/keras/engine/keras_tensor.py\", line 255, in __array__\n        f'You are passing {self}, an intermediate Keras symbolic input/output, '\n\n    TypeError: You are passing KerasTensor(type_spec=TensorSpec(shape=(), dtype=tf.float32, name=None), name='Placeholder:0', description=\"created by layer 'tf.cast_3'\"), an intermediate Keras symbolic input/output, to a TF API that does not allow registering custom dispatchers, such as `tf.cond`, `tf.function`, gradient tapes, or `tf.map_fn`. Keras Functional model construction only supports TF API calls that *do* support dispatching, such as `tf.math.add` or `tf.reshape`. Other APIs cannot be called directly on symbolic Kerasinputs/outputs. You can work around this limitation by putting the operation in a custom Keras layer `call` and calling that layer on this symbolic input/output.\n"
     ]
    }
   ],
   "source": [
    "vae2.fit(\n",
    "    {\"Y_train\" : train_dataset, \"Zeros_train\" : zeros_traindataset},\n",
    "    train_dataset,\n",
    "    validation_data = ((eval_dataset,tf.zeros(eval_dataset.shape)),eval_dataset),\n",
    "         batch_size=16,epochs=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(5.192957, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "alpha = [1., 1,2,2,2]\n",
    "dist = tfd.Dirichlet(alpha)\n",
    "\n",
    "Diri2 = tfd.Dirichlet([2,2,1,1,1])\n",
    "\n",
    "print(Diri2.log_prob([0.5,0.5,0,0,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ot\n",
    "def T(X,Y,n):\n",
    "    MX = ot.dist(X,X,'sqeuclidean')\n",
    "    MY = ot.dist(Y,Y,'sqeuclidean')\n",
    "    T1 = (1/n**2)*np.sum(MX*MY)\n",
    "    print(T1)\n",
    "    T2 = (2/n**3)*np.sum(MX@MY)\n",
    "    print(T2)\n",
    "    T3 = (1/n**4)*np.sum(np.tensordot(MX,MY,axes=0))\n",
    "    print(T3)\n",
    "    return T1-T2+T3\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mu1 = 0\n",
    "sig1 = 2\n",
    "mu2 = 0\n",
    "sig2=1\n",
    "n1 = 10\n",
    "n2 = 10\n",
    "d=5\n",
    "\n",
    "mvn1 = tfd.MultivariateNormalDiag(\n",
    "    loc=[mu1],\n",
    "scale_diag=[sig1])\n",
    "mvn2 = tfd.MultivariateNormalDiag(\n",
    "    loc=[mu2],\n",
    "scale_diag=[sig2])\n",
    "\n",
    "\n",
    "s1 = mvn1.sample([n1,d])\n",
    "s2 = mvn2.sample([n2,d])\n",
    "\n",
    "s1 = tf.reshape(s1,(n1,d)).numpy()\n",
    "s2=tf.reshape(s2,(n2,d)).numpy()\n",
    "a, b = np.ones((n1,)) / n1, np.ones((n2,)) / n2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 10, 10, 10)\n",
      "3909761.2\n",
      "390.97615205078125\n"
     ]
    }
   ],
   "source": [
    "Ms1 = ot.dist(s1,s1,'sqeuclidean')\n",
    "Ms2 = ot.dist(s2,s2,'sqeuclidean')\n",
    "U = np.tensordot(Ms1, Ms2, axes=0)\n",
    "print(U.shape)\n",
    "print(np.sum(U))\n",
    "c = 0\n",
    "for i in range(10):\n",
    "    for j in range(10):\n",
    "        c+=np.sum(Ms1[i,j]*Ms2)\n",
    "print(c/(10**4))      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "452.4153515625\n",
      "796.2014375\n",
      "390.976125\n",
      "47.190039062500034\n",
      "0:00:00.009921\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "start=datetime.now()\n",
    "print(T(s1,s2,10))\n",
    "print(datetime.now()-start)\n",
    "\n",
    "#Statements\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.distributions.empirical_distribution import ECDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tail_mod(f,n,x):\n",
    "    return((n/n+1)*(f(x))+1/(n+1))\n",
    "\n",
    "def L(X,Y,u):\n",
    "    Nn = np.sum(X>u)\n",
    "    m=Y.shape[0]\n",
    "    fY=ECDF(Y[:,0])\n",
    "    Xmax = np.maximum(X,u)\n",
    "    Xu = np.minimum(Xmax,u)\n",
    "    res = 1 + 1/Nn*(np.sum(np.log(tail_mod(fY,m,Xmax)/tail_mod(fY,m,Xu))))\n",
    "    return(res)\n",
    "\n",
    "def KLu(X,Y,u):\n",
    "    return (-L(X,Y,u)-L(Y,X,u))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 1)\n"
     ]
    }
   ],
   "source": [
    "U = mvn1.sample(100)\n",
    "print(U.shape)\n",
    "V=mvn2.sample(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.51]\n",
      "1.02990099009901\n"
     ]
    }
   ],
   "source": [
    "f = ECDF(U[:,0])\n",
    "print(f([0]))\n",
    "print(tail_mod(f,100,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1221948683002871\n",
      "-2.192934430676925\n"
     ]
    }
   ],
   "source": [
    "print(L(U,V,1))\n",
    "print(KLu(U,V,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.65983132 1.50076176 1.59115817 0.27775383 0.5077529  0.06202301\n",
      " 0.9839488  0.95484596 1.15179295 1.45840271 1.93456741 1.9638443\n",
      " 1.80364892 0.10772214 1.01307057 0.88137972 0.96054553 0.13318956\n",
      " 1.22826603 0.42344816 1.48274824 0.43472965 0.94998859 0.15179545\n",
      " 0.80723373 1.25307932 0.54172163 1.74197801 0.95045517 1.14322014]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7f1e6065a280>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "U = np.random.uniform(0,2,30)\n",
    "Y = np.random.uniform(0,2,30)\n",
    "print(U)\n",
    "plt.xlim(0, 5)\n",
    "plt.ylim(0,5)\n",
    "plt.scatter(U,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import Prob_models as PM\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import tensorboard\n",
    "tfk = tf.keras\n",
    "tfkl = tf.keras.layers\n",
    "tfpl = tfp.layers\n",
    "tfd = tfp.distributions\n",
    "import random\n",
    "from netCDF4 import Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_log_likelihood = lambda x, rv_x: -rv_x.log_prob(x)\n",
    "seed = 100\n",
    "random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18244, 31)\n"
     ]
    }
   ],
   "source": [
    "ncfile = Dataset('/home/nlafon/These/4Dvarnetstochastic/Danube_river_network/Dataset_danube.nc',\"r\")\n",
    "L=[]\n",
    "for i in range(31):\n",
    "    L.append(ncfile['S'+str(i+1)][:].reshape(18244,1))\n",
    "        \n",
    "dataset = np.concatenate((L[0],L[1],L[2],L[3],L[4],L[5],L[6],L[7],L[8],L[9],L[10],L[11],L[12],L[13],L[14],L[15],L[16],L[17],L[18],L[19],L[20],L[21],L[22],L[23],L[24],L[25],L[26],L[27],L[28],L[29],L[30]),axis=1)\n",
    "print(dataset.shape)\n",
    "R4 = dataset[:,22:27]\n",
    "train_dataset = R4[::25,:]/10\n",
    "eval_dataset  = R4/10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-18 15:43:05.577545: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-01-18 15:43:05.580656: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 16. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    }
   ],
   "source": [
    "class Sphere_Encoder(tfk.Model):\n",
    "    \n",
    "    def __init__(self):      \n",
    "        super(Sphere_Encoder,self).__init__()\n",
    "        self.encoded_size = 4\n",
    "        self.prior        =  tfd.Independent(tfd.Normal(loc=tf.zeros(self.encoded_size), scale=1),\n",
    "                                reinterpreted_batch_ndims=1)\n",
    "        self.concat       = tfkl.Concatenate()\n",
    "        self.dense1       = tfkl.Dense(8,activation='relu')\n",
    "        self.dense2       = tfkl.Dense(8,activation='relu')\n",
    "        self.lambda1      = tfkl.Lambda(lambda x: tf.abs(x)+0.001)\n",
    "        self.dense3       = tfkl.Dense(tfpl.IndependentNormal.params_size(self.encoded_size))\n",
    "        self.ind_norm1    = tfpl.IndependentNormal(self.encoded_size,\n",
    "                                                   activity_regularizer=tfpl.KLDivergenceRegularizer(self.prior,\n",
    "                                                                                                     weight=1.0))\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x = self.dense1(inputs)\n",
    "        x = self.dense2(x)\n",
    "        x = self.lambda1(x)\n",
    "        x = self.dense3(x)\n",
    "        x = self.ind_norm1(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "class Sphere_Decoder(tfk.Model):\n",
    "    def __init__(self):\n",
    "        super(Sphere_Decoder,self).__init__()\n",
    "        self.K          = 5\n",
    "        self.dense1     = tfkl.Dense(5, use_bias=True, activation='relu')\n",
    "        self.dense2     = tfkl.Dense(10, use_bias=True, activation='relu')\n",
    "        self.dense31    = tfkl.Dense(tfpl.IndependentNormal(self.K).params_size(self.K))        \n",
    "        self.ind_norm11 = tfpl.IndependentNormal(self.K)\n",
    "        self.dense32    = tfkl.Dense(5,activation = 'relu', bias_initializer =\n",
    "                                     tfk.initializers.RandomUniform(minval=2, maxval=3))\n",
    "        #self.lambda12   = tfkl.Lambda(lambda x: tf.abs(x),activation='softmax')\n",
    "        self.concat     = tfkl.Concatenate()\n",
    "        self.lambda1    = tfkl.Lambda(lambda x : 1/(1+x))\n",
    "        \n",
    "        self.diri12 = tfpl.DistributionLambda(\n",
    "                            make_distribution_fn=lambda c: tfd.Dirichlet(\n",
    "                                c))\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x               = self.dense1(inputs)\n",
    "        x               = self.dense2(x)\n",
    "        #normal without forcing mu to be on the sphere\n",
    "        x               = self.dense31(x)\n",
    "        x               = self.ind_norm11(x)\n",
    "        #dirichlet output \n",
    "        #x               = self.dense32(x)\n",
    "        #x = self.lambda12(x)\n",
    "        #x = self.diri12(x)\n",
    "        return x\n",
    "    \n",
    "class Sphere_VAE(tfk.Model):\n",
    "    def __init__(self):      \n",
    "        super(Sphere_VAE,self).__init__()\n",
    "        self.encoder = Sphere_Encoder()\n",
    "        self.decoder = Sphere_Decoder()\n",
    "    \n",
    "    def call(self,inputs):\n",
    "        return self.decoder(self.encoder(inputs))\n",
    "vae = Sphere_VAE()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x7f218de06820>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint_filepath = '/home/nlafon/These/Extreme_VAE/tmp/Danube/Std_VAE/checkpoint'\n",
    "metric ='val_loss'\n",
    "model_checkpoint_callback = tfk.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=True,\n",
    "    monitor=metric,\n",
    "    mode='min',\n",
    "    save_best_only=True)\n",
    "vae.load_weights(checkpoint_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_log_likelihood = lambda x, rv_x: -rv_x.log_prob(x)\n",
    "\n",
    "vae.compile(optimizer=tf.optimizers.Adam(learning_rate=1e-3),\n",
    "            loss=negative_log_likelihood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "WARNING:tensorflow:Please fix your imports. Module tensorflow.python.training.tracking.data_structures has been moved to tensorflow.python.trackable.data_structures. The old module will be deleted in version 2.11.\n",
      "23/23 [==============================] - 3s 73ms/step - loss: 7.9664 - val_loss: 8.4709\n",
      "Epoch 2/1000\n",
      "23/23 [==============================] - 1s 52ms/step - loss: 7.9531 - val_loss: 8.5011\n",
      "Epoch 3/1000\n",
      "23/23 [==============================] - 1s 60ms/step - loss: 8.0000 - val_loss: 8.4620\n",
      "Epoch 4/1000\n",
      "23/23 [==============================] - 1s 59ms/step - loss: 7.9707 - val_loss: 8.4871\n",
      "Epoch 5/1000\n",
      "23/23 [==============================] - 1s 56ms/step - loss: 7.9617 - val_loss: 8.4679\n",
      "Epoch 6/1000\n",
      "23/23 [==============================] - 1s 31ms/step - loss: 7.9749 - val_loss: 8.5775\n",
      "Epoch 7/1000\n",
      "23/23 [==============================] - 1s 51ms/step - loss: 8.0430 - val_loss: 8.4786\n",
      "Epoch 8/1000\n",
      "23/23 [==============================] - 1s 57ms/step - loss: 7.9717 - val_loss: 8.5094\n",
      "Epoch 9/1000\n",
      "23/23 [==============================] - 1s 61ms/step - loss: 8.0272 - val_loss: 8.4573\n",
      "Epoch 10/1000\n",
      "23/23 [==============================] - 1s 54ms/step - loss: 8.0064 - val_loss: 8.5637\n",
      "Epoch 11/1000\n",
      "23/23 [==============================] - 1s 58ms/step - loss: 7.9707 - val_loss: 8.4641\n",
      "Epoch 12/1000\n",
      "23/23 [==============================] - 1s 59ms/step - loss: 7.9616 - val_loss: 8.4951\n",
      "Epoch 13/1000\n",
      "23/23 [==============================] - 1s 56ms/step - loss: 7.9646 - val_loss: 8.4834\n",
      "Epoch 14/1000\n",
      "23/23 [==============================] - 1s 55ms/step - loss: 8.0616 - val_loss: 8.4713\n",
      "Epoch 15/1000\n",
      "23/23 [==============================] - 1s 62ms/step - loss: 8.0425 - val_loss: 8.6117\n",
      "Epoch 16/1000\n",
      "23/23 [==============================] - 1s 63ms/step - loss: 8.0024 - val_loss: 8.4817\n",
      "Epoch 17/1000\n",
      "23/23 [==============================] - 1s 48ms/step - loss: 7.9632 - val_loss: 8.4734\n",
      "Epoch 18/1000\n",
      "23/23 [==============================] - 1s 60ms/step - loss: 7.9975 - val_loss: 8.5067\n",
      "Epoch 19/1000\n",
      "23/23 [==============================] - 1s 57ms/step - loss: 7.9910 - val_loss: 8.4634\n",
      "Epoch 20/1000\n",
      "23/23 [==============================] - 1s 55ms/step - loss: 7.9446 - val_loss: 8.4634\n",
      "Epoch 21/1000\n",
      "23/23 [==============================] - 1s 62ms/step - loss: 7.9329 - val_loss: 8.4507\n",
      "Epoch 22/1000\n",
      "23/23 [==============================] - 1s 55ms/step - loss: 7.9155 - val_loss: 8.5145\n",
      "Epoch 23/1000\n",
      "23/23 [==============================] - 1s 56ms/step - loss: 7.9198 - val_loss: 8.4737\n",
      "Epoch 24/1000\n",
      "23/23 [==============================] - 1s 62ms/step - loss: 8.0163 - val_loss: 8.5087\n",
      "Epoch 25/1000\n",
      "23/23 [==============================] - 1s 56ms/step - loss: 7.9685 - val_loss: 8.4609\n",
      "Epoch 26/1000\n",
      "23/23 [==============================] - 1s 50ms/step - loss: 8.0124 - val_loss: 8.4560\n",
      "Epoch 27/1000\n",
      "23/23 [==============================] - 1s 58ms/step - loss: 7.9309 - val_loss: 8.4521\n",
      "Epoch 28/1000\n",
      "23/23 [==============================] - 1s 56ms/step - loss: 7.9359 - val_loss: 8.4439\n",
      "Epoch 29/1000\n",
      "23/23 [==============================] - 1s 59ms/step - loss: 7.9209 - val_loss: 8.4519\n",
      "Epoch 30/1000\n",
      "23/23 [==============================] - 1s 59ms/step - loss: 7.9895 - val_loss: 8.4635\n",
      "Epoch 31/1000\n",
      "23/23 [==============================] - 1s 59ms/step - loss: 7.9608 - val_loss: 8.4645\n",
      "Epoch 32/1000\n",
      "23/23 [==============================] - 1s 59ms/step - loss: 7.9079 - val_loss: 8.4682\n",
      "Epoch 33/1000\n",
      "23/23 [==============================] - 1s 57ms/step - loss: 7.9585 - val_loss: 8.4514\n",
      "Epoch 34/1000\n",
      "23/23 [==============================] - 1s 59ms/step - loss: 8.0493 - val_loss: 8.4868\n",
      "Epoch 35/1000\n",
      "23/23 [==============================] - 1s 60ms/step - loss: 7.9797 - val_loss: 8.4678\n",
      "Epoch 36/1000\n",
      "23/23 [==============================] - 1s 53ms/step - loss: 7.9113 - val_loss: 8.4594\n",
      "Epoch 37/1000\n",
      "23/23 [==============================] - 1s 55ms/step - loss: 7.9235 - val_loss: 8.4811\n",
      "Epoch 38/1000\n",
      "23/23 [==============================] - 1s 61ms/step - loss: 7.9510 - val_loss: 8.4696\n",
      "Epoch 39/1000\n",
      "23/23 [==============================] - 1s 65ms/step - loss: 7.9491 - val_loss: 8.4360\n",
      "Epoch 40/1000\n",
      "23/23 [==============================] - 1s 58ms/step - loss: 7.8848 - val_loss: 8.5134\n",
      "Epoch 41/1000\n",
      "23/23 [==============================] - 1s 55ms/step - loss: 8.0061 - val_loss: 8.6106\n",
      "Epoch 42/1000\n",
      "23/23 [==============================] - 1s 61ms/step - loss: 8.0097 - val_loss: 8.4608\n",
      "Epoch 43/1000\n",
      "23/23 [==============================] - 1s 52ms/step - loss: 7.9293 - val_loss: 8.4366\n",
      "Epoch 44/1000\n",
      "23/23 [==============================] - 1s 29ms/step - loss: 7.9565 - val_loss: 8.4863\n",
      "Epoch 45/1000\n",
      "23/23 [==============================] - 1s 56ms/step - loss: 7.9598 - val_loss: 8.4968\n",
      "Epoch 46/1000\n",
      "23/23 [==============================] - 1s 58ms/step - loss: 7.9716 - val_loss: 8.4464\n",
      "Epoch 47/1000\n",
      "23/23 [==============================] - 1s 56ms/step - loss: 8.0572 - val_loss: 8.8190\n",
      "Epoch 48/1000\n",
      "23/23 [==============================] - 1s 62ms/step - loss: 7.9625 - val_loss: 8.4923\n",
      "Epoch 49/1000\n",
      "23/23 [==============================] - 1s 51ms/step - loss: 7.9739 - val_loss: 8.4437\n",
      "Epoch 50/1000\n",
      "23/23 [==============================] - 1s 53ms/step - loss: 7.9740 - val_loss: 8.4866\n",
      "Epoch 51/1000\n",
      "23/23 [==============================] - 1s 62ms/step - loss: 7.9481 - val_loss: 8.4394\n",
      "Epoch 52/1000\n",
      "23/23 [==============================] - 1s 56ms/step - loss: 7.9031 - val_loss: 8.4541\n",
      "Epoch 53/1000\n",
      "23/23 [==============================] - 1s 58ms/step - loss: 7.9310 - val_loss: 8.4732\n",
      "Epoch 54/1000\n",
      "23/23 [==============================] - 1s 57ms/step - loss: 7.9689 - val_loss: 8.4562\n",
      "Epoch 55/1000\n",
      "23/23 [==============================] - 1s 60ms/step - loss: 8.0232 - val_loss: 8.4219\n",
      "Epoch 56/1000\n",
      "23/23 [==============================] - 1s 54ms/step - loss: 7.9105 - val_loss: 8.4416\n",
      "Epoch 57/1000\n",
      "23/23 [==============================] - 1s 62ms/step - loss: 7.9750 - val_loss: 8.4976\n",
      "Epoch 58/1000\n",
      "23/23 [==============================] - 1s 58ms/step - loss: 7.9045 - val_loss: 8.4545\n",
      "Epoch 59/1000\n",
      "23/23 [==============================] - 1s 55ms/step - loss: 7.9921 - val_loss: 8.4165\n",
      "Epoch 60/1000\n",
      "23/23 [==============================] - 1s 62ms/step - loss: 7.9589 - val_loss: 8.5030\n",
      "Epoch 61/1000\n",
      "23/23 [==============================] - 1s 57ms/step - loss: 7.9125 - val_loss: 8.4262\n",
      "Epoch 62/1000\n",
      "23/23 [==============================] - 1s 56ms/step - loss: 7.9065 - val_loss: 8.4896\n",
      "Epoch 63/1000\n",
      "23/23 [==============================] - 1s 60ms/step - loss: 7.9693 - val_loss: 8.4262\n",
      "Epoch 64/1000\n",
      "23/23 [==============================] - 1s 61ms/step - loss: 7.9855 - val_loss: 8.4377\n",
      "Epoch 65/1000\n",
      "23/23 [==============================] - 1s 52ms/step - loss: 7.8866 - val_loss: 8.4457\n",
      "Epoch 66/1000\n",
      "23/23 [==============================] - 1s 54ms/step - loss: 7.9937 - val_loss: 8.4649\n",
      "Epoch 67/1000\n",
      "23/23 [==============================] - 1s 57ms/step - loss: 7.9844 - val_loss: 8.4503\n",
      "Epoch 68/1000\n",
      "23/23 [==============================] - 1s 55ms/step - loss: 8.0049 - val_loss: 8.4680\n",
      "Epoch 69/1000\n",
      "23/23 [==============================] - 1s 53ms/step - loss: 7.8796 - val_loss: 8.4178\n",
      "Epoch 70/1000\n",
      "23/23 [==============================] - 1s 62ms/step - loss: 7.9958 - val_loss: 8.4205\n",
      "Epoch 71/1000\n",
      "23/23 [==============================] - 1s 30ms/step - loss: 7.9594 - val_loss: 8.4295\n",
      "Epoch 72/1000\n",
      "23/23 [==============================] - 1s 56ms/step - loss: 7.9539 - val_loss: 8.4778\n",
      "Epoch 73/1000\n",
      "23/23 [==============================] - 1s 59ms/step - loss: 8.0175 - val_loss: 8.4604\n",
      "Epoch 74/1000\n",
      "23/23 [==============================] - 1s 53ms/step - loss: 7.9475 - val_loss: 8.4457\n",
      "Epoch 75/1000\n",
      "23/23 [==============================] - 1s 57ms/step - loss: 7.8863 - val_loss: 8.4434\n",
      "Epoch 76/1000\n",
      "23/23 [==============================] - 1s 61ms/step - loss: 7.9820 - val_loss: 8.5162\n",
      "Epoch 77/1000\n",
      "23/23 [==============================] - 1s 52ms/step - loss: 7.9035 - val_loss: 8.5114\n",
      "Epoch 78/1000\n",
      "23/23 [==============================] - 1s 55ms/step - loss: 7.9169 - val_loss: 8.4529\n",
      "Epoch 79/1000\n",
      "23/23 [==============================] - 1s 41ms/step - loss: 7.9107 - val_loss: 8.4249\n",
      "Epoch 80/1000\n",
      "23/23 [==============================] - 1s 56ms/step - loss: 8.0238 - val_loss: 8.4609\n",
      "Epoch 81/1000\n",
      "23/23 [==============================] - 1s 50ms/step - loss: 7.9525 - val_loss: 8.4844\n",
      "Epoch 82/1000\n",
      "23/23 [==============================] - 1s 46ms/step - loss: 7.9245 - val_loss: 8.4469\n",
      "Epoch 83/1000\n",
      "23/23 [==============================] - 1s 59ms/step - loss: 7.9470 - val_loss: 8.4983\n",
      "Epoch 84/1000\n",
      "23/23 [==============================] - 1s 61ms/step - loss: 7.9219 - val_loss: 8.5285\n",
      "Epoch 85/1000\n",
      "23/23 [==============================] - 1s 29ms/step - loss: 7.9358 - val_loss: 8.4417\n",
      "Epoch 86/1000\n",
      "23/23 [==============================] - 1s 49ms/step - loss: 7.9202 - val_loss: 8.4280\n",
      "Epoch 87/1000\n",
      "23/23 [==============================] - 1s 60ms/step - loss: 7.9329 - val_loss: 8.4833\n",
      "Epoch 88/1000\n",
      "23/23 [==============================] - 1s 52ms/step - loss: 7.9280 - val_loss: 8.4680\n",
      "Epoch 89/1000\n",
      "23/23 [==============================] - 1s 56ms/step - loss: 7.9793 - val_loss: 8.4474\n",
      "Epoch 90/1000\n",
      "23/23 [==============================] - 1s 53ms/step - loss: 7.9150 - val_loss: 8.4533\n",
      "Epoch 91/1000\n",
      "23/23 [==============================] - 1s 61ms/step - loss: 7.9044 - val_loss: 8.4258\n",
      "Epoch 92/1000\n",
      "23/23 [==============================] - 1s 31ms/step - loss: 7.8970 - val_loss: 8.4348\n",
      "Epoch 93/1000\n",
      "23/23 [==============================] - 1s 56ms/step - loss: 7.8946 - val_loss: 8.4318\n",
      "Epoch 94/1000\n",
      "23/23 [==============================] - 1s 57ms/step - loss: 7.9326 - val_loss: 8.4706\n",
      "Epoch 95/1000\n",
      "23/23 [==============================] - 1s 57ms/step - loss: 7.9803 - val_loss: 8.4313\n",
      "Epoch 96/1000\n",
      "23/23 [==============================] - 1s 57ms/step - loss: 7.9185 - val_loss: 8.4195\n",
      "Epoch 97/1000\n",
      "23/23 [==============================] - 1s 52ms/step - loss: 7.9016 - val_loss: 8.4186\n",
      "Epoch 98/1000\n",
      "23/23 [==============================] - 1s 31ms/step - loss: 7.9188 - val_loss: 8.3992\n",
      "Epoch 99/1000\n",
      "23/23 [==============================] - 1s 53ms/step - loss: 7.9537 - val_loss: 8.6539\n",
      "Epoch 100/1000\n",
      "23/23 [==============================] - 1s 45ms/step - loss: 7.9743 - val_loss: 8.4352\n",
      "Epoch 101/1000\n",
      "23/23 [==============================] - 1s 54ms/step - loss: 7.8602 - val_loss: 8.4282\n",
      "Epoch 102/1000\n",
      "23/23 [==============================] - 1s 60ms/step - loss: 7.8922 - val_loss: 8.4429\n",
      "Epoch 103/1000\n",
      "23/23 [==============================] - 1s 59ms/step - loss: 7.8680 - val_loss: 8.4620\n",
      "Epoch 104/1000\n",
      "23/23 [==============================] - 1s 54ms/step - loss: 7.8986 - val_loss: 8.4021\n",
      "Epoch 105/1000\n",
      "23/23 [==============================] - 1s 58ms/step - loss: 7.9172 - val_loss: 8.4338\n",
      "Epoch 106/1000\n",
      "23/23 [==============================] - 1s 57ms/step - loss: 7.8577 - val_loss: 8.4381\n",
      "Epoch 107/1000\n",
      "23/23 [==============================] - 1s 64ms/step - loss: 8.0508 - val_loss: 8.4090\n",
      "Epoch 108/1000\n",
      "23/23 [==============================] - 1s 54ms/step - loss: 7.9087 - val_loss: 8.3930\n",
      "Epoch 109/1000\n",
      "23/23 [==============================] - 1s 58ms/step - loss: 7.9730 - val_loss: 8.4618\n",
      "Epoch 110/1000\n",
      "23/23 [==============================] - 1s 53ms/step - loss: 7.9491 - val_loss: 8.4803\n",
      "Epoch 111/1000\n",
      "23/23 [==============================] - 1s 62ms/step - loss: 7.9559 - val_loss: 8.4286\n",
      "Epoch 112/1000\n",
      "23/23 [==============================] - 1s 52ms/step - loss: 7.9787 - val_loss: 8.4371\n",
      "Epoch 113/1000\n",
      "23/23 [==============================] - 1s 52ms/step - loss: 7.9409 - val_loss: 8.5469\n",
      "Epoch 114/1000\n",
      "23/23 [==============================] - 1s 57ms/step - loss: 7.9431 - val_loss: 8.4234\n",
      "Epoch 115/1000\n",
      "23/23 [==============================] - 1s 54ms/step - loss: 8.0370 - val_loss: 8.5204\n",
      "Epoch 116/1000\n",
      "23/23 [==============================] - 1s 62ms/step - loss: 7.9987 - val_loss: 8.5213\n",
      "Epoch 117/1000\n",
      "23/23 [==============================] - 1s 58ms/step - loss: 7.9320 - val_loss: 8.4191\n",
      "Epoch 118/1000\n",
      "23/23 [==============================] - 1s 59ms/step - loss: 7.9064 - val_loss: 8.3919\n",
      "Epoch 119/1000\n",
      "23/23 [==============================] - 1s 61ms/step - loss: 7.9096 - val_loss: 8.4297\n",
      "Epoch 120/1000\n",
      "23/23 [==============================] - 1s 59ms/step - loss: 7.9230 - val_loss: 8.4398\n",
      "Epoch 121/1000\n",
      "23/23 [==============================] - 1s 57ms/step - loss: 7.8970 - val_loss: 8.4296\n",
      "Epoch 122/1000\n",
      "23/23 [==============================] - 1s 56ms/step - loss: 7.9805 - val_loss: 8.4177\n",
      "Epoch 123/1000\n",
      "23/23 [==============================] - 1s 59ms/step - loss: 7.9249 - val_loss: 8.4289\n",
      "Epoch 124/1000\n",
      "23/23 [==============================] - 1s 51ms/step - loss: 7.8992 - val_loss: 8.4555\n",
      "Epoch 125/1000\n",
      "23/23 [==============================] - 1s 55ms/step - loss: 7.9405 - val_loss: 8.5000\n",
      "Epoch 126/1000\n",
      "23/23 [==============================] - 1s 59ms/step - loss: 7.9833 - val_loss: 8.4166\n",
      "Epoch 127/1000\n",
      "23/23 [==============================] - 1s 55ms/step - loss: 7.9494 - val_loss: 8.4236\n",
      "Epoch 128/1000\n",
      "23/23 [==============================] - 1s 55ms/step - loss: 7.9487 - val_loss: 8.4375\n",
      "Epoch 129/1000\n",
      "23/23 [==============================] - 1s 58ms/step - loss: 7.8551 - val_loss: 8.4287\n",
      "Epoch 130/1000\n",
      "23/23 [==============================] - 1s 62ms/step - loss: 7.8993 - val_loss: 8.5034\n",
      "Epoch 131/1000\n",
      "23/23 [==============================] - 1s 58ms/step - loss: 8.0210 - val_loss: 8.4192\n",
      "Epoch 132/1000\n",
      "23/23 [==============================] - 1s 56ms/step - loss: 7.9097 - val_loss: 8.4526\n",
      "Epoch 133/1000\n",
      "23/23 [==============================] - 1s 61ms/step - loss: 7.9172 - val_loss: 8.4468\n",
      "Epoch 134/1000\n",
      "23/23 [==============================] - 1s 63ms/step - loss: 7.9071 - val_loss: 8.4227\n",
      "Epoch 135/1000\n",
      "23/23 [==============================] - 1s 58ms/step - loss: 7.9346 - val_loss: 8.4585\n",
      "Epoch 136/1000\n",
      "23/23 [==============================] - 1s 58ms/step - loss: 7.8998 - val_loss: 8.4047\n",
      "Epoch 137/1000\n",
      "23/23 [==============================] - 1s 54ms/step - loss: 7.8853 - val_loss: 8.3810\n",
      "Epoch 138/1000\n",
      "23/23 [==============================] - 1s 57ms/step - loss: 7.9385 - val_loss: 8.5332\n",
      "Epoch 139/1000\n",
      "23/23 [==============================] - 1s 52ms/step - loss: 7.8708 - val_loss: 8.4014\n",
      "Epoch 140/1000\n",
      "23/23 [==============================] - 1s 55ms/step - loss: 7.9227 - val_loss: 8.4391\n",
      "Epoch 141/1000\n",
      "23/23 [==============================] - 1s 57ms/step - loss: 7.9133 - val_loss: 8.4099\n",
      "Epoch 142/1000\n",
      "23/23 [==============================] - 1s 52ms/step - loss: 7.9276 - val_loss: 8.4121\n",
      "Epoch 143/1000\n",
      "23/23 [==============================] - 1s 57ms/step - loss: 7.9034 - val_loss: 8.4219\n",
      "Epoch 144/1000\n",
      "23/23 [==============================] - 1s 51ms/step - loss: 7.8912 - val_loss: 8.3975\n",
      "Epoch 145/1000\n",
      "23/23 [==============================] - 1s 51ms/step - loss: 7.9126 - val_loss: 8.4977\n",
      "Epoch 146/1000\n",
      "23/23 [==============================] - 1s 51ms/step - loss: 7.9197 - val_loss: 8.4124\n",
      "Epoch 147/1000\n",
      "23/23 [==============================] - 1s 55ms/step - loss: 7.8727 - val_loss: 8.4180\n",
      "Epoch 148/1000\n",
      "23/23 [==============================] - 1s 60ms/step - loss: 7.9092 - val_loss: 8.4439\n",
      "Epoch 149/1000\n",
      "23/23 [==============================] - 1s 52ms/step - loss: 7.9458 - val_loss: 8.4082\n",
      "Epoch 150/1000\n",
      "23/23 [==============================] - 1s 57ms/step - loss: 7.8887 - val_loss: 8.4053\n",
      "Epoch 151/1000\n",
      "23/23 [==============================] - 1s 55ms/step - loss: 7.9288 - val_loss: 8.5029\n",
      "Epoch 152/1000\n",
      "23/23 [==============================] - 1s 56ms/step - loss: 7.9714 - val_loss: 8.4628\n",
      "Epoch 153/1000\n",
      "23/23 [==============================] - 1s 55ms/step - loss: 7.9066 - val_loss: 8.3891\n",
      "Epoch 154/1000\n",
      "23/23 [==============================] - 1s 62ms/step - loss: 8.1224 - val_loss: 8.4842\n",
      "Epoch 155/1000\n",
      "23/23 [==============================] - 1s 57ms/step - loss: 7.9728 - val_loss: 8.4168\n",
      "Epoch 156/1000\n",
      "23/23 [==============================] - 1s 61ms/step - loss: 7.9651 - val_loss: 8.4507\n",
      "Epoch 157/1000\n",
      "23/23 [==============================] - 1s 57ms/step - loss: 7.9140 - val_loss: 8.4796\n",
      "Epoch 158/1000\n",
      "23/23 [==============================] - 1s 31ms/step - loss: 7.9769 - val_loss: 8.4341\n",
      "Epoch 159/1000\n",
      "23/23 [==============================] - 1s 57ms/step - loss: 7.9684 - val_loss: 8.4857\n",
      "Epoch 160/1000\n",
      "23/23 [==============================] - 1s 59ms/step - loss: 7.9257 - val_loss: 8.4438\n",
      "Epoch 161/1000\n",
      "23/23 [==============================] - 1s 62ms/step - loss: 7.9713 - val_loss: 8.4012\n",
      "Epoch 162/1000\n",
      "23/23 [==============================] - 1s 50ms/step - loss: 7.9548 - val_loss: 8.3860\n",
      "Epoch 163/1000\n",
      "23/23 [==============================] - 1s 54ms/step - loss: 7.9040 - val_loss: 8.3973\n",
      "Epoch 164/1000\n",
      "23/23 [==============================] - 1s 59ms/step - loss: 7.8802 - val_loss: 8.3978\n",
      "Epoch 165/1000\n",
      "23/23 [==============================] - 1s 56ms/step - loss: 7.9398 - val_loss: 8.4013\n",
      "Epoch 166/1000\n",
      "23/23 [==============================] - 1s 30ms/step - loss: 7.8990 - val_loss: 8.4042\n",
      "Epoch 167/1000\n",
      "23/23 [==============================] - 1s 50ms/step - loss: 7.8372 - val_loss: 8.4221\n",
      "Epoch 168/1000\n",
      "23/23 [==============================] - 1s 31ms/step - loss: 7.9104 - val_loss: 8.3743\n",
      "Epoch 169/1000\n",
      "23/23 [==============================] - 1s 62ms/step - loss: 7.9292 - val_loss: 8.4638\n",
      "Epoch 170/1000\n",
      "23/23 [==============================] - 1s 53ms/step - loss: 7.9294 - val_loss: 8.4787\n",
      "Epoch 171/1000\n",
      "23/23 [==============================] - 1s 59ms/step - loss: 7.8894 - val_loss: 8.3853\n",
      "Epoch 172/1000\n",
      "23/23 [==============================] - 1s 55ms/step - loss: 7.9743 - val_loss: 8.4004\n",
      "Epoch 173/1000\n",
      "23/23 [==============================] - 1s 61ms/step - loss: 7.8766 - val_loss: 8.4027\n",
      "Epoch 174/1000\n",
      "23/23 [==============================] - 1s 62ms/step - loss: 8.0607 - val_loss: 8.4207\n",
      "Epoch 175/1000\n",
      "23/23 [==============================] - 1s 56ms/step - loss: 7.8703 - val_loss: 8.4042\n",
      "Epoch 176/1000\n",
      "23/23 [==============================] - 1s 56ms/step - loss: 7.8824 - val_loss: 8.3918\n",
      "Epoch 177/1000\n",
      "23/23 [==============================] - 1s 58ms/step - loss: 7.8442 - val_loss: 8.4049\n",
      "Epoch 178/1000\n",
      "23/23 [==============================] - 1s 62ms/step - loss: 7.8869 - val_loss: 8.3948\n",
      "Epoch 179/1000\n",
      "23/23 [==============================] - 1s 55ms/step - loss: 7.9195 - val_loss: 8.4068\n",
      "Epoch 180/1000\n",
      "23/23 [==============================] - 1s 30ms/step - loss: 7.9465 - val_loss: 8.4472\n",
      "Epoch 181/1000\n",
      "23/23 [==============================] - 1s 54ms/step - loss: 7.9142 - val_loss: 8.3962\n",
      "Epoch 182/1000\n",
      "23/23 [==============================] - 1s 53ms/step - loss: 7.8839 - val_loss: 8.3966\n",
      "Epoch 183/1000\n",
      "23/23 [==============================] - 1s 29ms/step - loss: 7.8933 - val_loss: 8.4677\n",
      "Epoch 184/1000\n",
      "23/23 [==============================] - 1s 53ms/step - loss: 7.9662 - val_loss: 8.4228\n",
      "Epoch 185/1000\n",
      "23/23 [==============================] - 1s 61ms/step - loss: 7.9349 - val_loss: 8.4540\n",
      "Epoch 186/1000\n",
      "23/23 [==============================] - 1s 57ms/step - loss: 7.9435 - val_loss: 8.4216\n",
      "Epoch 187/1000\n",
      "23/23 [==============================] - 1s 57ms/step - loss: 7.8784 - val_loss: 8.4835\n",
      "Epoch 188/1000\n",
      "23/23 [==============================] - 1s 55ms/step - loss: 7.9475 - val_loss: 8.3946\n",
      "Epoch 189/1000\n",
      "23/23 [==============================] - 1s 49ms/step - loss: 7.9619 - val_loss: 8.4281\n",
      "Epoch 190/1000\n",
      "23/23 [==============================] - 1s 59ms/step - loss: 7.8845 - val_loss: 8.4016\n",
      "Epoch 191/1000\n",
      "23/23 [==============================] - 1s 53ms/step - loss: 7.8846 - val_loss: 8.4410\n",
      "Epoch 192/1000\n",
      "23/23 [==============================] - 1s 61ms/step - loss: 7.9253 - val_loss: 8.3884\n",
      "Epoch 193/1000\n",
      "23/23 [==============================] - 1s 57ms/step - loss: 7.9320 - val_loss: 8.4081\n",
      "Epoch 194/1000\n",
      "23/23 [==============================] - 1s 51ms/step - loss: 7.9300 - val_loss: 8.4161\n",
      "Epoch 195/1000\n",
      "23/23 [==============================] - 1s 54ms/step - loss: 7.9002 - val_loss: 8.4247\n",
      "Epoch 196/1000\n",
      "23/23 [==============================] - 1s 57ms/step - loss: 7.9565 - val_loss: 8.4175\n",
      "Epoch 197/1000\n",
      "23/23 [==============================] - 1s 54ms/step - loss: 7.8821 - val_loss: 8.3526\n",
      "Epoch 198/1000\n",
      "23/23 [==============================] - 1s 57ms/step - loss: 7.9615 - val_loss: 8.5561\n",
      "Epoch 199/1000\n",
      "23/23 [==============================] - 1s 58ms/step - loss: 7.8599 - val_loss: 8.4011\n",
      "Epoch 200/1000\n",
      "23/23 [==============================] - 1s 53ms/step - loss: 7.9113 - val_loss: 8.3906\n",
      "Epoch 201/1000\n",
      "23/23 [==============================] - 1s 51ms/step - loss: 7.8977 - val_loss: 8.4029\n",
      "Epoch 202/1000\n",
      "23/23 [==============================] - 1s 59ms/step - loss: 7.9389 - val_loss: 8.4126\n",
      "Epoch 203/1000\n",
      "23/23 [==============================] - 1s 55ms/step - loss: 7.8847 - val_loss: 8.4800\n",
      "Epoch 204/1000\n",
      "23/23 [==============================] - 1s 56ms/step - loss: 7.9332 - val_loss: 8.3777\n",
      "Epoch 205/1000\n",
      "23/23 [==============================] - 1s 59ms/step - loss: 7.9064 - val_loss: 8.3931\n",
      "Epoch 206/1000\n",
      "23/23 [==============================] - 1s 59ms/step - loss: 7.8616 - val_loss: 8.4178\n",
      "Epoch 207/1000\n",
      "23/23 [==============================] - 1s 54ms/step - loss: 7.9658 - val_loss: 8.4111\n",
      "Epoch 208/1000\n",
      "23/23 [==============================] - 1s 55ms/step - loss: 7.8892 - val_loss: 8.4316\n",
      "Epoch 209/1000\n",
      "23/23 [==============================] - 1s 55ms/step - loss: 7.9142 - val_loss: 8.4340\n",
      "Epoch 210/1000\n",
      "23/23 [==============================] - 1s 57ms/step - loss: 7.9665 - val_loss: 8.4098\n",
      "Epoch 211/1000\n",
      "23/23 [==============================] - 1s 59ms/step - loss: 7.8546 - val_loss: 8.4426\n",
      "Epoch 212/1000\n",
      "23/23 [==============================] - 1s 61ms/step - loss: 7.8451 - val_loss: 8.4262\n",
      "Epoch 213/1000\n",
      "23/23 [==============================] - 1s 63ms/step - loss: 7.8385 - val_loss: 8.4407\n",
      "Epoch 214/1000\n",
      "23/23 [==============================] - 1s 58ms/step - loss: 7.8924 - val_loss: 8.3785\n",
      "Epoch 215/1000\n",
      "23/23 [==============================] - 1s 62ms/step - loss: 7.8932 - val_loss: 8.3950\n",
      "Epoch 216/1000\n",
      "23/23 [==============================] - 1s 59ms/step - loss: 7.8535 - val_loss: 8.4002\n",
      "Epoch 217/1000\n",
      "23/23 [==============================] - 1s 62ms/step - loss: 7.8316 - val_loss: 8.4093\n",
      "Epoch 218/1000\n",
      "23/23 [==============================] - 1s 54ms/step - loss: 7.9197 - val_loss: 8.3900\n",
      "Epoch 219/1000\n",
      "23/23 [==============================] - 1s 60ms/step - loss: 7.9016 - val_loss: 8.4090\n",
      "Epoch 220/1000\n",
      "23/23 [==============================] - 1s 51ms/step - loss: 7.9109 - val_loss: 8.3969\n",
      "Epoch 221/1000\n",
      "23/23 [==============================] - 1s 52ms/step - loss: 7.8951 - val_loss: 8.4278\n",
      "Epoch 222/1000\n",
      "23/23 [==============================] - 1s 57ms/step - loss: 7.8571 - val_loss: 8.4013\n",
      "Epoch 223/1000\n",
      "23/23 [==============================] - 1s 37ms/step - loss: 7.8762 - val_loss: 8.4372\n",
      "Epoch 224/1000\n",
      "23/23 [==============================] - 1s 60ms/step - loss: 7.9715 - val_loss: 8.3848\n",
      "Epoch 225/1000\n",
      "23/23 [==============================] - 1s 57ms/step - loss: 7.8962 - val_loss: 8.3861\n",
      "Epoch 226/1000\n",
      "23/23 [==============================] - 1s 55ms/step - loss: 7.9427 - val_loss: 8.4345\n",
      "Epoch 227/1000\n",
      "23/23 [==============================] - 1s 59ms/step - loss: 7.9459 - val_loss: 8.3762\n",
      "Epoch 228/1000\n",
      "23/23 [==============================] - 1s 56ms/step - loss: 7.9738 - val_loss: 8.4216\n",
      "Epoch 229/1000\n",
      "23/23 [==============================] - 1s 52ms/step - loss: 7.9285 - val_loss: 8.3753\n",
      "Epoch 230/1000\n",
      "23/23 [==============================] - 1s 58ms/step - loss: 7.9617 - val_loss: 8.3824\n",
      "Epoch 231/1000\n",
      "23/23 [==============================] - 1s 43ms/step - loss: 7.9074 - val_loss: 8.4035\n",
      "Epoch 232/1000\n",
      "23/23 [==============================] - 1s 50ms/step - loss: 7.8989 - val_loss: 8.3865\n",
      "Epoch 233/1000\n",
      "23/23 [==============================] - 1s 58ms/step - loss: 7.8741 - val_loss: 8.3688\n",
      "Epoch 234/1000\n",
      "23/23 [==============================] - 1s 57ms/step - loss: 7.7986 - val_loss: 8.4128\n",
      "Epoch 235/1000\n",
      "23/23 [==============================] - 1s 60ms/step - loss: 7.9271 - val_loss: 8.5366\n",
      "Epoch 236/1000\n",
      "23/23 [==============================] - 1s 57ms/step - loss: 7.9544 - val_loss: 8.4210\n",
      "Epoch 237/1000\n",
      "23/23 [==============================] - 1s 54ms/step - loss: 7.8964 - val_loss: 8.4626\n",
      "Epoch 238/1000\n",
      "23/23 [==============================] - 1s 60ms/step - loss: 7.9156 - val_loss: 8.3782\n",
      "Epoch 239/1000\n",
      "23/23 [==============================] - 1s 52ms/step - loss: 7.8469 - val_loss: 8.3759\n",
      "Epoch 240/1000\n",
      "23/23 [==============================] - 1s 59ms/step - loss: 7.8881 - val_loss: 8.4971\n",
      "Epoch 241/1000\n",
      "23/23 [==============================] - 1s 54ms/step - loss: 7.9272 - val_loss: 8.4413\n",
      "Epoch 242/1000\n",
      "23/23 [==============================] - 1s 59ms/step - loss: 7.8783 - val_loss: 8.4006\n",
      "Epoch 243/1000\n",
      "23/23 [==============================] - 1s 64ms/step - loss: 7.8604 - val_loss: 8.4341\n",
      "Epoch 244/1000\n",
      "23/23 [==============================] - 1s 64ms/step - loss: 7.8978 - val_loss: 8.3457\n",
      "Epoch 245/1000\n",
      "23/23 [==============================] - 1s 58ms/step - loss: 7.8499 - val_loss: 8.4182\n",
      "Epoch 246/1000\n",
      "23/23 [==============================] - 1s 30ms/step - loss: 7.9876 - val_loss: 8.3858\n",
      "Epoch 247/1000\n",
      "23/23 [==============================] - 1s 57ms/step - loss: 7.8509 - val_loss: 8.4038\n",
      "Epoch 248/1000\n",
      "23/23 [==============================] - 1s 56ms/step - loss: 7.8632 - val_loss: 8.4136\n",
      "Epoch 249/1000\n",
      "23/23 [==============================] - 1s 58ms/step - loss: 7.9613 - val_loss: 8.3519\n",
      "Epoch 250/1000\n",
      "23/23 [==============================] - 1s 62ms/step - loss: 7.9231 - val_loss: 8.3963\n",
      "Epoch 251/1000\n",
      "23/23 [==============================] - 1s 58ms/step - loss: 7.8729 - val_loss: 8.3734\n",
      "Epoch 252/1000\n",
      "23/23 [==============================] - 1s 59ms/step - loss: 7.9049 - val_loss: 8.4049\n",
      "Epoch 253/1000\n",
      "23/23 [==============================] - 1s 60ms/step - loss: 7.9248 - val_loss: 8.3736\n",
      "Epoch 254/1000\n",
      "23/23 [==============================] - 1s 47ms/step - loss: 7.8609 - val_loss: 8.4263\n",
      "Epoch 255/1000\n",
      "23/23 [==============================] - 1s 59ms/step - loss: 7.8629 - val_loss: 8.4321\n",
      "Epoch 256/1000\n",
      "23/23 [==============================] - 1s 57ms/step - loss: 7.8437 - val_loss: 8.5505\n",
      "Epoch 257/1000\n",
      "23/23 [==============================] - 1s 60ms/step - loss: 7.8960 - val_loss: 8.4223\n",
      "Epoch 258/1000\n",
      "23/23 [==============================] - 1s 62ms/step - loss: 7.9501 - val_loss: 8.4798\n",
      "Epoch 259/1000\n",
      "23/23 [==============================] - 1s 56ms/step - loss: 7.8866 - val_loss: 8.3654\n",
      "Epoch 260/1000\n",
      "23/23 [==============================] - 1s 62ms/step - loss: 7.8676 - val_loss: 8.3714\n",
      "Epoch 261/1000\n",
      "23/23 [==============================] - 1s 50ms/step - loss: 7.8853 - val_loss: 8.3646\n",
      "Epoch 262/1000\n",
      "23/23 [==============================] - 1s 56ms/step - loss: 7.8268 - val_loss: 8.4346\n",
      "Epoch 263/1000\n",
      "23/23 [==============================] - 1s 62ms/step - loss: 7.9421 - val_loss: 8.3673\n",
      "Epoch 264/1000\n",
      "23/23 [==============================] - 1s 59ms/step - loss: 7.8238 - val_loss: 8.3782\n",
      "Epoch 265/1000\n",
      "23/23 [==============================] - 1s 54ms/step - loss: 7.9672 - val_loss: 8.3699\n",
      "Epoch 266/1000\n",
      "23/23 [==============================] - 1s 58ms/step - loss: 7.8770 - val_loss: 8.3566\n",
      "Epoch 267/1000\n",
      "23/23 [==============================] - 1s 58ms/step - loss: 7.8909 - val_loss: 8.4572\n",
      "Epoch 268/1000\n",
      "23/23 [==============================] - 1s 52ms/step - loss: 7.9109 - val_loss: 8.3946\n",
      "Epoch 269/1000\n",
      "23/23 [==============================] - 1s 55ms/step - loss: 7.9308 - val_loss: 8.3805\n",
      "Epoch 270/1000\n",
      "23/23 [==============================] - 1s 62ms/step - loss: 7.9367 - val_loss: 8.4677\n",
      "Epoch 271/1000\n",
      "23/23 [==============================] - 1s 63ms/step - loss: 7.8744 - val_loss: 8.3781\n",
      "Epoch 272/1000\n",
      "23/23 [==============================] - 1s 54ms/step - loss: 7.8987 - val_loss: 8.3801\n",
      "Epoch 273/1000\n",
      "23/23 [==============================] - 1s 58ms/step - loss: 7.8536 - val_loss: 8.4133\n",
      "Epoch 274/1000\n",
      "23/23 [==============================] - 1s 58ms/step - loss: 7.8289 - val_loss: 8.3472\n",
      "Epoch 275/1000\n",
      "23/23 [==============================] - 1s 57ms/step - loss: 7.8524 - val_loss: 8.3713\n",
      "Epoch 276/1000\n",
      "23/23 [==============================] - 1s 58ms/step - loss: 7.8539 - val_loss: 8.3662\n",
      "Epoch 277/1000\n",
      "23/23 [==============================] - 1s 57ms/step - loss: 7.9464 - val_loss: 8.3672\n",
      "Epoch 278/1000\n",
      "23/23 [==============================] - 1s 55ms/step - loss: 7.9314 - val_loss: 8.3916\n",
      "Epoch 279/1000\n",
      "23/23 [==============================] - 1s 56ms/step - loss: 7.9641 - val_loss: 8.4017\n",
      "Epoch 280/1000\n",
      "23/23 [==============================] - 1s 59ms/step - loss: 7.8667 - val_loss: 8.3799\n",
      "Epoch 281/1000\n",
      "23/23 [==============================] - 1s 49ms/step - loss: 7.8468 - val_loss: 8.3668\n",
      "Epoch 282/1000\n",
      "23/23 [==============================] - 1s 54ms/step - loss: 7.8899 - val_loss: 8.3863\n",
      "Epoch 283/1000\n",
      "23/23 [==============================] - 1s 60ms/step - loss: 7.8333 - val_loss: 8.4153\n",
      "Epoch 284/1000\n",
      "23/23 [==============================] - 1s 59ms/step - loss: 7.8917 - val_loss: 8.4226\n",
      "Epoch 285/1000\n",
      "23/23 [==============================] - 1s 63ms/step - loss: 7.9822 - val_loss: 8.3571\n",
      "Epoch 286/1000\n",
      "23/23 [==============================] - 1s 56ms/step - loss: 7.8775 - val_loss: 8.3591\n",
      "Epoch 287/1000\n",
      "23/23 [==============================] - 1s 55ms/step - loss: 7.9409 - val_loss: 8.4127\n",
      "Epoch 288/1000\n",
      "23/23 [==============================] - 1s 57ms/step - loss: 7.8980 - val_loss: 8.3675\n",
      "Epoch 289/1000\n",
      "23/23 [==============================] - 1s 60ms/step - loss: 7.8470 - val_loss: 8.3650\n",
      "Epoch 290/1000\n",
      "23/23 [==============================] - 1s 51ms/step - loss: 7.8865 - val_loss: 8.3490\n",
      "Epoch 291/1000\n",
      "23/23 [==============================] - 1s 59ms/step - loss: 7.8851 - val_loss: 8.4367\n",
      "Epoch 292/1000\n",
      "23/23 [==============================] - 1s 57ms/step - loss: 7.9001 - val_loss: 8.4056\n",
      "Epoch 293/1000\n",
      "23/23 [==============================] - 1s 53ms/step - loss: 7.9925 - val_loss: 8.6599\n",
      "Epoch 294/1000\n",
      "23/23 [==============================] - 1s 30ms/step - loss: 7.9233 - val_loss: 8.5354\n",
      "Epoch 295/1000\n",
      "23/23 [==============================] - 1s 58ms/step - loss: 7.9789 - val_loss: 8.3800\n",
      "Epoch 296/1000\n",
      "23/23 [==============================] - 1s 53ms/step - loss: 7.8776 - val_loss: 8.3707\n",
      "Epoch 297/1000\n",
      "23/23 [==============================] - 1s 56ms/step - loss: 7.8367 - val_loss: 8.3785\n",
      "Epoch 298/1000\n",
      "23/23 [==============================] - 1s 58ms/step - loss: 7.8365 - val_loss: 8.3961\n",
      "Epoch 299/1000\n",
      "23/23 [==============================] - 1s 58ms/step - loss: 7.9164 - val_loss: 8.3795\n",
      "Epoch 300/1000\n",
      "23/23 [==============================] - 1s 56ms/step - loss: 7.9251 - val_loss: 8.3839\n",
      "Epoch 301/1000\n",
      "23/23 [==============================] - 1s 57ms/step - loss: 7.8232 - val_loss: 8.3590\n",
      "Epoch 302/1000\n",
      "23/23 [==============================] - 1s 56ms/step - loss: 7.8599 - val_loss: 8.3710\n",
      "Epoch 303/1000\n",
      "23/23 [==============================] - 1s 58ms/step - loss: 7.8331 - val_loss: 8.3546\n",
      "Epoch 304/1000\n",
      "23/23 [==============================] - 1s 53ms/step - loss: 7.8569 - val_loss: 8.4519\n",
      "Epoch 305/1000\n",
      "23/23 [==============================] - 1s 63ms/step - loss: 7.8925 - val_loss: 8.3975\n",
      "Epoch 306/1000\n",
      "23/23 [==============================] - 1s 59ms/step - loss: 7.9211 - val_loss: 8.5780\n",
      "Epoch 307/1000\n",
      "23/23 [==============================] - 1s 64ms/step - loss: 7.8794 - val_loss: 8.4449\n",
      "Epoch 308/1000\n",
      "23/23 [==============================] - 1s 60ms/step - loss: 7.9556 - val_loss: 8.3952\n",
      "Epoch 309/1000\n",
      "23/23 [==============================] - 1s 56ms/step - loss: 7.9596 - val_loss: 8.4449\n",
      "Epoch 310/1000\n",
      "23/23 [==============================] - 1s 46ms/step - loss: 7.8622 - val_loss: 8.3795\n",
      "Epoch 311/1000\n",
      "23/23 [==============================] - 1s 45ms/step - loss: 7.9391 - val_loss: 8.4313\n",
      "Epoch 312/1000\n",
      "23/23 [==============================] - 1s 58ms/step - loss: 7.8705 - val_loss: 8.3964\n",
      "Epoch 313/1000\n",
      "23/23 [==============================] - 1s 57ms/step - loss: 7.8789 - val_loss: 8.3835\n",
      "Epoch 314/1000\n",
      "23/23 [==============================] - 1s 39ms/step - loss: 7.8787 - val_loss: 8.3701\n",
      "Epoch 315/1000\n",
      "23/23 [==============================] - 1s 58ms/step - loss: 7.8120 - val_loss: 8.6560\n",
      "Epoch 316/1000\n",
      "23/23 [==============================] - 1s 56ms/step - loss: 8.0582 - val_loss: 8.4970\n",
      "Epoch 317/1000\n",
      "23/23 [==============================] - 1s 54ms/step - loss: 7.8956 - val_loss: 8.3726\n",
      "Epoch 318/1000\n",
      "23/23 [==============================] - 1s 61ms/step - loss: 7.8904 - val_loss: 8.3810\n",
      "Epoch 319/1000\n",
      "23/23 [==============================] - 1s 58ms/step - loss: 7.9268 - val_loss: 8.3745\n",
      "Epoch 320/1000\n",
      "23/23 [==============================] - 1s 52ms/step - loss: 7.9311 - val_loss: 8.4183\n",
      "Epoch 321/1000\n",
      "23/23 [==============================] - 1s 60ms/step - loss: 7.9198 - val_loss: 8.4184\n",
      "Epoch 322/1000\n",
      "23/23 [==============================] - 1s 54ms/step - loss: 7.9376 - val_loss: 8.4820\n",
      "Epoch 323/1000\n",
      "23/23 [==============================] - 1s 54ms/step - loss: 7.8975 - val_loss: 8.4813\n",
      "Epoch 324/1000\n",
      "23/23 [==============================] - 1s 53ms/step - loss: 7.9228 - val_loss: 8.3711\n",
      "Epoch 325/1000\n",
      "23/23 [==============================] - 1s 62ms/step - loss: 7.9136 - val_loss: 8.4465\n",
      "Epoch 326/1000\n",
      "23/23 [==============================] - 1s 54ms/step - loss: 7.8559 - val_loss: 8.3727\n",
      "Epoch 327/1000\n",
      "23/23 [==============================] - 1s 56ms/step - loss: 7.9059 - val_loss: 8.4101\n",
      "Epoch 328/1000\n",
      "23/23 [==============================] - 1s 60ms/step - loss: 7.8572 - val_loss: 8.3399\n",
      "Epoch 329/1000\n",
      "23/23 [==============================] - 1s 56ms/step - loss: 7.9410 - val_loss: 8.4822\n",
      "Epoch 330/1000\n",
      "23/23 [==============================] - 1s 61ms/step - loss: 7.9442 - val_loss: 8.3929\n",
      "Epoch 331/1000\n",
      "23/23 [==============================] - 1s 63ms/step - loss: 7.8874 - val_loss: 8.4000\n",
      "Epoch 332/1000\n",
      "23/23 [==============================] - 1s 49ms/step - loss: 7.8827 - val_loss: 8.3823\n",
      "Epoch 333/1000\n",
      "23/23 [==============================] - 1s 48ms/step - loss: 7.8517 - val_loss: 8.3336\n",
      "Epoch 334/1000\n",
      "23/23 [==============================] - 1s 51ms/step - loss: 7.8950 - val_loss: 8.4084\n",
      "Epoch 335/1000\n",
      "23/23 [==============================] - 1s 56ms/step - loss: 7.8539 - val_loss: 8.3636\n",
      "Epoch 336/1000\n",
      "23/23 [==============================] - 1s 64ms/step - loss: 7.8447 - val_loss: 8.3856\n",
      "Epoch 337/1000\n",
      "23/23 [==============================] - 1s 61ms/step - loss: 7.9264 - val_loss: 8.3749\n",
      "Epoch 338/1000\n",
      "23/23 [==============================] - 1s 50ms/step - loss: 7.9514 - val_loss: 8.4010\n",
      "Epoch 339/1000\n",
      "23/23 [==============================] - 1s 58ms/step - loss: 7.8869 - val_loss: 8.3826\n",
      "Epoch 340/1000\n",
      "23/23 [==============================] - 1s 55ms/step - loss: 7.8887 - val_loss: 8.3714\n",
      "Epoch 341/1000\n",
      "23/23 [==============================] - 1s 55ms/step - loss: 7.8747 - val_loss: 8.4465\n",
      "Epoch 342/1000\n",
      "23/23 [==============================] - 1s 61ms/step - loss: 7.9031 - val_loss: 8.3749\n",
      "Epoch 343/1000\n",
      "23/23 [==============================] - 1s 56ms/step - loss: 7.8763 - val_loss: 8.4311\n",
      "Epoch 344/1000\n",
      "23/23 [==============================] - 1s 55ms/step - loss: 7.8908 - val_loss: 8.3644\n",
      "Epoch 345/1000\n",
      "23/23 [==============================] - 1s 59ms/step - loss: 7.8817 - val_loss: 8.4060\n",
      "Epoch 346/1000\n",
      "23/23 [==============================] - 1s 60ms/step - loss: 7.8815 - val_loss: 8.3590\n",
      "Epoch 347/1000\n",
      "23/23 [==============================] - 1s 54ms/step - loss: 7.9000 - val_loss: 8.3643\n",
      "Epoch 348/1000\n",
      "23/23 [==============================] - 1s 48ms/step - loss: 7.9168 - val_loss: 8.3777\n",
      "Epoch 349/1000\n",
      "23/23 [==============================] - 1s 30ms/step - loss: 7.8798 - val_loss: 8.3990\n",
      "Epoch 350/1000\n",
      "23/23 [==============================] - 1s 61ms/step - loss: 7.8620 - val_loss: 8.3550\n",
      "Epoch 351/1000\n",
      "23/23 [==============================] - 1s 60ms/step - loss: 7.8186 - val_loss: 8.4038\n",
      "Epoch 352/1000\n",
      "23/23 [==============================] - 1s 60ms/step - loss: 7.8796 - val_loss: 8.3756\n",
      "Epoch 353/1000\n",
      "23/23 [==============================] - 1s 52ms/step - loss: 7.8362 - val_loss: 8.3684\n",
      "Epoch 354/1000\n",
      "23/23 [==============================] - 1s 60ms/step - loss: 8.0201 - val_loss: 8.4290\n",
      "Epoch 355/1000\n",
      "23/23 [==============================] - 1s 49ms/step - loss: 7.8343 - val_loss: 8.3672\n",
      "Epoch 356/1000\n",
      "23/23 [==============================] - 1s 30ms/step - loss: 7.8755 - val_loss: 8.3475\n",
      "Epoch 357/1000\n",
      "23/23 [==============================] - 1s 52ms/step - loss: 7.8414 - val_loss: 8.4151\n",
      "Epoch 358/1000\n",
      "23/23 [==============================] - 1s 60ms/step - loss: 7.8529 - val_loss: 8.4319\n",
      "Epoch 359/1000\n",
      "23/23 [==============================] - 1s 57ms/step - loss: 7.9813 - val_loss: 8.3935\n",
      "Epoch 360/1000\n",
      "23/23 [==============================] - 1s 51ms/step - loss: 7.8611 - val_loss: 8.3588\n",
      "Epoch 361/1000\n",
      "23/23 [==============================] - 1s 41ms/step - loss: 7.8680 - val_loss: 8.4521\n",
      "Epoch 362/1000\n",
      "23/23 [==============================] - 1s 54ms/step - loss: 7.8483 - val_loss: 8.4106\n",
      "Epoch 363/1000\n",
      "23/23 [==============================] - 1s 56ms/step - loss: 7.8646 - val_loss: 8.3605\n",
      "Epoch 364/1000\n",
      "23/23 [==============================] - 1s 53ms/step - loss: 7.8745 - val_loss: 8.3743\n",
      "Epoch 365/1000\n",
      "23/23 [==============================] - 1s 58ms/step - loss: 7.8532 - val_loss: 8.3609\n",
      "Epoch 366/1000\n",
      "23/23 [==============================] - 1s 57ms/step - loss: 7.8843 - val_loss: 8.3603\n",
      "Epoch 367/1000\n",
      "23/23 [==============================] - 1s 50ms/step - loss: 7.9171 - val_loss: 8.3914\n",
      "Epoch 368/1000\n",
      "23/23 [==============================] - 1s 55ms/step - loss: 7.9363 - val_loss: 8.5042\n",
      "Epoch 369/1000\n",
      "23/23 [==============================] - 1s 57ms/step - loss: 7.9436 - val_loss: 8.3803\n",
      "Epoch 370/1000\n",
      "23/23 [==============================] - 1s 53ms/step - loss: 7.9511 - val_loss: 8.4800\n",
      "Epoch 371/1000\n",
      "23/23 [==============================] - 1s 57ms/step - loss: 7.9127 - val_loss: 8.4978\n",
      "Epoch 372/1000\n",
      "23/23 [==============================] - 1s 58ms/step - loss: 7.9251 - val_loss: 8.3849\n",
      "Epoch 373/1000\n",
      "23/23 [==============================] - 1s 54ms/step - loss: 7.9070 - val_loss: 8.3572\n",
      "Epoch 374/1000\n",
      "23/23 [==============================] - 1s 54ms/step - loss: 7.8522 - val_loss: 8.4151\n",
      "Epoch 375/1000\n",
      "23/23 [==============================] - 1s 58ms/step - loss: 7.9143 - val_loss: 8.3686\n",
      "Epoch 376/1000\n",
      "23/23 [==============================] - 1s 56ms/step - loss: 7.8774 - val_loss: 8.3618\n",
      "Epoch 377/1000\n",
      "23/23 [==============================] - 1s 54ms/step - loss: 7.8884 - val_loss: 8.4235\n",
      "Epoch 378/1000\n",
      "23/23 [==============================] - 1s 58ms/step - loss: 7.8456 - val_loss: 8.4938\n",
      "Epoch 379/1000\n",
      "23/23 [==============================] - 1s 42ms/step - loss: 7.9251 - val_loss: 8.3950\n",
      "Epoch 380/1000\n",
      "23/23 [==============================] - 1s 62ms/step - loss: 7.8555 - val_loss: 8.3507\n",
      "Epoch 381/1000\n",
      "23/23 [==============================] - 1s 59ms/step - loss: 7.8996 - val_loss: 8.3515\n",
      "Epoch 382/1000\n",
      "23/23 [==============================] - 1s 61ms/step - loss: 7.8586 - val_loss: 8.3676\n",
      "Epoch 383/1000\n",
      "23/23 [==============================] - 1s 57ms/step - loss: 7.8822 - val_loss: 8.3740\n",
      "Epoch 384/1000\n",
      "23/23 [==============================] - 1s 58ms/step - loss: 7.8151 - val_loss: 8.3567\n",
      "Epoch 385/1000\n",
      "23/23 [==============================] - 1s 51ms/step - loss: 7.8332 - val_loss: 8.3574\n",
      "Epoch 386/1000\n",
      "23/23 [==============================] - 1s 54ms/step - loss: 7.8557 - val_loss: 8.3549\n",
      "Epoch 387/1000\n",
      "23/23 [==============================] - 1s 34ms/step - loss: 7.8493 - val_loss: 8.5266\n",
      "Epoch 388/1000\n",
      "23/23 [==============================] - 1s 59ms/step - loss: 7.9010 - val_loss: 8.3792\n",
      "Epoch 389/1000\n",
      "23/23 [==============================] - 1s 51ms/step - loss: 7.8947 - val_loss: 8.3928\n",
      "Epoch 390/1000\n",
      "23/23 [==============================] - 1s 63ms/step - loss: 7.9428 - val_loss: 8.3592\n",
      "Epoch 391/1000\n",
      "23/23 [==============================] - 1s 51ms/step - loss: 7.8506 - val_loss: 8.4855\n",
      "Epoch 392/1000\n",
      "23/23 [==============================] - 1s 58ms/step - loss: 7.9035 - val_loss: 8.4398\n",
      "Epoch 393/1000\n",
      "23/23 [==============================] - 1s 61ms/step - loss: 7.9085 - val_loss: 8.3534\n",
      "Epoch 394/1000\n",
      "23/23 [==============================] - 1s 58ms/step - loss: 7.8668 - val_loss: 8.3688\n",
      "Epoch 395/1000\n",
      "23/23 [==============================] - 1s 54ms/step - loss: 7.9425 - val_loss: 8.4225\n",
      "Epoch 396/1000\n",
      "23/23 [==============================] - 1s 56ms/step - loss: 7.9210 - val_loss: 8.3580\n",
      "Epoch 397/1000\n",
      "23/23 [==============================] - 1s 50ms/step - loss: 7.8766 - val_loss: 8.3747\n",
      "Epoch 398/1000\n",
      "23/23 [==============================] - 1s 55ms/step - loss: 7.8759 - val_loss: 8.3649\n",
      "Epoch 399/1000\n",
      "23/23 [==============================] - 1s 57ms/step - loss: 7.9201 - val_loss: 8.4189\n",
      "Epoch 400/1000\n",
      "23/23 [==============================] - 1s 53ms/step - loss: 7.8935 - val_loss: 8.3603\n",
      "Epoch 401/1000\n",
      "23/23 [==============================] - 1s 48ms/step - loss: 7.8356 - val_loss: 8.3683\n",
      "Epoch 402/1000\n",
      "23/23 [==============================] - 1s 60ms/step - loss: 7.9154 - val_loss: 8.3943\n",
      "Epoch 403/1000\n",
      "23/23 [==============================] - 1s 59ms/step - loss: 7.9076 - val_loss: 8.3750\n",
      "Epoch 404/1000\n",
      "23/23 [==============================] - 1s 62ms/step - loss: 7.8575 - val_loss: 8.3544\n",
      "Epoch 405/1000\n",
      "23/23 [==============================] - 1s 62ms/step - loss: 7.8729 - val_loss: 8.3527\n",
      "Epoch 406/1000\n",
      "23/23 [==============================] - 1s 61ms/step - loss: 7.8824 - val_loss: 8.4073\n",
      "Epoch 407/1000\n",
      "23/23 [==============================] - 1s 61ms/step - loss: 7.8546 - val_loss: 8.3528\n",
      "Epoch 408/1000\n",
      "23/23 [==============================] - 1s 52ms/step - loss: 7.8557 - val_loss: 8.3365\n",
      "Epoch 409/1000\n",
      "23/23 [==============================] - 1s 56ms/step - loss: 7.9414 - val_loss: 8.7505\n",
      "Epoch 410/1000\n",
      "23/23 [==============================] - 1s 55ms/step - loss: 7.8665 - val_loss: 8.3697\n",
      "Epoch 411/1000\n",
      "23/23 [==============================] - 1s 55ms/step - loss: 7.9213 - val_loss: 8.3568\n",
      "Epoch 412/1000\n",
      "23/23 [==============================] - 1s 57ms/step - loss: 8.0132 - val_loss: 8.4274\n",
      "Epoch 413/1000\n",
      "23/23 [==============================] - 1s 51ms/step - loss: 7.8310 - val_loss: 8.3610\n",
      "Epoch 414/1000\n",
      "23/23 [==============================] - 1s 57ms/step - loss: 7.8669 - val_loss: 8.3785\n",
      "Epoch 415/1000\n",
      "23/23 [==============================] - 1s 55ms/step - loss: 7.8515 - val_loss: 8.3482\n",
      "Epoch 416/1000\n",
      "23/23 [==============================] - 1s 55ms/step - loss: 7.8466 - val_loss: 8.3637\n",
      "Epoch 417/1000\n",
      "23/23 [==============================] - 1s 58ms/step - loss: 7.8367 - val_loss: 8.3506\n",
      "Epoch 418/1000\n",
      "23/23 [==============================] - 1s 55ms/step - loss: 7.8615 - val_loss: 8.3745\n",
      "Epoch 419/1000\n",
      "23/23 [==============================] - 1s 52ms/step - loss: 7.8607 - val_loss: 8.3561\n",
      "Epoch 420/1000\n",
      "23/23 [==============================] - 1s 59ms/step - loss: 7.8852 - val_loss: 8.3596\n",
      "Epoch 421/1000\n",
      "23/23 [==============================] - 1s 58ms/step - loss: 7.7982 - val_loss: 8.3458\n",
      "Epoch 422/1000\n",
      "23/23 [==============================] - 1s 54ms/step - loss: 8.0221 - val_loss: 8.4904\n",
      "Epoch 423/1000\n",
      "23/23 [==============================] - 1s 57ms/step - loss: 7.9139 - val_loss: 8.3780\n",
      "Epoch 424/1000\n",
      "23/23 [==============================] - 1s 59ms/step - loss: 7.8437 - val_loss: 8.3625\n",
      "Epoch 425/1000\n",
      "23/23 [==============================] - 1s 50ms/step - loss: 7.8680 - val_loss: 8.3834\n",
      "Epoch 426/1000\n",
      "23/23 [==============================] - 1s 55ms/step - loss: 7.9248 - val_loss: 8.3405\n",
      "Epoch 427/1000\n",
      "23/23 [==============================] - 1s 57ms/step - loss: 7.7994 - val_loss: 8.3520\n",
      "Epoch 428/1000\n",
      "23/23 [==============================] - 1s 31ms/step - loss: 7.8845 - val_loss: 8.3481\n",
      "Epoch 429/1000\n",
      "23/23 [==============================] - 1s 55ms/step - loss: 7.8289 - val_loss: 8.3777\n",
      "Epoch 430/1000\n",
      "23/23 [==============================] - 1s 30ms/step - loss: 7.8626 - val_loss: 8.4734\n",
      "Epoch 431/1000\n",
      "23/23 [==============================] - 1s 53ms/step - loss: 7.9459 - val_loss: 8.3614\n",
      "Epoch 432/1000\n",
      "23/23 [==============================] - 1s 57ms/step - loss: 7.8610 - val_loss: 8.3750\n",
      "Epoch 433/1000\n",
      "23/23 [==============================] - 1s 60ms/step - loss: 7.8955 - val_loss: 8.5257\n",
      "Epoch 434/1000\n",
      "23/23 [==============================] - 1s 43ms/step - loss: 8.0076 - val_loss: 8.3415\n",
      "Epoch 435/1000\n",
      "23/23 [==============================] - 1s 51ms/step - loss: 7.9328 - val_loss: 8.3606\n",
      "Epoch 436/1000\n",
      "23/23 [==============================] - 1s 30ms/step - loss: 7.8338 - val_loss: 8.4126\n",
      "Epoch 437/1000\n",
      "23/23 [==============================] - 1s 53ms/step - loss: 7.8980 - val_loss: 8.3507\n",
      "Epoch 438/1000\n",
      "23/23 [==============================] - 1s 59ms/step - loss: 7.9348 - val_loss: 8.3508\n",
      "Epoch 439/1000\n",
      "23/23 [==============================] - 1s 54ms/step - loss: 7.8685 - val_loss: 8.4376\n",
      "Epoch 440/1000\n",
      "23/23 [==============================] - 1s 30ms/step - loss: 7.8491 - val_loss: 8.4089\n",
      "Epoch 441/1000\n",
      "23/23 [==============================] - 1s 59ms/step - loss: 7.8108 - val_loss: 8.3924\n",
      "Epoch 442/1000\n",
      "23/23 [==============================] - 1s 60ms/step - loss: 7.9726 - val_loss: 8.3445\n",
      "Epoch 443/1000\n",
      "23/23 [==============================] - 1s 60ms/step - loss: 7.8929 - val_loss: 8.3796\n",
      "Epoch 444/1000\n",
      "23/23 [==============================] - 1s 48ms/step - loss: 7.8528 - val_loss: 8.3995\n",
      "Epoch 445/1000\n",
      "23/23 [==============================] - 1s 52ms/step - loss: 7.8887 - val_loss: 8.3457\n",
      "Epoch 446/1000\n",
      "23/23 [==============================] - 1s 62ms/step - loss: 7.8854 - val_loss: 8.3770\n",
      "Epoch 447/1000\n",
      "23/23 [==============================] - 1s 62ms/step - loss: 7.8136 - val_loss: 8.3631\n",
      "Epoch 448/1000\n",
      "23/23 [==============================] - 1s 63ms/step - loss: 7.8827 - val_loss: 8.3815\n",
      "Epoch 449/1000\n",
      "23/23 [==============================] - 1s 58ms/step - loss: 7.8347 - val_loss: 8.3804\n",
      "Epoch 450/1000\n",
      "23/23 [==============================] - 1s 55ms/step - loss: 7.8214 - val_loss: 8.3443\n",
      "Epoch 451/1000\n",
      "23/23 [==============================] - 1s 54ms/step - loss: 7.8304 - val_loss: 8.3794\n",
      "Epoch 452/1000\n",
      "23/23 [==============================] - 1s 50ms/step - loss: 7.9046 - val_loss: 8.3648\n",
      "Epoch 453/1000\n",
      "23/23 [==============================] - 1s 54ms/step - loss: 7.8424 - val_loss: 8.3433\n",
      "Epoch 454/1000\n",
      "23/23 [==============================] - 1s 58ms/step - loss: 7.9712 - val_loss: 8.3350\n",
      "Epoch 455/1000\n",
      "23/23 [==============================] - 1s 55ms/step - loss: 7.8047 - val_loss: 8.3494\n",
      "Epoch 456/1000\n",
      "23/23 [==============================] - 1s 62ms/step - loss: 7.9606 - val_loss: 8.3880\n",
      "Epoch 457/1000\n",
      "23/23 [==============================] - 1s 50ms/step - loss: 7.9157 - val_loss: 8.4019\n",
      "Epoch 458/1000\n",
      "23/23 [==============================] - 1s 58ms/step - loss: 7.8813 - val_loss: 8.3523\n",
      "Epoch 459/1000\n",
      "23/23 [==============================] - 1s 54ms/step - loss: 7.8847 - val_loss: 8.3270\n",
      "Epoch 460/1000\n",
      "23/23 [==============================] - 1s 62ms/step - loss: 7.8616 - val_loss: 8.3649\n",
      "Epoch 461/1000\n",
      "23/23 [==============================] - 1s 60ms/step - loss: 7.7963 - val_loss: 8.3529\n",
      "Epoch 462/1000\n",
      "23/23 [==============================] - 1s 54ms/step - loss: 7.8670 - val_loss: 8.3830\n",
      "Epoch 463/1000\n",
      "23/23 [==============================] - 1s 57ms/step - loss: 7.8939 - val_loss: 8.3734\n",
      "Epoch 464/1000\n",
      "23/23 [==============================] - 1s 55ms/step - loss: 7.8964 - val_loss: 8.3697\n",
      "Epoch 465/1000\n",
      "23/23 [==============================] - 1s 54ms/step - loss: 7.9442 - val_loss: 8.3874\n",
      "Epoch 466/1000\n",
      "23/23 [==============================] - 1s 54ms/step - loss: 7.8447 - val_loss: 8.3392\n",
      "Epoch 467/1000\n",
      "23/23 [==============================] - 1s 57ms/step - loss: 7.8586 - val_loss: 8.3161\n",
      "Epoch 468/1000\n",
      "23/23 [==============================] - 1s 58ms/step - loss: 7.8604 - val_loss: 8.5703\n",
      "Epoch 469/1000\n",
      "23/23 [==============================] - 1s 55ms/step - loss: 7.8791 - val_loss: 8.3355\n",
      "Epoch 470/1000\n",
      "23/23 [==============================] - 1s 60ms/step - loss: 7.8927 - val_loss: 8.3420\n",
      "Epoch 471/1000\n",
      "23/23 [==============================] - 1s 59ms/step - loss: 7.8197 - val_loss: 8.3515\n",
      "Epoch 472/1000\n",
      "23/23 [==============================] - 1s 62ms/step - loss: 7.8882 - val_loss: 8.3969\n",
      "Epoch 473/1000\n",
      "23/23 [==============================] - 1s 61ms/step - loss: 7.9396 - val_loss: 8.3694\n",
      "Epoch 474/1000\n",
      "23/23 [==============================] - 1s 56ms/step - loss: 7.8758 - val_loss: 8.3492\n",
      "Epoch 475/1000\n",
      "23/23 [==============================] - 1s 56ms/step - loss: 7.8729 - val_loss: 8.3706\n",
      "Epoch 476/1000\n",
      "23/23 [==============================] - 1s 56ms/step - loss: 7.8884 - val_loss: 8.3534\n",
      "Epoch 477/1000\n",
      "23/23 [==============================] - 1s 60ms/step - loss: 7.8464 - val_loss: 8.4360\n",
      "Epoch 478/1000\n",
      "23/23 [==============================] - 1s 56ms/step - loss: 7.8693 - val_loss: 8.3643\n",
      "Epoch 479/1000\n",
      "23/23 [==============================] - 1s 59ms/step - loss: 7.8718 - val_loss: 8.4081\n",
      "Epoch 480/1000\n",
      "23/23 [==============================] - 1s 58ms/step - loss: 7.8974 - val_loss: 8.3712\n",
      "Epoch 481/1000\n",
      "23/23 [==============================] - 1s 54ms/step - loss: 7.9057 - val_loss: 8.5474\n",
      "Epoch 482/1000\n",
      "23/23 [==============================] - 1s 59ms/step - loss: 7.8511 - val_loss: 8.3618\n",
      "Epoch 483/1000\n",
      "23/23 [==============================] - 1s 57ms/step - loss: 7.8202 - val_loss: 8.3885\n",
      "Epoch 484/1000\n",
      "23/23 [==============================] - 1s 58ms/step - loss: 7.8974 - val_loss: 8.4686\n",
      "Epoch 485/1000\n",
      "23/23 [==============================] - 1s 54ms/step - loss: 7.9676 - val_loss: 8.4959\n",
      "Epoch 486/1000\n",
      "23/23 [==============================] - 1s 60ms/step - loss: 7.8991 - val_loss: 8.4600\n",
      "Epoch 487/1000\n",
      "23/23 [==============================] - 1s 53ms/step - loss: 7.8855 - val_loss: 8.3821\n",
      "Epoch 488/1000\n",
      "23/23 [==============================] - 1s 55ms/step - loss: 7.8718 - val_loss: 8.3434\n",
      "Epoch 489/1000\n",
      "23/23 [==============================] - 1s 55ms/step - loss: 7.8945 - val_loss: 8.3603\n",
      "Epoch 490/1000\n",
      "23/23 [==============================] - 1s 30ms/step - loss: 8.0206 - val_loss: 8.5187\n",
      "Epoch 491/1000\n",
      "23/23 [==============================] - 1s 57ms/step - loss: 7.9123 - val_loss: 8.3538\n",
      "Epoch 492/1000\n",
      "23/23 [==============================] - 1s 52ms/step - loss: 7.8876 - val_loss: 8.3836\n",
      "Epoch 493/1000\n",
      "23/23 [==============================] - 1s 59ms/step - loss: 7.8332 - val_loss: 8.3504\n",
      "Epoch 494/1000\n",
      "23/23 [==============================] - 1s 47ms/step - loss: 7.8805 - val_loss: 8.3837\n",
      "Epoch 495/1000\n",
      "23/23 [==============================] - 1s 55ms/step - loss: 7.8096 - val_loss: 8.3513\n",
      "Epoch 496/1000\n",
      "23/23 [==============================] - 1s 51ms/step - loss: 7.7967 - val_loss: 8.3416\n",
      "Epoch 497/1000\n",
      "23/23 [==============================] - 1s 60ms/step - loss: 7.8913 - val_loss: 8.3412\n",
      "Epoch 498/1000\n",
      "23/23 [==============================] - 1s 56ms/step - loss: 7.8811 - val_loss: 8.3487\n",
      "Epoch 499/1000\n",
      "23/23 [==============================] - 1s 60ms/step - loss: 7.8504 - val_loss: 8.4924\n",
      "Epoch 500/1000\n",
      "23/23 [==============================] - 1s 59ms/step - loss: 7.9209 - val_loss: 8.3406\n",
      "Epoch 501/1000\n",
      "23/23 [==============================] - 1s 63ms/step - loss: 7.9581 - val_loss: 8.3229\n",
      "Epoch 502/1000\n",
      "23/23 [==============================] - 1s 52ms/step - loss: 7.8927 - val_loss: 8.3298\n",
      "Epoch 503/1000\n",
      "23/23 [==============================] - 1s 54ms/step - loss: 7.8713 - val_loss: 8.3309\n",
      "Epoch 504/1000\n",
      "23/23 [==============================] - 1s 63ms/step - loss: 7.8612 - val_loss: 8.3605\n",
      "Epoch 505/1000\n",
      "23/23 [==============================] - 1s 56ms/step - loss: 7.8387 - val_loss: 8.3628\n",
      "Epoch 506/1000\n",
      "23/23 [==============================] - 1s 50ms/step - loss: 7.8534 - val_loss: 8.3428\n",
      "Epoch 507/1000\n",
      "23/23 [==============================] - 1s 61ms/step - loss: 7.8822 - val_loss: 8.3440\n",
      "Epoch 508/1000\n",
      "23/23 [==============================] - 1s 55ms/step - loss: 7.8782 - val_loss: 8.3429\n",
      "Epoch 509/1000\n",
      "23/23 [==============================] - 1s 61ms/step - loss: 7.8829 - val_loss: 8.3475\n",
      "Epoch 510/1000\n",
      "23/23 [==============================] - 1s 57ms/step - loss: 7.8131 - val_loss: 8.3866\n",
      "Epoch 511/1000\n",
      "23/23 [==============================] - 1s 32ms/step - loss: 7.9137 - val_loss: 8.3533\n",
      "Epoch 512/1000\n",
      "23/23 [==============================] - 1s 43ms/step - loss: 7.8358 - val_loss: 8.3375\n",
      "Epoch 513/1000\n",
      "23/23 [==============================] - 1s 57ms/step - loss: 7.8546 - val_loss: 8.3304\n",
      "Epoch 514/1000\n",
      "23/23 [==============================] - 1s 57ms/step - loss: 7.9265 - val_loss: 8.3544\n",
      "Epoch 515/1000\n",
      "23/23 [==============================] - 1s 54ms/step - loss: 7.9653 - val_loss: 8.3989\n",
      "Epoch 516/1000\n",
      "23/23 [==============================] - 1s 62ms/step - loss: 7.9087 - val_loss: 8.4840\n",
      "Epoch 517/1000\n",
      "23/23 [==============================] - 1s 62ms/step - loss: 7.8551 - val_loss: 8.3770\n",
      "Epoch 518/1000\n",
      "23/23 [==============================] - 1s 54ms/step - loss: 7.8731 - val_loss: 8.3382\n",
      "Epoch 519/1000\n",
      "23/23 [==============================] - 1s 56ms/step - loss: 7.8788 - val_loss: 8.3474\n",
      "Epoch 520/1000\n",
      "23/23 [==============================] - 1s 55ms/step - loss: 7.8673 - val_loss: 8.4639\n",
      "Epoch 521/1000\n",
      "23/23 [==============================] - 1s 51ms/step - loss: 7.8961 - val_loss: 8.3906\n",
      "Epoch 522/1000\n",
      "23/23 [==============================] - 1s 30ms/step - loss: 7.8837 - val_loss: 8.3902\n",
      "Epoch 523/1000\n",
      "23/23 [==============================] - 1s 50ms/step - loss: 7.8654 - val_loss: 8.3428\n",
      "Epoch 524/1000\n",
      "23/23 [==============================] - 1s 55ms/step - loss: 7.8871 - val_loss: 8.3531\n",
      "Epoch 525/1000\n",
      "23/23 [==============================] - 1s 61ms/step - loss: 7.7911 - val_loss: 8.3438\n",
      "Epoch 526/1000\n",
      "23/23 [==============================] - 1s 54ms/step - loss: 7.8990 - val_loss: 8.3527\n",
      "Epoch 527/1000\n",
      "23/23 [==============================] - 1s 54ms/step - loss: 7.9571 - val_loss: 8.5236\n",
      "Epoch 528/1000\n",
      "23/23 [==============================] - 1s 54ms/step - loss: 7.8775 - val_loss: 8.3612\n",
      "Epoch 529/1000\n",
      "23/23 [==============================] - 1s 55ms/step - loss: 7.8818 - val_loss: 8.3548\n",
      "Epoch 530/1000\n",
      "23/23 [==============================] - 1s 51ms/step - loss: 7.8425 - val_loss: 8.3484\n",
      "Epoch 531/1000\n",
      "23/23 [==============================] - 1s 57ms/step - loss: 7.9102 - val_loss: 8.3791\n",
      "Epoch 532/1000\n",
      "23/23 [==============================] - 1s 61ms/step - loss: 7.9219 - val_loss: 8.4494\n",
      "Epoch 533/1000\n",
      "23/23 [==============================] - 1s 51ms/step - loss: 7.8259 - val_loss: 8.4494\n",
      "Epoch 534/1000\n",
      "23/23 [==============================] - 1s 44ms/step - loss: 7.9548 - val_loss: 8.3551\n",
      "Epoch 535/1000\n",
      "23/23 [==============================] - 1s 58ms/step - loss: 7.9151 - val_loss: 8.3607\n",
      "Epoch 536/1000\n",
      "23/23 [==============================] - 1s 60ms/step - loss: 7.8443 - val_loss: 8.3649\n",
      "Epoch 537/1000\n",
      "23/23 [==============================] - 1s 63ms/step - loss: 7.9028 - val_loss: 8.3706\n",
      "Epoch 538/1000\n",
      "23/23 [==============================] - 1s 50ms/step - loss: 7.8488 - val_loss: 8.3702\n",
      "Epoch 539/1000\n",
      "23/23 [==============================] - 1s 49ms/step - loss: 7.8531 - val_loss: 8.3556\n",
      "Epoch 540/1000\n",
      "23/23 [==============================] - 1s 54ms/step - loss: 7.8268 - val_loss: 8.3381\n",
      "Epoch 541/1000\n",
      "23/23 [==============================] - 1s 55ms/step - loss: 7.8297 - val_loss: 8.4128\n",
      "Epoch 542/1000\n",
      "23/23 [==============================] - 1s 39ms/step - loss: 7.8215 - val_loss: 8.3323\n",
      "Epoch 543/1000\n",
      "23/23 [==============================] - 1s 54ms/step - loss: 7.7794 - val_loss: 8.3546\n",
      "Epoch 544/1000\n",
      "23/23 [==============================] - 1s 55ms/step - loss: 7.8294 - val_loss: 8.3327\n",
      "Epoch 545/1000\n",
      "23/23 [==============================] - 1s 64ms/step - loss: 7.8525 - val_loss: 8.4181\n",
      "Epoch 546/1000\n",
      "23/23 [==============================] - 1s 63ms/step - loss: 7.8185 - val_loss: 8.5249\n",
      "Epoch 547/1000\n",
      "23/23 [==============================] - 1s 57ms/step - loss: 7.9241 - val_loss: 8.3575\n",
      "Epoch 548/1000\n",
      "23/23 [==============================] - 1s 63ms/step - loss: 7.8434 - val_loss: 8.3208\n",
      "Epoch 549/1000\n",
      "23/23 [==============================] - 1s 57ms/step - loss: 7.8558 - val_loss: 8.4096\n",
      "Epoch 550/1000\n",
      "23/23 [==============================] - 1s 63ms/step - loss: 7.8877 - val_loss: 8.3736\n",
      "Epoch 551/1000\n",
      "23/23 [==============================] - 1s 57ms/step - loss: 7.9080 - val_loss: 8.3651\n",
      "Epoch 552/1000\n",
      "23/23 [==============================] - 1s 54ms/step - loss: 7.8771 - val_loss: 8.5233\n",
      "Epoch 553/1000\n",
      "23/23 [==============================] - 1s 53ms/step - loss: 7.9403 - val_loss: 8.3417\n",
      "Epoch 554/1000\n",
      "23/23 [==============================] - 1s 53ms/step - loss: 7.7929 - val_loss: 8.3276\n",
      "Epoch 555/1000\n",
      "23/23 [==============================] - 1s 61ms/step - loss: 7.7903 - val_loss: 8.3427\n",
      "Epoch 556/1000\n",
      "23/23 [==============================] - 1s 55ms/step - loss: 7.8636 - val_loss: 8.3615\n",
      "Epoch 557/1000\n",
      "23/23 [==============================] - 1s 58ms/step - loss: 7.8825 - val_loss: 8.4858\n",
      "Epoch 558/1000\n",
      "23/23 [==============================] - 1s 52ms/step - loss: 7.8292 - val_loss: 8.3190\n",
      "Epoch 559/1000\n",
      "23/23 [==============================] - 1s 59ms/step - loss: 7.8924 - val_loss: 8.4954\n",
      "Epoch 560/1000\n",
      "23/23 [==============================] - 1s 49ms/step - loss: 7.8307 - val_loss: 8.3410\n",
      "Epoch 561/1000\n",
      "23/23 [==============================] - 1s 59ms/step - loss: 7.8948 - val_loss: 8.3449\n",
      "Epoch 562/1000\n",
      "23/23 [==============================] - 1s 60ms/step - loss: 7.9571 - val_loss: 8.3493\n",
      "Epoch 563/1000\n",
      "23/23 [==============================] - 1s 57ms/step - loss: 7.8514 - val_loss: 8.3414\n",
      "Epoch 564/1000\n",
      "23/23 [==============================] - 1s 59ms/step - loss: 7.9001 - val_loss: 8.3500\n",
      "Epoch 565/1000\n",
      "23/23 [==============================] - 1s 56ms/step - loss: 7.8617 - val_loss: 8.3435\n",
      "Epoch 566/1000\n",
      "23/23 [==============================] - 1s 59ms/step - loss: 7.8413 - val_loss: 8.3384\n",
      "Epoch 567/1000\n",
      "23/23 [==============================] - 1s 60ms/step - loss: 7.8773 - val_loss: 8.4152\n",
      "Epoch 568/1000\n",
      "23/23 [==============================] - 1s 54ms/step - loss: 7.8906 - val_loss: 8.3586\n",
      "Epoch 569/1000\n",
      "23/23 [==============================] - 1s 57ms/step - loss: 7.8709 - val_loss: 8.3545\n",
      "Epoch 570/1000\n",
      "23/23 [==============================] - 1s 59ms/step - loss: 7.8353 - val_loss: 8.3745\n",
      "Epoch 571/1000\n",
      "23/23 [==============================] - 1s 64ms/step - loss: 7.8885 - val_loss: 8.3614\n",
      "Epoch 572/1000\n",
      "23/23 [==============================] - 1s 60ms/step - loss: 7.8144 - val_loss: 8.3549\n",
      "Epoch 573/1000\n",
      "23/23 [==============================] - 1s 55ms/step - loss: 7.8317 - val_loss: 8.3837\n",
      "Epoch 574/1000\n",
      "23/23 [==============================] - 1s 57ms/step - loss: 7.8697 - val_loss: 8.3725\n",
      "Epoch 575/1000\n",
      "23/23 [==============================] - 1s 59ms/step - loss: 7.9013 - val_loss: 8.3382\n",
      "Epoch 576/1000\n",
      "23/23 [==============================] - 1s 58ms/step - loss: 7.8519 - val_loss: 8.3532\n",
      "Epoch 577/1000\n",
      "23/23 [==============================] - 1s 62ms/step - loss: 7.8952 - val_loss: 8.3669\n",
      "Epoch 578/1000\n",
      "23/23 [==============================] - 1s 58ms/step - loss: 7.7975 - val_loss: 8.3456\n",
      "Epoch 579/1000\n",
      "23/23 [==============================] - 1s 57ms/step - loss: 7.9304 - val_loss: 8.3530\n",
      "Epoch 580/1000\n",
      "23/23 [==============================] - 1s 60ms/step - loss: 7.9255 - val_loss: 8.4040\n",
      "Epoch 581/1000\n",
      "23/23 [==============================] - 1s 60ms/step - loss: 7.8450 - val_loss: 8.3501\n",
      "Epoch 582/1000\n",
      "23/23 [==============================] - 1s 62ms/step - loss: 7.9130 - val_loss: 8.3372\n",
      "Epoch 583/1000\n",
      "23/23 [==============================] - 1s 58ms/step - loss: 7.8572 - val_loss: 8.3407\n",
      "Epoch 584/1000\n",
      "23/23 [==============================] - 1s 50ms/step - loss: 7.9041 - val_loss: 8.3344\n",
      "Epoch 585/1000\n",
      "23/23 [==============================] - 1s 54ms/step - loss: 7.9253 - val_loss: 8.9676\n",
      "Epoch 586/1000\n",
      "23/23 [==============================] - 1s 55ms/step - loss: 7.9638 - val_loss: 8.7077\n",
      "Epoch 587/1000\n",
      "23/23 [==============================] - 1s 60ms/step - loss: 7.9180 - val_loss: 8.3626\n",
      "Epoch 588/1000\n",
      "23/23 [==============================] - 1s 56ms/step - loss: 7.8568 - val_loss: 8.3483\n",
      "Epoch 589/1000\n",
      "23/23 [==============================] - 1s 54ms/step - loss: 7.8830 - val_loss: 8.5024\n",
      "Epoch 590/1000\n",
      "23/23 [==============================] - 1s 57ms/step - loss: 7.9128 - val_loss: 8.3435\n",
      "Epoch 591/1000\n",
      "23/23 [==============================] - 1s 60ms/step - loss: 7.8853 - val_loss: 8.3740\n",
      "Epoch 592/1000\n",
      "23/23 [==============================] - 1s 52ms/step - loss: 7.8318 - val_loss: 8.3403\n",
      "Epoch 593/1000\n",
      "23/23 [==============================] - 1s 61ms/step - loss: 7.9139 - val_loss: 8.4249\n",
      "Epoch 594/1000\n",
      "23/23 [==============================] - 1s 55ms/step - loss: 7.8801 - val_loss: 8.3457\n",
      "Epoch 595/1000\n",
      "23/23 [==============================] - 1s 59ms/step - loss: 7.9309 - val_loss: 8.4465\n",
      "Epoch 596/1000\n",
      "23/23 [==============================] - 1s 60ms/step - loss: 7.8434 - val_loss: 8.3475\n",
      "Epoch 597/1000\n",
      "23/23 [==============================] - 1s 29ms/step - loss: 7.8393 - val_loss: 8.4107\n",
      "Epoch 598/1000\n",
      "23/23 [==============================] - 1s 35ms/step - loss: 7.9515 - val_loss: 8.3785\n",
      "Epoch 599/1000\n",
      "23/23 [==============================] - 1s 50ms/step - loss: 7.9385 - val_loss: 8.5705\n",
      "Epoch 600/1000\n",
      "23/23 [==============================] - 1s 52ms/step - loss: 7.8495 - val_loss: 8.3581\n",
      "Epoch 601/1000\n",
      "23/23 [==============================] - 1s 56ms/step - loss: 7.8313 - val_loss: 8.3493\n",
      "Epoch 602/1000\n",
      "23/23 [==============================] - 1s 54ms/step - loss: 7.8592 - val_loss: 8.3415\n",
      "Epoch 603/1000\n",
      "23/23 [==============================] - 1s 62ms/step - loss: 7.8394 - val_loss: 8.4356\n",
      "Epoch 604/1000\n",
      "23/23 [==============================] - 1s 59ms/step - loss: 7.9005 - val_loss: 8.3609\n",
      "Epoch 605/1000\n",
      "23/23 [==============================] - 1s 58ms/step - loss: 7.9176 - val_loss: 8.5044\n",
      "Epoch 606/1000\n",
      "23/23 [==============================] - 1s 53ms/step - loss: 7.8721 - val_loss: 8.3473\n",
      "Epoch 607/1000\n",
      "23/23 [==============================] - 1s 60ms/step - loss: 7.8829 - val_loss: 8.3348\n",
      "Epoch 608/1000\n",
      "23/23 [==============================] - 1s 63ms/step - loss: 7.8019 - val_loss: 8.3940\n",
      "Epoch 609/1000\n",
      "23/23 [==============================] - 1s 63ms/step - loss: 7.8956 - val_loss: 8.4189\n",
      "Epoch 610/1000\n",
      "23/23 [==============================] - 1s 58ms/step - loss: 7.9076 - val_loss: 8.3572\n",
      "Epoch 611/1000\n",
      "23/23 [==============================] - 1s 60ms/step - loss: 7.7882 - val_loss: 8.4964\n",
      "Epoch 612/1000\n",
      "23/23 [==============================] - 1s 60ms/step - loss: 7.9027 - val_loss: 8.4206\n",
      "Epoch 613/1000\n",
      "23/23 [==============================] - 1s 34ms/step - loss: 7.8387 - val_loss: 8.3847\n",
      "Epoch 614/1000\n",
      "23/23 [==============================] - 1s 57ms/step - loss: 7.9203 - val_loss: 8.3639\n",
      "Epoch 615/1000\n",
      "23/23 [==============================] - 1s 61ms/step - loss: 7.9230 - val_loss: 8.3654\n",
      "Epoch 616/1000\n",
      "23/23 [==============================] - 1s 52ms/step - loss: 7.8845 - val_loss: 8.3322\n",
      "Epoch 617/1000\n",
      "23/23 [==============================] - 1s 58ms/step - loss: 7.8867 - val_loss: 8.3612\n",
      "Epoch 618/1000\n",
      "23/23 [==============================] - 1s 55ms/step - loss: 7.8083 - val_loss: 8.4000\n",
      "Epoch 619/1000\n",
      "23/23 [==============================] - 1s 54ms/step - loss: 7.8010 - val_loss: 8.4023\n",
      "Epoch 620/1000\n",
      "23/23 [==============================] - 1s 53ms/step - loss: 7.8679 - val_loss: 8.3897\n",
      "Epoch 621/1000\n",
      "23/23 [==============================] - 1s 53ms/step - loss: 7.8537 - val_loss: 8.4102\n",
      "Epoch 622/1000\n",
      "23/23 [==============================] - 1s 62ms/step - loss: 7.7808 - val_loss: 8.4015\n",
      "Epoch 623/1000\n",
      "23/23 [==============================] - 1s 56ms/step - loss: 7.9039 - val_loss: 8.3605\n",
      "Epoch 624/1000\n",
      "23/23 [==============================] - 1s 56ms/step - loss: 7.8768 - val_loss: 8.3792\n",
      "Epoch 625/1000\n",
      "23/23 [==============================] - 1s 50ms/step - loss: 7.9527 - val_loss: 8.3698\n",
      "Epoch 626/1000\n",
      "23/23 [==============================] - 1s 56ms/step - loss: 7.9150 - val_loss: 8.3823\n",
      "Epoch 627/1000\n",
      "23/23 [==============================] - 1s 52ms/step - loss: 7.8123 - val_loss: 8.3387\n",
      "Epoch 628/1000\n",
      "23/23 [==============================] - 1s 60ms/step - loss: 7.9058 - val_loss: 8.3496\n",
      "Epoch 629/1000\n",
      "23/23 [==============================] - 1s 53ms/step - loss: 7.9163 - val_loss: 8.3145\n",
      "Epoch 630/1000\n",
      "23/23 [==============================] - 1s 60ms/step - loss: 7.8249 - val_loss: 8.3573\n",
      "Epoch 631/1000\n",
      "23/23 [==============================] - 1s 48ms/step - loss: 7.8335 - val_loss: 8.3805\n",
      "Epoch 632/1000\n",
      "23/23 [==============================] - 1s 53ms/step - loss: 7.8481 - val_loss: 8.3392\n",
      "Epoch 633/1000\n",
      "23/23 [==============================] - 1s 55ms/step - loss: 7.8836 - val_loss: 8.3513\n",
      "Epoch 634/1000\n",
      "23/23 [==============================] - 1s 58ms/step - loss: 7.8743 - val_loss: 8.3622\n",
      "Epoch 635/1000\n",
      "23/23 [==============================] - 1s 54ms/step - loss: 7.8913 - val_loss: 8.3337\n",
      "Epoch 636/1000\n",
      "23/23 [==============================] - 1s 54ms/step - loss: 7.8629 - val_loss: 8.3505\n",
      "Epoch 637/1000\n",
      "23/23 [==============================] - 1s 60ms/step - loss: 7.8744 - val_loss: 8.5545\n",
      "Epoch 638/1000\n",
      "23/23 [==============================] - 1s 40ms/step - loss: 7.9769 - val_loss: 8.3518\n",
      "Epoch 639/1000\n",
      "23/23 [==============================] - 1s 55ms/step - loss: 7.7856 - val_loss: 8.3414\n",
      "Epoch 640/1000\n",
      "23/23 [==============================] - 1s 56ms/step - loss: 7.8992 - val_loss: 8.3765\n",
      "Epoch 641/1000\n",
      "23/23 [==============================] - 1s 35ms/step - loss: 7.8447 - val_loss: 8.4683\n",
      "Epoch 642/1000\n",
      "23/23 [==============================] - 1s 58ms/step - loss: 7.8666 - val_loss: 8.3403\n",
      "Epoch 643/1000\n",
      "23/23 [==============================] - 1s 30ms/step - loss: 7.8375 - val_loss: 8.3649\n",
      "Epoch 644/1000\n",
      "23/23 [==============================] - 1s 56ms/step - loss: 7.8656 - val_loss: 8.3638\n",
      "Epoch 645/1000\n",
      "23/23 [==============================] - 1s 55ms/step - loss: 7.8091 - val_loss: 8.3528\n",
      "Epoch 646/1000\n",
      "23/23 [==============================] - 1s 61ms/step - loss: 7.9205 - val_loss: 8.3544\n",
      "Epoch 647/1000\n",
      "23/23 [==============================] - 1s 41ms/step - loss: 7.8440 - val_loss: 8.3398\n",
      "Epoch 648/1000\n",
      "23/23 [==============================] - 1s 60ms/step - loss: 7.8328 - val_loss: 8.3519\n",
      "Epoch 649/1000\n",
      "23/23 [==============================] - 1s 53ms/step - loss: 7.8343 - val_loss: 8.3625\n",
      "Epoch 650/1000\n",
      "23/23 [==============================] - 1s 58ms/step - loss: 7.8760 - val_loss: 8.3835\n",
      "Epoch 651/1000\n",
      "23/23 [==============================] - 1s 51ms/step - loss: 7.8270 - val_loss: 8.3372\n",
      "Epoch 652/1000\n",
      "23/23 [==============================] - 1s 57ms/step - loss: 7.8725 - val_loss: 8.3626\n",
      "Epoch 653/1000\n",
      "23/23 [==============================] - 1s 61ms/step - loss: 7.8572 - val_loss: 8.3279\n",
      "Epoch 654/1000\n",
      "23/23 [==============================] - 1s 57ms/step - loss: 7.8153 - val_loss: 8.3639\n",
      "Epoch 655/1000\n",
      "23/23 [==============================] - 1s 53ms/step - loss: 7.8007 - val_loss: 8.3535\n",
      "Epoch 656/1000\n",
      "23/23 [==============================] - 1s 60ms/step - loss: 7.8102 - val_loss: 8.3417\n",
      "Epoch 657/1000\n",
      "23/23 [==============================] - 1s 60ms/step - loss: 7.8101 - val_loss: 8.3657\n",
      "Epoch 658/1000\n",
      "23/23 [==============================] - 1s 56ms/step - loss: 7.8381 - val_loss: 8.3961\n",
      "Epoch 659/1000\n",
      "23/23 [==============================] - 1s 59ms/step - loss: 7.9301 - val_loss: 8.3253\n",
      "Epoch 660/1000\n",
      "23/23 [==============================] - 1s 59ms/step - loss: 7.8339 - val_loss: 8.3477\n",
      "Epoch 661/1000\n",
      "23/23 [==============================] - 1s 58ms/step - loss: 7.8821 - val_loss: 8.3483\n",
      "Epoch 662/1000\n",
      "23/23 [==============================] - 1s 30ms/step - loss: 7.7981 - val_loss: 8.3243\n",
      "Epoch 663/1000\n",
      "23/23 [==============================] - 1s 56ms/step - loss: 7.8617 - val_loss: 8.4124\n",
      "Epoch 664/1000\n",
      "23/23 [==============================] - 1s 55ms/step - loss: 7.8207 - val_loss: 8.4065\n",
      "Epoch 665/1000\n",
      "23/23 [==============================] - 1s 62ms/step - loss: 7.8114 - val_loss: 8.3299\n",
      "Epoch 666/1000\n",
      "23/23 [==============================] - 1s 56ms/step - loss: 7.8804 - val_loss: 8.5202\n",
      "Epoch 667/1000\n",
      "23/23 [==============================] - 1s 59ms/step - loss: 7.8689 - val_loss: 8.3405\n",
      "Epoch 668/1000\n",
      "23/23 [==============================] - 1s 62ms/step - loss: 7.9084 - val_loss: 8.4070\n",
      "Epoch 669/1000\n",
      "23/23 [==============================] - 1s 38ms/step - loss: 7.9875 - val_loss: 8.3562\n",
      "Epoch 670/1000\n",
      "23/23 [==============================] - 1s 64ms/step - loss: 7.8755 - val_loss: 8.4529\n",
      "Epoch 671/1000\n",
      "23/23 [==============================] - 1s 30ms/step - loss: 7.8748 - val_loss: 8.3740\n",
      "Epoch 672/1000\n",
      "23/23 [==============================] - 1s 57ms/step - loss: 7.8570 - val_loss: 8.3934\n",
      "Epoch 673/1000\n",
      "23/23 [==============================] - 1s 62ms/step - loss: 7.8523 - val_loss: 8.3731\n",
      "Epoch 674/1000\n",
      "23/23 [==============================] - 1s 54ms/step - loss: 7.8457 - val_loss: 8.3483\n",
      "Epoch 675/1000\n",
      "23/23 [==============================] - 1s 57ms/step - loss: 7.8200 - val_loss: 8.3528\n",
      "Epoch 676/1000\n",
      "23/23 [==============================] - 1s 60ms/step - loss: 7.8291 - val_loss: 8.3604\n",
      "Epoch 677/1000\n",
      "23/23 [==============================] - 1s 59ms/step - loss: 7.9068 - val_loss: 8.3727\n",
      "Epoch 678/1000\n",
      "23/23 [==============================] - 1s 58ms/step - loss: 7.8274 - val_loss: 8.3372\n",
      "Epoch 679/1000\n",
      "23/23 [==============================] - 1s 55ms/step - loss: 7.7988 - val_loss: 8.5223\n",
      "Epoch 680/1000\n",
      "23/23 [==============================] - 1s 55ms/step - loss: 7.8079 - val_loss: 8.3951\n",
      "Epoch 681/1000\n",
      "23/23 [==============================] - 1s 63ms/step - loss: 7.8513 - val_loss: 8.4124\n",
      "Epoch 682/1000\n",
      "23/23 [==============================] - 1s 61ms/step - loss: 7.8374 - val_loss: 8.4821\n",
      "Epoch 683/1000\n",
      "23/23 [==============================] - 1s 61ms/step - loss: 7.8337 - val_loss: 8.3225\n",
      "Epoch 684/1000\n",
      "23/23 [==============================] - 1s 53ms/step - loss: 7.8190 - val_loss: 8.3398\n",
      "Epoch 685/1000\n",
      "23/23 [==============================] - 1s 60ms/step - loss: 7.8529 - val_loss: 8.3291\n",
      "Epoch 686/1000\n",
      "23/23 [==============================] - 1s 48ms/step - loss: 7.8400 - val_loss: 8.3570\n",
      "Epoch 687/1000\n",
      "23/23 [==============================] - 1s 61ms/step - loss: 7.7653 - val_loss: 8.3546\n",
      "Epoch 688/1000\n",
      "23/23 [==============================] - 1s 63ms/step - loss: 7.8638 - val_loss: 8.5014\n",
      "Epoch 689/1000\n",
      "23/23 [==============================] - 1s 55ms/step - loss: 7.7915 - val_loss: 8.3980\n",
      "Epoch 690/1000\n",
      "23/23 [==============================] - 1s 30ms/step - loss: 7.8663 - val_loss: 8.3274\n",
      "Epoch 691/1000\n",
      "23/23 [==============================] - 1s 38ms/step - loss: 7.8484 - val_loss: 8.3768\n",
      "Epoch 692/1000\n",
      "23/23 [==============================] - 1s 59ms/step - loss: 7.8288 - val_loss: 8.4414\n",
      "Epoch 693/1000\n",
      "23/23 [==============================] - 1s 64ms/step - loss: 7.8934 - val_loss: 8.3504\n",
      "Epoch 694/1000\n",
      "23/23 [==============================] - 1s 59ms/step - loss: 7.8735 - val_loss: 8.3598\n",
      "Epoch 695/1000\n",
      "23/23 [==============================] - 1s 52ms/step - loss: 7.7644 - val_loss: 8.3378\n",
      "Epoch 696/1000\n",
      "23/23 [==============================] - 1s 54ms/step - loss: 7.9307 - val_loss: 8.3465\n",
      "Epoch 697/1000\n",
      "23/23 [==============================] - 1s 54ms/step - loss: 7.8852 - val_loss: 8.3189\n",
      "Epoch 698/1000\n",
      "23/23 [==============================] - 1s 42ms/step - loss: 7.8094 - val_loss: 8.3345\n",
      "Epoch 699/1000\n",
      "23/23 [==============================] - 1s 59ms/step - loss: 7.8337 - val_loss: 8.3232\n",
      "Epoch 700/1000\n",
      "23/23 [==============================] - 1s 65ms/step - loss: 7.8808 - val_loss: 8.3082\n",
      "Epoch 701/1000\n",
      "23/23 [==============================] - 1s 54ms/step - loss: 7.8687 - val_loss: 8.3476\n",
      "Epoch 702/1000\n",
      "23/23 [==============================] - 1s 58ms/step - loss: 7.8591 - val_loss: 8.3243\n",
      "Epoch 703/1000\n",
      "23/23 [==============================] - 1s 55ms/step - loss: 7.8959 - val_loss: 8.3349\n",
      "Epoch 704/1000\n",
      "23/23 [==============================] - 1s 51ms/step - loss: 7.8750 - val_loss: 8.3863\n",
      "Epoch 705/1000\n",
      "23/23 [==============================] - 1s 59ms/step - loss: 7.8359 - val_loss: 8.3387\n",
      "Epoch 706/1000\n",
      "23/23 [==============================] - 1s 51ms/step - loss: 7.8829 - val_loss: 8.3380\n",
      "Epoch 707/1000\n",
      "23/23 [==============================] - 1s 58ms/step - loss: 7.8421 - val_loss: 8.3744\n",
      "Epoch 708/1000\n",
      "23/23 [==============================] - 1s 57ms/step - loss: 7.8060 - val_loss: 8.3367\n",
      "Epoch 709/1000\n",
      "23/23 [==============================] - 1s 64ms/step - loss: 7.8866 - val_loss: 8.3485\n",
      "Epoch 710/1000\n",
      "23/23 [==============================] - 1s 53ms/step - loss: 7.8197 - val_loss: 8.3429\n",
      "Epoch 711/1000\n",
      "23/23 [==============================] - 1s 51ms/step - loss: 7.8410 - val_loss: 8.3608\n",
      "Epoch 712/1000\n",
      "23/23 [==============================] - 1s 54ms/step - loss: 7.8716 - val_loss: 8.3723\n",
      "Epoch 713/1000\n",
      "23/23 [==============================] - 1s 56ms/step - loss: 7.8402 - val_loss: 8.3442\n",
      "Epoch 714/1000\n",
      "23/23 [==============================] - 1s 30ms/step - loss: 7.9970 - val_loss: 8.3657\n",
      "Epoch 715/1000\n",
      "23/23 [==============================] - 1s 51ms/step - loss: 7.9345 - val_loss: 8.4790\n",
      "Epoch 716/1000\n",
      "23/23 [==============================] - 1s 49ms/step - loss: 7.7627 - val_loss: 8.3812\n",
      "Epoch 717/1000\n",
      "23/23 [==============================] - 1s 51ms/step - loss: 7.8817 - val_loss: 8.3425\n",
      "Epoch 718/1000\n",
      "23/23 [==============================] - 1s 55ms/step - loss: 7.8449 - val_loss: 8.3609\n",
      "Epoch 719/1000\n",
      "23/23 [==============================] - 1s 51ms/step - loss: 7.8360 - val_loss: 8.3969\n",
      "Epoch 720/1000\n",
      "23/23 [==============================] - 1s 30ms/step - loss: 7.8337 - val_loss: 8.3356\n",
      "Epoch 721/1000\n",
      "23/23 [==============================] - 1s 59ms/step - loss: 7.8689 - val_loss: 8.3652\n",
      "Epoch 722/1000\n",
      "23/23 [==============================] - 1s 55ms/step - loss: 7.8771 - val_loss: 8.3754\n",
      "Epoch 723/1000\n",
      "23/23 [==============================] - 1s 59ms/step - loss: 7.8512 - val_loss: 8.4767\n",
      "Epoch 724/1000\n",
      "23/23 [==============================] - 1s 62ms/step - loss: 7.8811 - val_loss: 8.3677\n",
      "Epoch 725/1000\n",
      "23/23 [==============================] - 1s 62ms/step - loss: 7.8321 - val_loss: 8.3425\n",
      "Epoch 726/1000\n",
      "23/23 [==============================] - 1s 56ms/step - loss: 7.8992 - val_loss: 8.3826\n",
      "Epoch 727/1000\n",
      "23/23 [==============================] - 1s 61ms/step - loss: 7.8386 - val_loss: 8.3687\n",
      "Epoch 728/1000\n",
      "23/23 [==============================] - 1s 59ms/step - loss: 7.8862 - val_loss: 8.4031\n",
      "Epoch 729/1000\n",
      "23/23 [==============================] - 1s 56ms/step - loss: 7.9008 - val_loss: 8.4039\n",
      "Epoch 730/1000\n",
      "23/23 [==============================] - 1s 57ms/step - loss: 7.9068 - val_loss: 8.3309\n",
      "Epoch 731/1000\n",
      "23/23 [==============================] - 1s 58ms/step - loss: 7.9022 - val_loss: 8.3274\n",
      "Epoch 732/1000\n",
      "23/23 [==============================] - 1s 57ms/step - loss: 7.8603 - val_loss: 8.3229\n",
      "Epoch 733/1000\n",
      "23/23 [==============================] - 1s 58ms/step - loss: 7.9240 - val_loss: 8.4379\n",
      "Epoch 734/1000\n",
      "23/23 [==============================] - 1s 54ms/step - loss: 7.9295 - val_loss: 8.3404\n",
      "Epoch 735/1000\n",
      "23/23 [==============================] - 1s 51ms/step - loss: 7.8113 - val_loss: 8.3567\n",
      "Epoch 736/1000\n",
      "23/23 [==============================] - 1s 61ms/step - loss: 7.9284 - val_loss: 8.6659\n",
      "Epoch 737/1000\n",
      "23/23 [==============================] - 1s 57ms/step - loss: 7.9043 - val_loss: 8.4053\n",
      "Epoch 738/1000\n",
      "23/23 [==============================] - 1s 49ms/step - loss: 7.7984 - val_loss: 8.3123\n",
      "Epoch 739/1000\n",
      "23/23 [==============================] - 1s 57ms/step - loss: 7.8337 - val_loss: 8.4392\n",
      "Epoch 740/1000\n",
      "23/23 [==============================] - 1s 57ms/step - loss: 7.9117 - val_loss: 8.3402\n",
      "Epoch 741/1000\n",
      "23/23 [==============================] - 1s 53ms/step - loss: 7.8586 - val_loss: 8.3402\n",
      "Epoch 742/1000\n",
      "23/23 [==============================] - 1s 49ms/step - loss: 7.8732 - val_loss: 8.3472\n",
      "Epoch 743/1000\n",
      "23/23 [==============================] - 1s 52ms/step - loss: 7.8248 - val_loss: 8.3381\n",
      "Epoch 744/1000\n",
      "23/23 [==============================] - 1s 56ms/step - loss: 7.9435 - val_loss: 8.3534\n",
      "Epoch 745/1000\n",
      "23/23 [==============================] - 1s 60ms/step - loss: 7.8638 - val_loss: 8.3341\n",
      "Epoch 746/1000\n",
      "23/23 [==============================] - 1s 58ms/step - loss: 7.8832 - val_loss: 8.4569\n",
      "Epoch 747/1000\n",
      "23/23 [==============================] - 1s 56ms/step - loss: 7.8774 - val_loss: 8.5513\n",
      "Epoch 748/1000\n",
      "23/23 [==============================] - 1s 42ms/step - loss: 7.8792 - val_loss: 8.3662\n",
      "Epoch 749/1000\n",
      "23/23 [==============================] - 1s 60ms/step - loss: 7.8613 - val_loss: 8.3777\n",
      "Epoch 750/1000\n",
      "23/23 [==============================] - 1s 56ms/step - loss: 7.8487 - val_loss: 8.3368\n",
      "Epoch 751/1000\n",
      "23/23 [==============================] - 1s 45ms/step - loss: 7.8272 - val_loss: 8.3356\n",
      "Epoch 752/1000\n",
      "23/23 [==============================] - 1s 57ms/step - loss: 7.8423 - val_loss: 8.3618\n",
      "Epoch 753/1000\n",
      "23/23 [==============================] - 1s 58ms/step - loss: 7.8442 - val_loss: 8.4065\n",
      "Epoch 754/1000\n",
      "23/23 [==============================] - 1s 53ms/step - loss: 7.8357 - val_loss: 8.3360\n",
      "Epoch 755/1000\n",
      "23/23 [==============================] - 1s 52ms/step - loss: 7.8768 - val_loss: 8.3633\n",
      "Epoch 756/1000\n",
      "23/23 [==============================] - 1s 57ms/step - loss: 7.8878 - val_loss: 8.4057\n",
      "Epoch 757/1000\n",
      "23/23 [==============================] - 1s 61ms/step - loss: 7.8806 - val_loss: 8.3703\n",
      "Epoch 758/1000\n",
      "23/23 [==============================] - 1s 52ms/step - loss: 7.8071 - val_loss: 8.3389\n",
      "Epoch 759/1000\n",
      "23/23 [==============================] - 1s 60ms/step - loss: 7.8446 - val_loss: 8.3335\n",
      "Epoch 760/1000\n",
      "23/23 [==============================] - 1s 29ms/step - loss: 7.8416 - val_loss: 8.3599\n",
      "Epoch 761/1000\n",
      "23/23 [==============================] - 1s 50ms/step - loss: 7.8440 - val_loss: 8.3934\n",
      "Epoch 762/1000\n",
      "23/23 [==============================] - 1s 57ms/step - loss: 7.8519 - val_loss: 8.3375\n",
      "Epoch 763/1000\n",
      "23/23 [==============================] - 1s 52ms/step - loss: 7.8941 - val_loss: 8.3599\n",
      "Epoch 764/1000\n",
      "23/23 [==============================] - 1s 50ms/step - loss: 7.8985 - val_loss: 8.3270\n",
      "Epoch 765/1000\n",
      "23/23 [==============================] - 1s 57ms/step - loss: 7.7875 - val_loss: 8.3613\n",
      "Epoch 766/1000\n",
      "23/23 [==============================] - 1s 58ms/step - loss: 7.8139 - val_loss: 8.3573\n",
      "Epoch 767/1000\n",
      "23/23 [==============================] - 1s 57ms/step - loss: 7.8735 - val_loss: 8.5009\n",
      "Epoch 768/1000\n",
      "23/23 [==============================] - 1s 59ms/step - loss: 7.8976 - val_loss: 8.4323\n",
      "Epoch 769/1000\n",
      "23/23 [==============================] - 1s 58ms/step - loss: 7.9570 - val_loss: 8.3429\n",
      "Epoch 770/1000\n",
      "23/23 [==============================] - 1s 56ms/step - loss: 7.8116 - val_loss: 8.3460\n",
      "Epoch 771/1000\n",
      "23/23 [==============================] - 1s 61ms/step - loss: 7.8759 - val_loss: 8.3331\n",
      "Epoch 772/1000\n",
      "23/23 [==============================] - 1s 50ms/step - loss: 7.8412 - val_loss: 8.3479\n",
      "Epoch 773/1000\n",
      "23/23 [==============================] - 1s 46ms/step - loss: 7.8378 - val_loss: 8.3642\n",
      "Epoch 774/1000\n",
      "23/23 [==============================] - 1s 56ms/step - loss: 7.8873 - val_loss: 8.3694\n",
      "Epoch 775/1000\n",
      "23/23 [==============================] - 1s 59ms/step - loss: 7.8173 - val_loss: 8.3878\n",
      "Epoch 776/1000\n",
      "23/23 [==============================] - 1s 59ms/step - loss: 7.7986 - val_loss: 8.3399\n",
      "Epoch 777/1000\n",
      "23/23 [==============================] - 1s 53ms/step - loss: 7.8492 - val_loss: 8.3178\n",
      "Epoch 778/1000\n",
      "23/23 [==============================] - 1s 60ms/step - loss: 7.8271 - val_loss: 8.3847\n",
      "Epoch 779/1000\n",
      "23/23 [==============================] - 1s 51ms/step - loss: 7.8320 - val_loss: 8.3767\n",
      "Epoch 780/1000\n",
      "23/23 [==============================] - 1s 51ms/step - loss: 7.8535 - val_loss: 8.3444\n",
      "Epoch 781/1000\n",
      "23/23 [==============================] - 1s 54ms/step - loss: 7.8304 - val_loss: 8.3615\n",
      "Epoch 782/1000\n",
      "23/23 [==============================] - 1s 56ms/step - loss: 7.8845 - val_loss: 8.3317\n",
      "Epoch 783/1000\n",
      "23/23 [==============================] - 1s 54ms/step - loss: 7.8547 - val_loss: 8.3397\n",
      "Epoch 784/1000\n",
      "23/23 [==============================] - 1s 55ms/step - loss: 7.8394 - val_loss: 8.3342\n",
      "Epoch 785/1000\n",
      "23/23 [==============================] - 1s 31ms/step - loss: 7.8197 - val_loss: 8.3533\n",
      "Epoch 786/1000\n",
      "23/23 [==============================] - 1s 58ms/step - loss: 7.8766 - val_loss: 8.3682\n",
      "Epoch 787/1000\n",
      "23/23 [==============================] - 1s 59ms/step - loss: 7.8227 - val_loss: 8.3532\n",
      "Epoch 788/1000\n",
      "23/23 [==============================] - 1s 54ms/step - loss: 7.7811 - val_loss: 8.3469\n",
      "Epoch 789/1000\n",
      "23/23 [==============================] - 1s 52ms/step - loss: 7.8712 - val_loss: 8.3710\n",
      "Epoch 790/1000\n",
      "23/23 [==============================] - 1s 55ms/step - loss: 7.8768 - val_loss: 8.3328\n",
      "Epoch 791/1000\n",
      "23/23 [==============================] - 1s 52ms/step - loss: 7.7981 - val_loss: 8.3144\n",
      "Epoch 792/1000\n",
      "23/23 [==============================] - 1s 58ms/step - loss: 7.8707 - val_loss: 8.3468\n",
      "Epoch 793/1000\n",
      "23/23 [==============================] - 1s 52ms/step - loss: 7.8692 - val_loss: 8.3392\n",
      "Epoch 794/1000\n",
      "23/23 [==============================] - 1s 54ms/step - loss: 7.8573 - val_loss: 8.4489\n",
      "Epoch 795/1000\n",
      "23/23 [==============================] - 1s 30ms/step - loss: 7.8825 - val_loss: 8.3454\n",
      "Epoch 796/1000\n",
      "23/23 [==============================] - 1s 58ms/step - loss: 7.8061 - val_loss: 8.3372\n",
      "Epoch 797/1000\n",
      "23/23 [==============================] - 1s 30ms/step - loss: 7.8600 - val_loss: 8.3312\n",
      "Epoch 798/1000\n",
      "23/23 [==============================] - 1s 56ms/step - loss: 7.8845 - val_loss: 8.3493\n",
      "Epoch 799/1000\n",
      "23/23 [==============================] - 1s 63ms/step - loss: 7.8663 - val_loss: 8.4260\n",
      "Epoch 800/1000\n",
      "23/23 [==============================] - 1s 61ms/step - loss: 7.8431 - val_loss: 8.3232\n",
      "Epoch 801/1000\n",
      "23/23 [==============================] - 1s 60ms/step - loss: 7.8887 - val_loss: 8.3546\n",
      "Epoch 802/1000\n",
      "23/23 [==============================] - 1s 63ms/step - loss: 7.8740 - val_loss: 8.3504\n",
      "Epoch 803/1000\n",
      "23/23 [==============================] - 1s 62ms/step - loss: 7.8628 - val_loss: 8.3370\n",
      "Epoch 804/1000\n",
      "23/23 [==============================] - 1s 58ms/step - loss: 7.8698 - val_loss: 8.3730\n",
      "Epoch 805/1000\n",
      "23/23 [==============================] - 1s 62ms/step - loss: 7.8520 - val_loss: 8.4347\n",
      "Epoch 806/1000\n",
      "23/23 [==============================] - 1s 50ms/step - loss: 7.8932 - val_loss: 8.3476\n",
      "Epoch 807/1000\n",
      "23/23 [==============================] - 1s 57ms/step - loss: 7.8360 - val_loss: 8.3459\n",
      "Epoch 808/1000\n",
      "23/23 [==============================] - 1s 61ms/step - loss: 7.8770 - val_loss: 8.3076\n",
      "Epoch 809/1000\n",
      "23/23 [==============================] - 1s 60ms/step - loss: 7.8859 - val_loss: 8.3400\n",
      "Epoch 810/1000\n",
      "23/23 [==============================] - 1s 60ms/step - loss: 7.8195 - val_loss: 8.3178\n",
      "Epoch 811/1000\n",
      "23/23 [==============================] - 1s 59ms/step - loss: 7.8483 - val_loss: 8.3517\n",
      "Epoch 812/1000\n",
      "23/23 [==============================] - 1s 58ms/step - loss: 7.8712 - val_loss: 8.3495\n",
      "Epoch 813/1000\n",
      "23/23 [==============================] - 1s 57ms/step - loss: 7.7932 - val_loss: 8.3194\n",
      "Epoch 814/1000\n",
      "23/23 [==============================] - 1s 55ms/step - loss: 7.8822 - val_loss: 8.3704\n",
      "Epoch 815/1000\n",
      "23/23 [==============================] - 1s 58ms/step - loss: 7.8474 - val_loss: 8.3385\n",
      "Epoch 816/1000\n",
      "23/23 [==============================] - 1s 53ms/step - loss: 7.9294 - val_loss: 8.3792\n",
      "Epoch 817/1000\n",
      "23/23 [==============================] - 1s 60ms/step - loss: 7.8116 - val_loss: 8.3599\n",
      "Epoch 818/1000\n",
      "23/23 [==============================] - 1s 64ms/step - loss: 7.8280 - val_loss: 8.3454\n",
      "Epoch 819/1000\n",
      "23/23 [==============================] - 1s 61ms/step - loss: 7.8465 - val_loss: 8.3364\n",
      "Epoch 820/1000\n",
      "23/23 [==============================] - 1s 59ms/step - loss: 7.8951 - val_loss: 8.4529\n",
      "Epoch 821/1000\n",
      "23/23 [==============================] - 1s 56ms/step - loss: 7.8470 - val_loss: 8.3343\n",
      "Epoch 822/1000\n",
      "23/23 [==============================] - 1s 55ms/step - loss: 7.8516 - val_loss: 8.3456\n",
      "Epoch 823/1000\n",
      "23/23 [==============================] - 1s 55ms/step - loss: 7.8173 - val_loss: 8.3102\n",
      "Epoch 824/1000\n",
      "23/23 [==============================] - 1s 61ms/step - loss: 7.8237 - val_loss: 8.3712\n",
      "Epoch 825/1000\n",
      "23/23 [==============================] - 1s 63ms/step - loss: 7.8093 - val_loss: 8.3207\n",
      "Epoch 826/1000\n",
      "23/23 [==============================] - 1s 53ms/step - loss: 7.9306 - val_loss: 8.3708\n",
      "Epoch 827/1000\n",
      "23/23 [==============================] - 1s 60ms/step - loss: 7.8828 - val_loss: 8.3398\n",
      "Epoch 828/1000\n",
      "23/23 [==============================] - 1s 63ms/step - loss: 7.8411 - val_loss: 8.3509\n",
      "Epoch 829/1000\n",
      "23/23 [==============================] - 1s 61ms/step - loss: 7.8156 - val_loss: 8.3119\n",
      "Epoch 830/1000\n",
      "23/23 [==============================] - 1s 56ms/step - loss: 7.7496 - val_loss: 8.3925\n",
      "Epoch 831/1000\n",
      "23/23 [==============================] - 1s 56ms/step - loss: 7.8143 - val_loss: 8.3056\n",
      "Epoch 832/1000\n",
      "23/23 [==============================] - 1s 46ms/step - loss: 7.8382 - val_loss: 8.3710\n",
      "Epoch 833/1000\n",
      "23/23 [==============================] - 1s 58ms/step - loss: 7.8597 - val_loss: 8.3769\n",
      "Epoch 834/1000\n",
      "23/23 [==============================] - 1s 57ms/step - loss: 7.9338 - val_loss: 8.3503\n",
      "Epoch 835/1000\n",
      "23/23 [==============================] - 1s 56ms/step - loss: 7.8975 - val_loss: 8.3686\n",
      "Epoch 836/1000\n",
      "23/23 [==============================] - 1s 51ms/step - loss: 7.8691 - val_loss: 8.3685\n",
      "Epoch 837/1000\n",
      "23/23 [==============================] - 1s 52ms/step - loss: 7.8534 - val_loss: 8.3840\n",
      "Epoch 838/1000\n",
      "23/23 [==============================] - 1s 49ms/step - loss: 7.9040 - val_loss: 8.3468\n",
      "Epoch 839/1000\n",
      "23/23 [==============================] - 1s 30ms/step - loss: 7.8456 - val_loss: 8.3228\n",
      "Epoch 840/1000\n",
      "23/23 [==============================] - 1s 57ms/step - loss: 7.8911 - val_loss: 8.3891\n",
      "Epoch 841/1000\n",
      "23/23 [==============================] - 1s 59ms/step - loss: 7.8706 - val_loss: 8.3418\n",
      "Epoch 842/1000\n",
      "23/23 [==============================] - 1s 59ms/step - loss: 7.8443 - val_loss: 8.3590\n",
      "Epoch 843/1000\n",
      "23/23 [==============================] - 1s 50ms/step - loss: 7.8700 - val_loss: 8.5124\n",
      "Epoch 844/1000\n",
      "23/23 [==============================] - 1s 56ms/step - loss: 7.8750 - val_loss: 8.3413\n",
      "Epoch 845/1000\n",
      "23/23 [==============================] - 1s 57ms/step - loss: 7.8258 - val_loss: 8.3438\n",
      "Epoch 846/1000\n",
      "23/23 [==============================] - 1s 58ms/step - loss: 7.7833 - val_loss: 8.3355\n",
      "Epoch 847/1000\n",
      "23/23 [==============================] - 1s 58ms/step - loss: 7.8678 - val_loss: 8.3186\n",
      "Epoch 848/1000\n",
      "23/23 [==============================] - 1s 54ms/step - loss: 7.8335 - val_loss: 8.3539\n",
      "Epoch 849/1000\n",
      "23/23 [==============================] - 1s 53ms/step - loss: 7.8454 - val_loss: 8.3899\n",
      "Epoch 850/1000\n",
      "23/23 [==============================] - 1s 57ms/step - loss: 7.8622 - val_loss: 8.3366\n",
      "Epoch 851/1000\n",
      "23/23 [==============================] - 1s 58ms/step - loss: 7.8825 - val_loss: 8.3222\n",
      "Epoch 852/1000\n",
      "23/23 [==============================] - 1s 49ms/step - loss: 7.8223 - val_loss: 8.3959\n",
      "Epoch 853/1000\n",
      "23/23 [==============================] - 1s 50ms/step - loss: 7.8907 - val_loss: 8.3398\n",
      "Epoch 854/1000\n",
      "23/23 [==============================] - 1s 58ms/step - loss: 7.9341 - val_loss: 8.4138\n",
      "Epoch 855/1000\n",
      "23/23 [==============================] - 1s 30ms/step - loss: 7.9072 - val_loss: 8.3570\n",
      "Epoch 856/1000\n",
      "23/23 [==============================] - 1s 61ms/step - loss: 7.8297 - val_loss: 8.4512\n",
      "Epoch 857/1000\n",
      "23/23 [==============================] - 1s 57ms/step - loss: 7.8282 - val_loss: 8.3677\n",
      "Epoch 858/1000\n",
      "23/23 [==============================] - 1s 55ms/step - loss: 7.8998 - val_loss: 8.4835\n",
      "Epoch 859/1000\n",
      "23/23 [==============================] - 1s 60ms/step - loss: 7.8586 - val_loss: 8.3561\n",
      "Epoch 860/1000\n",
      "23/23 [==============================] - 1s 47ms/step - loss: 7.8830 - val_loss: 8.3686\n",
      "Epoch 861/1000\n",
      "23/23 [==============================] - 1s 55ms/step - loss: 7.8836 - val_loss: 8.3177\n",
      "Epoch 862/1000\n",
      "23/23 [==============================] - 1s 61ms/step - loss: 7.8867 - val_loss: 8.3152\n",
      "Epoch 863/1000\n",
      "23/23 [==============================] - 1s 59ms/step - loss: 7.8651 - val_loss: 8.3833\n",
      "Epoch 864/1000\n",
      "23/23 [==============================] - 1s 56ms/step - loss: 7.8376 - val_loss: 8.3168\n",
      "Epoch 865/1000\n",
      "23/23 [==============================] - 1s 30ms/step - loss: 7.8725 - val_loss: 8.3381\n",
      "Epoch 866/1000\n",
      "23/23 [==============================] - 1s 60ms/step - loss: 7.9484 - val_loss: 8.4692\n",
      "Epoch 867/1000\n",
      "23/23 [==============================] - 1s 52ms/step - loss: 7.8233 - val_loss: 8.3812\n",
      "Epoch 868/1000\n",
      "23/23 [==============================] - 1s 56ms/step - loss: 7.8829 - val_loss: 8.3757\n",
      "Epoch 869/1000\n",
      "23/23 [==============================] - 1s 62ms/step - loss: 7.8279 - val_loss: 8.3985\n",
      "Epoch 870/1000\n",
      "23/23 [==============================] - 1s 54ms/step - loss: 7.8326 - val_loss: 8.3533\n",
      "Epoch 871/1000\n",
      "23/23 [==============================] - 1s 56ms/step - loss: 7.8502 - val_loss: 8.3443\n",
      "Epoch 872/1000\n",
      "23/23 [==============================] - 1s 61ms/step - loss: 7.9089 - val_loss: 8.3838\n",
      "Epoch 873/1000\n",
      "23/23 [==============================] - 1s 59ms/step - loss: 7.8744 - val_loss: 8.3259\n",
      "Epoch 874/1000\n",
      "23/23 [==============================] - 1s 54ms/step - loss: 7.7858 - val_loss: 8.3999\n",
      "Epoch 875/1000\n",
      "23/23 [==============================] - 1s 58ms/step - loss: 7.8884 - val_loss: 8.3589\n",
      "Epoch 876/1000\n",
      "23/23 [==============================] - 1s 55ms/step - loss: 7.8304 - val_loss: 8.3328\n",
      "Epoch 877/1000\n",
      "23/23 [==============================] - 1s 63ms/step - loss: 7.8243 - val_loss: 8.4060\n",
      "Epoch 878/1000\n",
      "23/23 [==============================] - 1s 58ms/step - loss: 7.8920 - val_loss: 8.3177\n",
      "Epoch 879/1000\n",
      "23/23 [==============================] - 1s 53ms/step - loss: 7.8384 - val_loss: 8.3236\n",
      "Epoch 880/1000\n",
      "23/23 [==============================] - 1s 29ms/step - loss: 7.7440 - val_loss: 8.3398\n",
      "Epoch 881/1000\n",
      "23/23 [==============================] - 1s 58ms/step - loss: 7.8295 - val_loss: 8.3342\n",
      "Epoch 882/1000\n",
      "23/23 [==============================] - 1s 51ms/step - loss: 7.8332 - val_loss: 8.3596\n",
      "Epoch 883/1000\n",
      "23/23 [==============================] - 1s 64ms/step - loss: 7.8634 - val_loss: 8.3396\n",
      "Epoch 884/1000\n",
      "23/23 [==============================] - 1s 49ms/step - loss: 7.8126 - val_loss: 8.3406\n",
      "Epoch 885/1000\n",
      "23/23 [==============================] - 1s 47ms/step - loss: 7.8674 - val_loss: 8.3719\n",
      "Epoch 886/1000\n",
      "23/23 [==============================] - 1s 57ms/step - loss: 7.8213 - val_loss: 8.3811\n",
      "Epoch 887/1000\n",
      "23/23 [==============================] - 1s 61ms/step - loss: 7.9848 - val_loss: 8.3944\n",
      "Epoch 888/1000\n",
      "23/23 [==============================] - 1s 49ms/step - loss: 7.9615 - val_loss: 8.3602\n",
      "Epoch 889/1000\n",
      "23/23 [==============================] - 1s 61ms/step - loss: 7.8039 - val_loss: 8.3606\n",
      "Epoch 890/1000\n",
      "23/23 [==============================] - 1s 63ms/step - loss: 7.8350 - val_loss: 8.3346\n",
      "Epoch 891/1000\n",
      "23/23 [==============================] - 1s 62ms/step - loss: 7.8483 - val_loss: 8.3315\n",
      "Epoch 892/1000\n",
      "23/23 [==============================] - 1s 52ms/step - loss: 7.7725 - val_loss: 8.3787\n",
      "Epoch 893/1000\n",
      "23/23 [==============================] - 1s 52ms/step - loss: 7.9309 - val_loss: 8.3447\n",
      "Epoch 894/1000\n",
      "23/23 [==============================] - 1s 56ms/step - loss: 7.8636 - val_loss: 8.3624\n",
      "Epoch 895/1000\n",
      "23/23 [==============================] - 1s 49ms/step - loss: 7.8743 - val_loss: 8.3533\n",
      "Epoch 896/1000\n",
      "23/23 [==============================] - 1s 57ms/step - loss: 7.8406 - val_loss: 8.3523\n",
      "Epoch 897/1000\n",
      "23/23 [==============================] - 1s 57ms/step - loss: 7.8852 - val_loss: 8.3526\n",
      "Epoch 898/1000\n",
      "23/23 [==============================] - 1s 56ms/step - loss: 7.8599 - val_loss: 8.3677\n",
      "Epoch 899/1000\n",
      "23/23 [==============================] - 1s 49ms/step - loss: 7.9039 - val_loss: 8.3381\n",
      "Epoch 900/1000\n",
      "23/23 [==============================] - 1s 52ms/step - loss: 7.8099 - val_loss: 8.5077\n",
      "Epoch 901/1000\n",
      "23/23 [==============================] - 1s 55ms/step - loss: 7.9404 - val_loss: 8.3445\n",
      "Epoch 902/1000\n",
      "23/23 [==============================] - 1s 58ms/step - loss: 7.7880 - val_loss: 8.3476\n",
      "Epoch 903/1000\n",
      "23/23 [==============================] - 1s 54ms/step - loss: 7.8313 - val_loss: 8.3582\n",
      "Epoch 904/1000\n",
      "23/23 [==============================] - 1s 61ms/step - loss: 7.8195 - val_loss: 8.3368\n",
      "Epoch 905/1000\n",
      "23/23 [==============================] - 1s 55ms/step - loss: 7.8449 - val_loss: 8.3700\n",
      "Epoch 906/1000\n",
      "23/23 [==============================] - 1s 61ms/step - loss: 7.8581 - val_loss: 8.3302\n",
      "Epoch 907/1000\n",
      "23/23 [==============================] - 1s 43ms/step - loss: 7.9597 - val_loss: 8.3373\n",
      "Epoch 908/1000\n",
      "23/23 [==============================] - 1s 65ms/step - loss: 7.9292 - val_loss: 8.4277\n",
      "Epoch 909/1000\n",
      "23/23 [==============================] - 1s 57ms/step - loss: 7.8570 - val_loss: 8.3264\n",
      "Epoch 910/1000\n",
      "23/23 [==============================] - 1s 54ms/step - loss: 7.8439 - val_loss: 8.3469\n",
      "Epoch 911/1000\n",
      "23/23 [==============================] - 1s 52ms/step - loss: 7.8022 - val_loss: 8.3407\n",
      "Epoch 912/1000\n",
      "23/23 [==============================] - 1s 53ms/step - loss: 7.9080 - val_loss: 8.3421\n",
      "Epoch 913/1000\n",
      "23/23 [==============================] - 1s 61ms/step - loss: 7.9005 - val_loss: 8.3468\n",
      "Epoch 914/1000\n",
      "23/23 [==============================] - 1s 58ms/step - loss: 7.8414 - val_loss: 8.3987\n",
      "Epoch 915/1000\n",
      "23/23 [==============================] - 1s 52ms/step - loss: 7.8865 - val_loss: 8.3450\n",
      "Epoch 916/1000\n",
      "23/23 [==============================] - 1s 52ms/step - loss: 7.9305 - val_loss: 8.3393\n",
      "Epoch 917/1000\n",
      "23/23 [==============================] - 1s 60ms/step - loss: 7.9483 - val_loss: 8.4739\n",
      "Epoch 918/1000\n",
      "23/23 [==============================] - 1s 52ms/step - loss: 7.8617 - val_loss: 8.3541\n",
      "Epoch 919/1000\n",
      "23/23 [==============================] - 1s 60ms/step - loss: 7.8072 - val_loss: 8.3384\n",
      "Epoch 920/1000\n",
      "23/23 [==============================] - 1s 55ms/step - loss: 7.7456 - val_loss: 8.3840\n",
      "Epoch 921/1000\n",
      "23/23 [==============================] - 1s 57ms/step - loss: 7.8158 - val_loss: 8.3745\n",
      "Epoch 922/1000\n",
      "23/23 [==============================] - 1s 49ms/step - loss: 7.8405 - val_loss: 8.3450\n",
      "Epoch 923/1000\n",
      "23/23 [==============================] - 1s 58ms/step - loss: 7.8602 - val_loss: 8.3878\n",
      "Epoch 924/1000\n",
      "23/23 [==============================] - 1s 58ms/step - loss: 7.8408 - val_loss: 8.3418\n",
      "Epoch 925/1000\n",
      "23/23 [==============================] - 1s 55ms/step - loss: 7.8874 - val_loss: 8.3492\n",
      "Epoch 926/1000\n",
      "23/23 [==============================] - 1s 62ms/step - loss: 7.8158 - val_loss: 8.3419\n",
      "Epoch 927/1000\n",
      "23/23 [==============================] - 1s 60ms/step - loss: 7.8799 - val_loss: 8.3302\n",
      "Epoch 928/1000\n",
      "23/23 [==============================] - 1s 56ms/step - loss: 7.8233 - val_loss: 8.3289\n",
      "Epoch 929/1000\n",
      "23/23 [==============================] - 1s 58ms/step - loss: 7.9162 - val_loss: 8.4027\n",
      "Epoch 930/1000\n",
      "23/23 [==============================] - 1s 57ms/step - loss: 7.8911 - val_loss: 8.3855\n",
      "Epoch 931/1000\n",
      "23/23 [==============================] - 1s 50ms/step - loss: 7.8455 - val_loss: 8.3675\n",
      "Epoch 932/1000\n",
      "23/23 [==============================] - 1s 62ms/step - loss: 7.8709 - val_loss: 8.3370\n",
      "Epoch 933/1000\n",
      "23/23 [==============================] - 1s 53ms/step - loss: 7.8134 - val_loss: 8.3492\n",
      "Epoch 934/1000\n",
      "23/23 [==============================] - 1s 30ms/step - loss: 7.8114 - val_loss: 8.3511\n",
      "Epoch 935/1000\n",
      "23/23 [==============================] - 1s 61ms/step - loss: 7.8483 - val_loss: 8.4493\n",
      "Epoch 936/1000\n",
      "23/23 [==============================] - 1s 56ms/step - loss: 7.8219 - val_loss: 8.3559\n",
      "Epoch 937/1000\n",
      "23/23 [==============================] - 1s 58ms/step - loss: 7.8479 - val_loss: 8.3393\n",
      "Epoch 938/1000\n",
      "23/23 [==============================] - 1s 56ms/step - loss: 7.8300 - val_loss: 8.3591\n",
      "Epoch 939/1000\n",
      "23/23 [==============================] - 1s 59ms/step - loss: 7.8308 - val_loss: 8.3615\n",
      "Epoch 940/1000\n",
      "23/23 [==============================] - 1s 58ms/step - loss: 7.8550 - val_loss: 8.4003\n",
      "Epoch 941/1000\n",
      "23/23 [==============================] - 1s 61ms/step - loss: 7.8506 - val_loss: 8.3334\n",
      "Epoch 942/1000\n",
      "23/23 [==============================] - 1s 58ms/step - loss: 7.8082 - val_loss: 8.3591\n",
      "Epoch 943/1000\n",
      "23/23 [==============================] - 1s 53ms/step - loss: 7.8252 - val_loss: 8.4463\n",
      "Epoch 944/1000\n",
      "23/23 [==============================] - 1s 63ms/step - loss: 7.7850 - val_loss: 8.3501\n",
      "Epoch 945/1000\n",
      "23/23 [==============================] - 1s 58ms/step - loss: 7.8606 - val_loss: 8.3509\n",
      "Epoch 946/1000\n",
      "23/23 [==============================] - 1s 59ms/step - loss: 7.8158 - val_loss: 8.3438\n",
      "Epoch 947/1000\n",
      "23/23 [==============================] - 1s 57ms/step - loss: 7.8342 - val_loss: 8.3489\n",
      "Epoch 948/1000\n",
      "23/23 [==============================] - 1s 55ms/step - loss: 7.8148 - val_loss: 8.3869\n",
      "Epoch 949/1000\n",
      "23/23 [==============================] - 1s 49ms/step - loss: 7.8422 - val_loss: 8.3586\n",
      "Epoch 950/1000\n",
      "23/23 [==============================] - 1s 53ms/step - loss: 7.8783 - val_loss: 8.3953\n",
      "Epoch 951/1000\n",
      "23/23 [==============================] - 1s 39ms/step - loss: 7.8260 - val_loss: 8.3396\n",
      "Epoch 952/1000\n",
      "23/23 [==============================] - 1s 56ms/step - loss: 7.8547 - val_loss: 8.3753\n",
      "Epoch 953/1000\n",
      "23/23 [==============================] - 1s 61ms/step - loss: 7.8652 - val_loss: 8.3562\n",
      "Epoch 954/1000\n",
      "23/23 [==============================] - 1s 59ms/step - loss: 7.8063 - val_loss: 8.3316\n",
      "Epoch 955/1000\n",
      "23/23 [==============================] - 1s 57ms/step - loss: 7.8284 - val_loss: 8.3430\n",
      "Epoch 956/1000\n",
      "23/23 [==============================] - 1s 49ms/step - loss: 7.8299 - val_loss: 8.3453\n",
      "Epoch 957/1000\n",
      "23/23 [==============================] - 1s 60ms/step - loss: 7.8661 - val_loss: 8.3884\n",
      "Epoch 958/1000\n",
      "23/23 [==============================] - 1s 59ms/step - loss: 7.8433 - val_loss: 8.3581\n",
      "Epoch 959/1000\n",
      "23/23 [==============================] - 1s 58ms/step - loss: 7.8215 - val_loss: 8.4288\n",
      "Epoch 960/1000\n",
      "23/23 [==============================] - 1s 57ms/step - loss: 7.8453 - val_loss: 8.3484\n",
      "Epoch 961/1000\n",
      "23/23 [==============================] - 1s 54ms/step - loss: 7.8049 - val_loss: 8.3625\n",
      "Epoch 962/1000\n",
      "23/23 [==============================] - 1s 50ms/step - loss: 7.8419 - val_loss: 8.3892\n",
      "Epoch 963/1000\n",
      "23/23 [==============================] - 1s 56ms/step - loss: 7.8785 - val_loss: 8.3374\n",
      "Epoch 964/1000\n",
      "23/23 [==============================] - 1s 33ms/step - loss: 7.8679 - val_loss: 8.3426\n",
      "Epoch 965/1000\n",
      "23/23 [==============================] - 1s 55ms/step - loss: 7.8298 - val_loss: 8.3785\n",
      "Epoch 966/1000\n",
      "23/23 [==============================] - 1s 56ms/step - loss: 7.8583 - val_loss: 8.4440\n",
      "Epoch 967/1000\n",
      "23/23 [==============================] - 1s 54ms/step - loss: 7.9085 - val_loss: 8.3533\n",
      "Epoch 968/1000\n",
      "23/23 [==============================] - 1s 56ms/step - loss: 7.8468 - val_loss: 8.3554\n",
      "Epoch 969/1000\n",
      "23/23 [==============================] - 1s 59ms/step - loss: 7.8706 - val_loss: 8.3492\n",
      "Epoch 970/1000\n",
      "23/23 [==============================] - 1s 58ms/step - loss: 7.9012 - val_loss: 8.3591\n",
      "Epoch 971/1000\n",
      "23/23 [==============================] - 1s 53ms/step - loss: 7.8292 - val_loss: 8.3588\n",
      "Epoch 972/1000\n",
      "23/23 [==============================] - 1s 57ms/step - loss: 7.8825 - val_loss: 8.3980\n",
      "Epoch 973/1000\n",
      "23/23 [==============================] - 1s 58ms/step - loss: 7.8341 - val_loss: 8.3218\n",
      "Epoch 974/1000\n",
      "23/23 [==============================] - 1s 57ms/step - loss: 7.8798 - val_loss: 8.3865\n",
      "Epoch 975/1000\n",
      "23/23 [==============================] - 1s 57ms/step - loss: 7.8421 - val_loss: 8.3349\n",
      "Epoch 976/1000\n",
      "23/23 [==============================] - 1s 50ms/step - loss: 7.8649 - val_loss: 8.3358\n",
      "Epoch 977/1000\n",
      "23/23 [==============================] - 1s 59ms/step - loss: 7.8929 - val_loss: 8.3329\n",
      "Epoch 978/1000\n",
      "23/23 [==============================] - 1s 56ms/step - loss: 7.9056 - val_loss: 8.3487\n",
      "Epoch 979/1000\n",
      "23/23 [==============================] - 1s 61ms/step - loss: 7.8638 - val_loss: 8.3392\n",
      "Epoch 980/1000\n",
      "23/23 [==============================] - 1s 30ms/step - loss: 7.8934 - val_loss: 8.3372\n",
      "Epoch 981/1000\n",
      "23/23 [==============================] - 1s 61ms/step - loss: 7.8647 - val_loss: 8.3324\n",
      "Epoch 982/1000\n",
      "23/23 [==============================] - 1s 62ms/step - loss: 7.8618 - val_loss: 8.3799\n",
      "Epoch 983/1000\n",
      "23/23 [==============================] - 1s 57ms/step - loss: 7.8502 - val_loss: 8.3340\n",
      "Epoch 984/1000\n",
      "23/23 [==============================] - 1s 55ms/step - loss: 7.8805 - val_loss: 8.3362\n",
      "Epoch 985/1000\n",
      "23/23 [==============================] - 1s 61ms/step - loss: 7.8248 - val_loss: 8.3742\n",
      "Epoch 986/1000\n",
      "23/23 [==============================] - 1s 57ms/step - loss: 7.8484 - val_loss: 8.3787\n",
      "Epoch 987/1000\n",
      "23/23 [==============================] - 1s 58ms/step - loss: 7.8521 - val_loss: 8.4270\n",
      "Epoch 988/1000\n",
      "23/23 [==============================] - 1s 53ms/step - loss: 7.7536 - val_loss: 8.3393\n",
      "Epoch 989/1000\n",
      "23/23 [==============================] - 1s 60ms/step - loss: 7.8776 - val_loss: 8.4029\n",
      "Epoch 990/1000\n",
      "23/23 [==============================] - 1s 57ms/step - loss: 8.0281 - val_loss: 8.3622\n",
      "Epoch 991/1000\n",
      "23/23 [==============================] - 1s 59ms/step - loss: 7.9048 - val_loss: 8.3546\n",
      "Epoch 992/1000\n",
      "23/23 [==============================] - 1s 59ms/step - loss: 7.8494 - val_loss: 8.3811\n",
      "Epoch 993/1000\n",
      "23/23 [==============================] - 1s 59ms/step - loss: 7.8085 - val_loss: 8.3488\n",
      "Epoch 994/1000\n",
      "23/23 [==============================] - 1s 45ms/step - loss: 7.8717 - val_loss: 8.3341\n",
      "Epoch 995/1000\n",
      "23/23 [==============================] - 1s 58ms/step - loss: 7.8828 - val_loss: 8.3230\n",
      "Epoch 996/1000\n",
      "23/23 [==============================] - 1s 58ms/step - loss: 7.8073 - val_loss: 8.3480\n",
      "Epoch 997/1000\n",
      "23/23 [==============================] - 1s 57ms/step - loss: 7.8216 - val_loss: 8.3405\n",
      "Epoch 998/1000\n",
      "23/23 [==============================] - 1s 29ms/step - loss: 7.8944 - val_loss: 8.3648\n",
      "Epoch 999/1000\n",
      "23/23 [==============================] - 1s 54ms/step - loss: 7.8368 - val_loss: 8.3643\n",
      "Epoch 1000/1000\n",
      "23/23 [==============================] - 1s 52ms/step - loss: 7.8190 - val_loss: 8.3318\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f21847a9850>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vae.fit(train_dataset,train_dataset, \n",
    "        validation_data=(eval_dataset,eval_dataset), \n",
    "        batch_size=32,\n",
    "        epochs=1000, \n",
    "        callbacks = [model_checkpoint_callback]\n",
    "       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tfp.distributions.Normal 'Normal' batch_shape=[4] event_shape=[] dtype=float32>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfd.Normal(loc=tf.zeros(4), scale=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96.5\n",
      "78.6\n",
      "69.0\n",
      "62.1\n",
      "47.6\n",
      "0.09531900898925674\n",
      "0.0676386757290068\n",
      "0.0629796097347073\n",
      "0.05470291602718702\n",
      "0.06851567638675729\n",
      "0.0639662354746766\n",
      "0.05574435430826573\n",
      "0.08726156544617408\n",
      "0.07229774172330629\n",
      "0.08167068625301468\n",
      "0.06643279982459988\n",
      "0.05985529489147117\n",
      "0.05207191405393554\n",
      "0.05064678798509099\n",
      "(730, 5)\n",
      "0.08493150684931507\n",
      "0.06027397260273973\n",
      "0.05342465753424658\n",
      "0.04246575342465753\n",
      "0.06027397260273973\n",
      "0.052054794520547946\n",
      "0.04246575342465753\n",
      "0.0684931506849315\n",
      "0.052054794520547946\n",
      "0.06301369863013699\n",
      "0.0589041095890411\n",
      "0.049315068493150684\n",
      "0.038356164383561646\n",
      "0.038356164383561646\n"
     ]
    }
   ],
   "source": [
    "R4_23 = np.sort(R4[:,0])\n",
    "R4_24 = np.sort(R4[:,1])\n",
    "R4_25 = np.sort(R4[:,2])\n",
    "R4_26 = np.sort(R4[:,3])\n",
    "R4_27 = np.sort(R4[:,4])\n",
    "\n",
    "q = 0.9\n",
    "n = int(q*18244)\n",
    "\n",
    "print(R4_23[n])\n",
    "print(R4_24[n])\n",
    "print(R4_25[n])\n",
    "print(R4_26[n])\n",
    "print(R4_27[n])\n",
    "\n",
    "U1 = R4[:,0]>R4_23[n]\n",
    "U2 = R4[:,1]>R4_24[n]\n",
    "U3 = R4[:,2]>R4_25[n]\n",
    "U4 = R4[:,3]>R4_26[n]\n",
    "U5 = R4[:,4]>R4_27[n]\n",
    "print(np.sum(U1*U2)/18244)\n",
    "print(np.sum(U1*U3)/18244)\n",
    "print(np.sum(U1*U4)/18244)\n",
    "print(np.sum(U1*U5)/18244)\n",
    "print(np.sum(U2*U3)/18244)\n",
    "\n",
    "print(np.sum(U2*U4)/18244)\n",
    "print(np.sum(U2*U5)/18244)\n",
    "print(np.sum(U3*U4)/18244)\n",
    "print(np.sum(U5*U3)/18244)\n",
    "print(np.sum(U5*U4)/18244)\n",
    "\n",
    "print(np.sum(U1*U2*U3)/18244)\n",
    "print(np.sum(U1*U2*U3*U4)/18244)\n",
    "print(np.sum(U2*U3*U4*U5)/18244)\n",
    "print(np.sum(U1*U2*U3*U4*U5)/18244)\n",
    "#print(np.count_nonzero(R4[:,0]>R4_23[n] & R4[:,1]>R4_24[n]))\n",
    "\n",
    "train_dataset = R4[::25,:]\n",
    "print(train_dataset.shape)\n",
    "\n",
    "\n",
    "U1 = train_dataset[:,0]>R4_23[n]\n",
    "U2 = train_dataset[:,1]>R4_24[n]\n",
    "U3 = train_dataset[:,2]>R4_25[n]\n",
    "U4 = train_dataset[:,3]>R4_26[n]\n",
    "U5 = train_dataset[:,4]>R4_27[n]\n",
    "print(np.sum(U1*U2)/730)\n",
    "print(np.sum(U1*U3)/730)\n",
    "print(np.sum(U1*U4)/730)\n",
    "print(np.sum(U1*U5)/730)\n",
    "print(np.sum(U2*U3)/730)\n",
    "print(np.sum(U2*U4)/730)\n",
    "print(np.sum(U2*U5)/730)\n",
    "print(np.sum(U3*U4)/730)\n",
    "print(np.sum(U5*U3)/730)\n",
    "print(np.sum(U5*U4)/730)\n",
    "\n",
    "print(np.sum(U1*U2*U3)/730)\n",
    "print(np.sum(U1*U2*U3*U4)/730)\n",
    "print(np.sum(U2*U3*U4*U5)/730)\n",
    "print(np.sum(U1*U2*U3*U4*U5)/730)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18244, 4)\n",
      "(18244, 5)\n"
     ]
    }
   ],
   "source": [
    "size = 18244\n",
    "\n",
    "prior_samples = vae.encoder.prior.sample((size))\n",
    "print(prior_samples.shape)\n",
    "angles_dist   = vae.decoder(prior_samples)\n",
    "genD          = 10*angles_dist.sample().numpy()\n",
    "print(genD.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.04889278666959\n",
      "0.05777241832931375\n",
      "0.05788204341153256\n",
      "0.058868669151501865\n",
      "0.05141416356062267\n",
      "0.07026967770225828\n",
      "0.06001973251479938\n",
      "0.06084192063144048\n",
      "0.058484981363736026\n",
      "0.05141416356062267\n",
      "0.03836877877658408\n",
      "0.03316158737119053\n",
      "0.034312650734488054\n",
      "0.025487831615873712\n"
     ]
    }
   ],
   "source": [
    "#Generated samples\n",
    "\n",
    "\n",
    "U1 = genD[:,0]>R4_23[n]\n",
    "U2 = genD[:,1]>R4_24[n]\n",
    "U3 = genD[:,2]>R4_25[n]\n",
    "U4 = genD[:,3]>R4_26[n]\n",
    "U5 = genD[:,4]>R4_27[n]\n",
    "print(np.sum(U1*U2)/18244)\n",
    "print(np.sum(U1*U3)/18244)\n",
    "print(np.sum(U2*U3)/18244)\n",
    "print(np.sum(U2*U4)/18244)\n",
    "print(np.sum(U2*U5)/18244)\n",
    "print(np.sum(U3*U4)/18244)\n",
    "print(np.sum(U5*U3)/18244)\n",
    "print(np.sum(U5*U4)/18244)\n",
    "print(np.sum(U1*U4)/18244)\n",
    "print(np.sum(U1*U5)/18244)\n",
    "print(np.sum(U1*U2*U3)/18244)\n",
    "print(np.sum(U1*U2*U3*U4)/18244)\n",
    "print(np.sum(U2*U3*U4*U5)/18244)\n",
    "print(np.sum(U1*U2*U3*U4*U5)/18244)\n",
    "#print(np.count_nonzero(R4[:,0]>R4_23[n] & R4[:,1]>R4_24[n]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
